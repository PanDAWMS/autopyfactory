\documentclass[a4paper]{jpconf}
%\usepackage{graphicx}
\begin{document}
\title{AutoPyFactory: A Scalable Flexible Pilot Factory Implementation}

\author{J. Caballero$^1$, J. Hover $^1$, P. Love$^2$, G. Stewart$^3$}

\address{$^1$ Brookhaven National Laboratory, PO BOX 5000 Upton, NY 11973, USA}
\address{$^2$ Department of Physics, Lancaster University, Lancaster, LA1 4YB, UK }
\address{$^3$ Department of Physics and Astronomy, University of Glasgow, Glasgow G12 8QQ, UK}

\ead{jcaballero@bnl.gov}

\begin{abstract}
The ATLAS experiment at the CERN LHC is one of the largest users of grid computing
infrastructure, which is a central part of the experiment's computing operations.
Considerable efforts have been made to use grid technology in the most efficient
and effective way, including the use of a pilot job based workload management framework.
In this model the experiment submits 'pilot' jobs to sites without payload. When these
jobs begin to run they contact a central service to pick-up a real payload to execute.
The first generation of pilot factories were usually specific to a single VO, and were
bound to the particular architecture of that VO's distributed processing. A second
generation provides factories which are more flexible, not tied to any particular VO,
and provide new or improved features such as monitoring, logging, profiling, etc.
In this paper we describe this key part of the ATLAS pilot architecture, a second
generation pilot factory, AutoPyFactory.
AutoPyFactory has a modular design and is highly configurable. It is able to send
different types of pilots to sites and exploit different submission mechanisms and queue
characteristics. It is tightly integrated with the PanDA job submission framework,
coupling pilot flow to the amount of work the site has to run. It gathers information
from many sources in order to correctly configure itself for a site, and its decision logic
can easily be updated.
Integrated into AutoPyFactory is a flexible system for delivering both generic and
specific job wrappers which can perform many useful actions before starting to run
end-user scientific applications, e.g. validation of the middleware, node profiling
and diagnostics, and monitoring.
AutoPyFactory now also has a robust monitoring system and we show how this has helped
establish a reliable pilot factory service for ATLAS.
\end{abstract}


\section{Introduction}

\subsection{}

\section{Architecture}


AutoPyFactory has a modular design and is highly configurable. 
It is able to send different types of pilots to sites, 
able to exploit different submission mechanisms and different charateristics of queues at sites. 
It has excellent integration with the PanDA job submission framework, 
tying pilot flows closely to the amount of work the site has to run. 
It is able to gather information from many sources, 
in order to correctly conigure itself for a site and its decision logic can easily be updated.

\subsection{Plug-ins design}

AutoPyFactory can serve to different queues in different ways 
thanks to its modular design based on plug-ins. 
There are currently 5 types of plug-ins:

\subsubsection{WMS Status Plug-in} 
it queries a given WMS system. E.g. PandaWMSStatusPlugin queries PanDA. 
The only requirements is the WMS must provide for an API. 
This API has to return the number of jobs in different status (ready, running, finished...) per queue, 
in such a way that information can be converted internally into APF nomenclature. 

\subsubsection{Batch Status Plug-in} 
it queries the specific batch system being used to submit jobs (or pilots). 
E.g. CondorBatchStatusPlugin queries condor (condor_q)

\subsubsection{Scheduler Plugin-in} 
it is the component in charge of making a decision based on the information provided by the two Status Plug-ins. 
It implements a given algorithm to decide how many jobs (or pilots) should be submitted next cyle. 
E.g. ActivatedSchedPlugin decides the number of new jobs based on the number of jobs in a ready status in the WMS service, 
with some restrictions to prevent burning a CE. 
FixedSchedPlugin always submits a fixed number of jobs. Etc.

\subsubsection{Execution Plug-in} 
It is the component in charge of submitting new jobs (or pilots), 
based on the decision made by the Sched Plug-in. 
Examples are CondorGT2BatchSubmitPlugin, CondorGT5BatchSubmitPlugin, CondorLocalBatchSubmitPlugin, CondorCREAMBatchSubmitPlugin, and CondorEC2BatchSubmitPlugin.

\subsubsection{Configuration Plug-in} 
a plug-in to retrieve extra configuration files. 
E.g PandaConfigPlugin queries SchedConfig. 

\subsection{Usage}

The normal usage will make use of one of set of 4 plug-ins, each per category, per workflow (called APFQueue). 
Typically, each APFQueue will correspond with one queue as it is defined in the WMS service, 
or the unique combination of a queue in the WMS service and a queue as defined in the batch system. 
This allows for many-to-many combination, each one served by an APFQueue. 

However, the combination of two APFQueues working together allows for more complex workflows.

\subsubsection{APF working \emph{a la CloudScheduler}}

The combination of two APFQueues can allow using Cloud Computing resources. 
For this to happen, the configuration of both APFQueues is like this:

\begin{enumerate}
\item First APFQueue: 
the WMS Status Plug-in queries the VO WMS service, and the Execution Plugin submits jobs (or pilots) to a local condor pool, for example.
\item Second APFQueue: 
the WMS Status Plug-in queries the local condor pool, 
and gives to idle jobs in queue the same meaning that jobs ready in an VO WMS service. 
The Execution Plug-in submits Virtual Machine instantiation orders (to EC2, for example). 
Those VMs will join the local condor pool allowing the idle jobs (or pilot) to flow an start execution. 
\end{enumerate}

\subsubsection{APF working \emph{a la glideinWMS}}

It would be possible to replicate the above mechanism to allow APF to run in a similar way that glideinWMS does. 
Two APF workflows combined, one queries the VO WMS service and submitting jobs (or pilot) to a local pool, 
and a second APF workflow submitting glideins to join that local pool. 

\section{Monitor}


%%\section{Acknowledgements}
\ack{
The authors would like to thank 
}

\section*{References}
\begin{thebibliography}{99}


\item ATLAS Collaboration 1994 ATLAS Technical Proposal 
      {\it CERN/LHCC/94-43} 

\end{thebibliography}

~

%%Notice:
%%This manuscript has been authored by employees of Brookhaven Science Associates, 
%%LLC under Contract No. XXXXXXXXXX with the U.S. Department of Energy. 
%%The publisher by accepting the manuscript for publication acknowledges 
%%that the United States Government retains a non-exclusive, paid-up, irrevocable, 
%%world-wide license to publish or reproduce the published form of this manuscript, 
%%or allow others to do so, for United States Government purposes.

\end{document}
