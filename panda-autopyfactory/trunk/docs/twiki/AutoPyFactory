*WARNING: THIS DOCUMENTATION IS WORK IN PROGRESS!!!*


<!-- some useful definitions  (need 3 white spaces before * to enable it)
   * Set UCL_PROMPT_ROOT = [root@factory ~]$
   * Set VERSION = 2.4.3
-->

---+!! Deployment of !AutoPyFactory
<!--
%DOC_STATUS_TABLE%
-->
%TOC{depth="3"}%

---# About this Document

This document describes how to install and configure AutoPyFactory

%INCLUDE{"Documentation/DocumentationTeam/DocConventions" section="Header"}%
%INCLUDE{"Documentation/DocumentationTeam/DocConventions" section="CommandLine"}%

%STARTSECTION{"Version"}%
---# Applicable versions
This document explains how to install and configure latest version of APF: %VERSION%
%ENDSECTION{"Version"}%

---# Deployment using RPM

Installation as root via RPMs has now been quite simplified. These instructions assume Red Hat /
Enterprise Linux 6.x (and derivates) and the system Python 2.6.x. Other distros and higher 
Python versions should work with some extra work. 

1) Install and enable a supported batch system. Condor is the current supported default. /
Software available from  http://www.cs.wisc.edu/condor/. Condor/Condor-G setup and 
configuration is beyond the scope of this documentation. Ensure that it is working
properly before proceeding. 

2) Install a grid client and set up the grid certificate+key under the user APF will run as. 
Please read the section [[#4_4_proxy_conf][proxy.conf]] regarding the proxy.conf file, so you see what 
will be needed. Make sure voms-proxy-* commands work properly. 

3) Add the racf-grid YUM repo to your system

<pre class="rootscreen">
%UCL_PROMPT_ROOT% rpm -ivh http://dev.racf.bnl.gov/yum/grid/production/rhel/6Workstation/x86_64/racf-grid-release-latest.noarch.rpm</pre>

The warning about NOKEY is expected. This release RPM sets up YUM to point at our 
repository, and installs the GPG key with which all our RPMs are signed. By default
the racf-grid-release RPM sets our production repository to enabled (see 
=/etc/yum.repos.d/racf-grid-production.repo=). 

*NOTE*: If you are testing APF and want to run
a pre-release version, enable the racf-grid-development or racf-grid-testing repository. 

4) If you will be performing *local* batch system submission (as opposed to remote submission
via grid interfaces) you must confirm that whatever account you'll be submitting as exists on
the batch cluster. This is also the user you should set APF to run as. 

*NOTE*: You do not want local batch logs being written to NFS, so you will need to define a 
local directory for logs and be sure the APF user can write there. 


5) Install the APF RPM:

<pre class="rootscreen">
%UCL_PROMPT_ROOT% yum install autopyfactory</pre>
   

This performs several setup steps that otherwise would need to be done manually:

   * Creates 'autopyfactory' user that APF will run under.
   * Enables the factory init script via chkconfig.
   * Pulls in the panda userinterface Python library RPM from our repository. 
   * Pulls in the python-simplejson RPM from the standard repository.

*NOTE*: If the yum install command did not work as expected, and no package was installed, or the version is too old, you should probably want to enable the development repository. 
Edit the file =/etc/yum.repos.d/racf-grid-development.repo=  and set 
<pre class="file">
enabled=1
</pre>   

6) Copy the configuration files to the right place.

<pre class="rootscreen">
%UCL_PROMPT_ROOT% cp /usr/share/doc/autopyfactory-%VERSION%/sysconfig/autopyfactory-example /etc/sysconfig/autopyfactory
%UCL_PROMPT_ROOT% cp /usr/share/doc/autopyfactory-%VERSION%/logrotate/autopyfactory-example /etc/logrotate.d/autopyfactory
%UCL_PROMPT_ROOT% cp /usr/share/doc/autopyfactory-%VERSION%/autopyfactory.conf-example  /etc/autopyfactory/autopyfactory.conf
%UCL_PROMPT_ROOT% cp /usr/share/doc/autopyfactory-%VERSION%/queues.conf-example  /etc/autopyfactory/queues.conf
%UCL_PROMPT_ROOT% cp /usr/share/doc/autopyfactory-%VERSION%/proxy.conf-example  /etc/autopyfactory/proxy.conf
%UCL_PROMPT_ROOT% cp /usr/share/doc/autopyfactory-%VERSION%/monitor.conf-example  /etc/autopyfactory/monitor.conf
%UCL_PROMPT_ROOT% cp /usr/share/doc/autopyfactory-%VERSION%/mappings.conf-example  /etc/autopyfactory/mappings.conf</pre>

Configure APF queues/job submission as desired. Read the CONFIGURATION documentation in 
   order to do this. Be sure to configure at least one queue in order to test function. 

7) Start APF:

<pre class="rootscreen">
%UCL_PROMPT_ROOT% /etc/init.d/factory start</pre>
    
8) Confirm that everything is OK:

   *  Check to see if APF is running:
<pre class="rootscreen">
%UCL_PROMPT_ROOT% /etc/init.d/factory status</pre>

   * Look at the output of ps to see that APF is running under the expected user. 
     This should show who it is running as, and the arguments in 
     =/etc/sysconfig/factory=: 

<pre class="rootscreen">
%UCL_PROMPT_ROOT% ps aux | grep autofactory | grep -v grep
502       6624  0.1  0.0 721440 12392 pts/0    Sl   Oct20   2:04 /usr/bin/python /usr/bin/autopyfactory --conf /etc/autopyfactory/autopyfactory.conf --info --sleep=60 --runas=autopyfactory --log=/var/log/autopyfactory/autopyfactory.log
</pre>

   *  Tail the log output and look for problems.

 <pre class="rootscreen">
%UCL_PROMPT_ROOT% tail -f /var/log/autopyfactory/autopyfactory.log</pre>
 
   * Check to be sure jobs are being submitted by whatever account APF is using by executing condor_q manually:  

 <pre class="rootscreen">
%UCL_PROMPT_ROOT% condor_q | grep autopyfactory</pre>


---# Configuration

---## Format of the configuration files

The format of the configuration files is similar to the Microsoft INI files. The configuration file consists of sections, led by a =[section]= header and followed by =name: value= entries, with continuations in the style of [[http://tools.ietf.org/html/rfc822.html][RFC 822]] (see section 3.1.1, “LONG HEADER FIELDS”); =name=value= is also accepted. Note that leading whitespace is removed from values. Additional defaults can be provided on initialization and retrieval. Lines beginning with '#' or ';' are ignored and may be used to provide comments.

It accepts interpolation. This means values can contain format strings which refer to other values in the same section, or values in a special =[DEFAULT]= section.

Configuration files may include comments, prefixed by specific characters (# and ;). Comments may appear on their own in an otherwise empty line, or may be entered in lines holding values or section names. In the latter case, they need to be preceded by a whitespace character to be recognized as a comment. (For backwards compatibility, only ; starts an inline comment, while # does not.)

As the python package !ConfigParser is being used to digest the configuration files, a wider explanation and examples can be found in the [[https://docs.python.org/2/library/configparser.html][python documentation page]]. 

---## sysconfig/autopyfactory

The first set of configuration for the factory is done via the =/etc/sysconfig/autopyfactory= config file. 
This is the configuration file used by the daemon service to decide how to run the factory. 
It set the following variables:
   * log level: *WARNING*, *INFO* or *DEBUG*
   * time to sleep between internal cycles
   * the username to switch into to. Note the factory will not run as root, but as a non priviledged account
   * the log file
Also, any other modifications to the environment needed by the factory can be set in this file.
Example

<pre class="file">
OPTIONS="--debug --sleep=60 --runas=autopyfactory --log=/var/log/autopyfactory/autopyfactory.log"
CONSOLE_LOG=/var/log/autopyfactory/console.log
env | sort >> $CONSOLE_LOG
</pre>

---## autopyfactory.conf
<!--
*FIXME: check which variables are really mandatory and which ones are optional*

*FIXME: fix the format on the multiline cells*

*FIXME: fix the fake wiki words*
-->

| *Variable* | *Description* |
| =baseLogDir= | where outputs from pilots are stored NOTE: No trailing '/'!!! |
| =baseLogDirUrl= |where outputs from pilots are available via http.  NOTE: It must include the port.  NOTE: No trailing '/'!!! |
| =batchstatus.condor.sleep= |time the Condor !BatchStatus Plugin waits between cycles Value is in seconds. |
| =batchstatus.maxtime= |maximum time while the info is considered reasonable.  If info stored is older than that, is considered not valid, and some NULL output will be returned. |
| =cycles= |maximum number of times the queues will loop.  None means forever. |
| =cleanlogs.keepdays= |maximum number of days the condor logs will be kept, in case they are placed in a subdirectory for an APFQueue that is not being currently managed by !AutoPyFactory.  For example, an apfqueue that has been created and used for a short amount of time, and it does not exist anymore.  Still the created logs have to be cleaned at some point... |
| =enablequeues= |default value to enable/disable all queues at once.  When True, its value will be overriden by the queue config variable 'enabled', queue by queue.  When False, all queues will stop working, but the factory will still be alive performing basic actions (eg. printing logs). |
| =factoryId= |Name that the factory instance will have in the APF web monitor.  Make factoryId something descriptive and unique for your factory, for example <site>-<host>-<admin> (e.g. BNL-gridui11-jhover) |
| =factoryAdminEmail= |Email of the local admin to contact in case of a problem with an specific APF instance. |
| =factorySMTPServer= |Server to use to send alert emails to admin.  |
| =factory.sleep= |sleep time between cycles in mainLoop in Factory object Value is in seconds. |
| =factoryUser= |account under which APF will run |
| =maxperfactory.maximum= |maximum number of condor jobs to be running at the same time per Factory.  It is a global number, used by all APFQueues submitting pilots with condor.  The value will be used by !MaxPerFactorySchedPlugin plugin |
| =logserver.enabled= |determines if batch logs are exported via HTTP.  Valid values are True&verbar;False |
| =logserver.index= |determines if automatic directory indexing is allowed when log directories are browsed.  Valid values are True&verbar;False |
| =logserver.allowrobots= |if false, creates a robots.txt file in the docroot.  Valid valudes are True&verbar;False |
| =proxyConf= |local path to the configuration file for automatic proxy management.  NOTE: must be a local path, not a URI.  |
| =proxymanager.enabled= |to determine if automatic proxy management is used or not.  Accepted values are True&verbar;False |
| =proxymanager.sleep= | Sleep interval for proxymanager thread.  |
| =queueConf= |URI plus path to the configuration file for APF queues.  NOTE: Must be expressed as a URI (file:// or http://) Cannot be used at the same time that queueDirConf |
| =queueDirConf= |directory with a set of configuration files, all of them to be used at the same time.  i.e.  /etc/autopyfactory/queues.d/ Cannot be used at the same time that queueConf |
| =monitorConf= |local path to the configuration file for Monitor plugins. |
| =mappingsConf= |local path to the configuration file with the mappings: for example, globus2info, jobstatus2info, etc. |
| =wmsstatus.maximum= |maximum time while the info is considered reasonable.  If info stored is older than that, is considered not valid, and some NULL output will be returned. |
| =wmsstatus.panda.sleep= |time the WMSStatus Plugin waits between cycles Value is in seconds. |

---## queues.conf

<!--
*FIXME: check which variables are really mandatory and which ones are optional*

*FIXME: fix the format on the multiline cells*

*FIXME: fix the fake wiki words*
-->

| *Variable* | *Description* |
| =cloud= | is the cloud this queue is in. You should set this to suppress pilot submission when the cloud goes offline N.B. Panda clouds are UPPER CASE, e.g., UK |
| =vo= | Virtual Organization |
| =grid= | Grid middleware flavor at the site. (e.g. OSG, EGI, !NorduGrid)  |
| =batchqueue= | the Batch system related queue name.  E.g. the !PanDA queue name (formerly called nickname) |
| =wmsqueue= | the WMS system queue name.  E.g. the !PanDA siteid name |
| =enabled= | determines if each queue section must be used by !AutoPyFactory or not. Allows to disable a queue without commenting out all the values.  Valid values are True&verbar;False. |
| =status= | can be "test", "offline" or "online" |
| =apfqueue.sleep= | sleep time between cycles in APFQueue object.  Value is in seconds.    |
| =cleanlogs.keepdays= | maximum number of days the condor logs will be kept |
|  WMS Status Plugin variables  ||
| =wmsstatusplugin= | WMS Status Plugin. |
| =wmsstatus.condor.queryargs= | list of command line input options to be included in the query command *verbatim*. E.g.  wmsstatus.condorqueryargs = -name <schedd_name> ...  |
|  Batch Status Plugin variables  ||
| =batchstatusplugin= | Batch Status Plugin. |
| =batchstatus.condor.queryargs= | list of command line input options to be included in the query command *verbatim*. E.g.  batchstatus.condor.queryargs = -name <schedd_name> -pool <centralmanagerhostname[:portnumber]> |
|  Sched Plugin variables  ||
| =schedplugin= | specific Scheduler Plugin implementing the algorithm deciding how many new pilots to submit next cycle.  The value can be a single Plugin or a split by comma list of Plugins.  In the case of more than one plugin, each one will acts as a filter with respect to the value returned by the previous one.  By selecting the right combination of Plugins in a given order, a complex algorithm can be built. <BR> E.g., the algorithm can start by using Ready Plugin, which will determine the number of pilots based on the number of activated jobs in the WMS queue and the number of already submitted pilots.  After that, this number can be filtered to a maximum (!MaxPerCycleSchedPlugin) or a minimum (!MinPerCycleSchedPlugin) number of pilots.  Or even can be filtered to a maximum number of pilots per factory (!MaxPerFactorySchedPlugin) Also it can be filtered depending on the status of the wmsqueue (!StatusTestSchedPlugin, !StatusOfflineSchedPlugin). |
|  Configuration when schedplugin is Ready  ||
| =sched.ready.offset= | the minimum value in the number of ready jobs to trigger submission. |
|  Configuration when schedplugin is Fixed   ||
| =sched.fixed.pilotspercycle= | fixed number of pilots to be submitted each cycle, when using the Fixed Scheduler Plugin. |
|  Configuration when schedplugin is !MaxPerCycle  ||
| =sched.maxpercycle.maximum= | maximum number of pilots to be submitted per cycle |
|  Configuration when schedplugin is !MinPerCycle  ||
| =sched.minpercycle.minimum= | minimum number of pilots to be submitted per cycle |
|  Configuration when schedplugin is !MaxPending  ||
| =sched.maxpending.maximum= | maximum number of pilots to be pending |
|  Configuration when schedplugin is !MinPending  ||
| =sched.minpending.minimum= | minimum number of pilots to be pending |
|  Configuration when schedplugin is !MaxToRun  ||
| =sched.maxtorun.maximum= | maximum number of pilots allowed to, potentially, be running at a time.  |
|  Configuration when schedplugin is !StatusTest  ||
| =sched.statustest.pilots= | number of pilots to submit when the wmsqueue is in status = test |
|  Configuration when schedplugin is !StatusOffline  ||
| =sched.statusoffline.pilots= | number of pilots to submit when the wmsqueue or the cloud is in status = offline |
|  Configuration when schedplugin is Simple  ||
| =sched.simple.default= | default number of pilots to be submitted when the context information does not exist or is not reliable.  To be used in Simple Scheduler Plugin. |
| =sched.simple.maxpendingpilots= | maximum number of pilots to be idle on queue waiting to start execution.  To be used in Simple Scheduler Plugin. |
| =sched.simple.maxpilotspercycle= | maximum number of pilots to be submitted per cycle.  To be used in Simple Scheduler Plugin. |
|  Configuration when schedplugin is Trivial  ||
| =sched.trivial.default= | default number of pilots to be submitted when the context information does not exist or is not reliable.  To be used in Trivial Scheduler Plugin. |
|  Configuration when schedplugin is Scale  ||
| =sched.scale.factor= | scale factor to correct the previous value of the number of pilots.  Value is a float number. |
|  Configuration when schedplugin is !KeepNRunning  ||
| =sched.keepnrunning.keep_running= | number of total jobs to keep running and/or pending |
|  Batch Submit Plugin variables  ||
| =batchsubmitplugin= | Batch Submit Plugin.  Currently available options are: !CondorGT2, !CondorGT5, !CondorCREAM, !CondorLocal, !CondorLSF, !CondorEC2, !CondorDeltaCloud. |
|  Configuration when batchsubmitplugin is condorgt2   ||
| =batchsubmit.condorgt2.gridresource= | name of the CE (e.g. gridtest01.racf.bnl.gov/jobmanager-condor) |
| =batchsubmit.condorgt2.submitargs= | list of command line input options to be included in the submission command *verbatim* e.g.  batchsubmit.condorgt2.submitargs = -remote my_schedd will drive into a command like condor_submit -remote my_schedd submit.jdl |
| =batchsubmit.condorgt2.condor_attributes= | list of condor attributes, splited by comma, to be included in the condor submit file *verbatim* <BR>e.g. +Experiment = "ATLAS",+VO = "usatlas",+Job_Type = "cas" Can be used to include any line in the Condor-G file that is not otherwise added programmatically by !AutoPyFactory.  Note the following directives are added by default: <BR>transfer_executable=True <BR>stream_output=False <BR>stream_error=False <BR>notification=Error <BR>copy_to_spool=false |
| =batchsubmit.condorgt2.environ= | list of environment variables, splitted by white spaces, to be included in the condor attribute environment *verbatim* Therefore, the format should be env1=var1 env2=var2 envN=varN split by whitespaces. |
| =batchsubmit.condorgt2.proxy= | name of the proxy handler in proxymanager for automatic proxy renewal (See etc/proxy.conf) None if no automatic proxy renewal is desired. |
|  !GlobusRSL GRAM2 variables  ||
| =gram2= | The following are GRAM2 RSL variables.  They are just used to build batchsubmit.condorgt2.globusrsl (if needed) The globusrsl directive in the condor submission file looks like globusrsl=(jobtype=single)(queue=short) Documentation can be found here: http://www.globus.org/toolkit/docs/2.4/gram/gram_rsl_parameters.html |
| =globusrsl.gram2.&#60;argument&#62;= | globusrsl.gram2.count <BR> globusrsl.gram2.directory <BR> globusrsl.gram2.dryRun <BR> globusrsl.gram2.environment <BR> globusrsl.gram2.executable <BR> globusrsl.gram2.gramMyJob <BR> globusrsl.gram2.hostCount <BR> globusrsl.gram2.jobType <BR> globusrsl.gram2.maxCpuTime <BR> globusrsl.gram2.maxMemory <BR> globusrsl.gram2.maxTime <BR> globusrsl.gram2.maxWallTime <BR> globusrsl.gram2.minMemory <BR> globusrsl.gram2.project <BR> globusrsl.gram2.queue <BR> globusrsl.gram2.remote_io_url <BR> globusrsl.gram2.restart <BR> globusrsl.gram2.save_state <BR> globusrsl.gram2.stderr <BR> globusrsl.gram2.stderr_position <BR> globusrsl.gram2.stdin <BR> globusrsl.gram2.stdout <BR> globusrsl.gram2.stdout_position <BR> globusrsl.gram2.two_phase |
| =globusrsl.gram2.globusrsl= | GRAM RSL directive.  If this variable is not setup, then it will be built programmatically from all non empty globusrsl.gram2.XYZ variables.  If this variable is setup, then its value will be taken *verbatim*, and all possible values for globusrsl.gram2.XYZ variables will be ignored.  |
| =globusrsl.gram2.globusrsladd= | custom fields to be added *verbatim* to the GRAM RSL directive, after it has been built either from globusrsl.gram2.globusrsl value or from all globusrsl.gram2.XYZ variables.  e.g. (condorsubmit=('+AccountingGroup' '\"group_atlastest.usatlas1\"')('+Requirements' 'True')) |
|  Configuration when batchsubmitplugin is condorgt5  ||
| =batchsubmit.condorgt5.gridresource= | name of the CE (e.g. gridtest01.racf.bnl.gov/jobmanager-condor) |
| =batchsubmit.condorgt5.submitargs= | list of command line input options to be included in the submission command *verbatim* e.g.  batchsubmit.condorgt2.submitargs = -remote my_schedd will drive into a command like condor_submit -remote my_schedd submit.jdl |
| =batchsubmit.condorgt5.condor_attributes= | list of condor attributes, splited by comma, to be included in the condor submit file *verbatim* e.g. +Experiment = "ATLAS",+VO = "usatlas",+Job_Type = "cas" Can be used to include any line in the Condor-G file that is not otherwise added programmatically by !AutoPyFactory.  Note the following directives are added by default: <BR>transfer_executable=True <BR>stream_output=False <BR>stream_error=False <BR>notification=Error <BR>copy_to_spool=false |
| =batchsubmit.condorgt5.environ= | list of environment variables, splitted by white spaces, to be included in the condor attribute environment *verbatim* Therefore, the format should be env1=var1 env2=var2 envN=varN split by whitespaces. |
| =batchsubmit.condorgt5.proxy= | name of the proxy handler in proxymanager for automatic proxy renewal (See etc/proxy.conf) None if no automatic proxy renewal is desired. |
|  !GlobusRSL GRAM5 variables  ||
| =gram5= | The following are GRAM5 RSL variables.  They are just used to build batchsubmit.condorgt5.globusrsl (if needed) The globusrsl directive in the condor submission file looks like globusrsl=(jobtype=single)(queue=short) Documentation can be found here: http://www.globus.org/toolkit/docs/5.2/5.2.0/gram5/user/#gram5-user-rsl  |
| =globusrsl.gram5.&#60;argument&#62;= | globusrsl.gram5.count <BR> globusrsl.gram5.directory <BR> globusrsl.gram5.dry_run <BR> globusrsl.gram5.environment <BR> globusrsl.gram5.executable <BR> globusrsl.gram5.file_clean_up <BR> globusrsl.gram5.file_stage_in <BR> globusrsl.gram5.file_stage_in_shared <BR> globusrsl.gram5.file_stage_out <BR> globusrsl.gram5.gass_cache <BR> globusrsl.gram5.gram_my_job <BR> globusrsl.gram5.host_count <BR> globusrsl.gram5.job_type <BR> globusrsl.gram5.library_path <BR> globusrsl.gram5.loglevel <BR> globusrsl.gram5.logpattern <BR> globusrsl.gram5.max_cpu_time <BR> globusrsl.gram5.max_memory <BR> globusrsl.gram5.max_time <BR> globusrsl.gram5.max_wall_time <BR> globusrsl.gram5.min_memory <BR> globusrsl.gram5.project <BR> globusrsl.gram5.proxy_timeout <BR> globusrsl.gram5.queue <BR> globusrsl.gram5.remote_io_url <BR> globusrsl.gram5.restart <BR> globusrsl.gram5.rsl_substitution <BR> globusrsl.gram5.savejobdescription <BR> globusrsl.gram5.save_state <BR> globusrsl.gram5.scratch_dir <BR> globusrsl.gram5.stderr <BR> globusrsl.gram5.stderr_position <BR> globusrsl.gram5.stdin <BR> globusrsl.gram5.stdout <BR> globusrsl.gram5.stdout_position <BR> globusrsl.gram5.two_phase <BR> globusrsl.gram5.username |
| =globusrsl.gram5.globusrsl= | GRAM RSL directive.  If this variable is not setup, then it will be built programmatically from all non empty globusrsl.gram5.XYZ variables.  If this variable is setup, then its value will be taken *verbatim*, and all possible values for globusrsl.gram5.XYZ variables will be ignored.  |
| =globusrsl.gram5.globusrsladd= | custom fields to be added *verbatim* to the GRAM RSL directive, after it has been built either from globusrsl.gram5.globusrsl value or from all globusrsl.gram5.XYZ variables.  e.g. (condorsubmit=('+AccountingGroup' '\"group_atlastest.usatlas1\"')('+Requirements' 'True')) |
|  Configuration when batchsubmitplugin is condorcream  ||
| =batchsubmit.condorcream.webservice= | web service address (e.g. ce04.esc.qmul.ac.uk:8443/ce-cream/services/CREAM2) |
| =batchsubmit.condorcream.submitargs= | list of command line input options to be included in the submission command *verbatim* e.g.  batchsubmit.condorgt2.submitargs = -remote my_schedd will drive into a command like condor_submit -remote my_schedd submit.jdl |
| =batchsubmit.condorcream.condor_attributes= | list of condor attributes, splited by comma, to be included in the condor submit file *verbatim* e.g. +Experiment = "ATLAS",+VO = "usatlas",+Job_Type = "cas" Can be used to include any line in the Condor-G file that is not otherwise added programmatically by !AutoPyFactory.  Note the following directives are added by default: <BR>transfer_executable=True <BR>stream_output=False <BR>stream_error=False <BR>notification=Error <BR>copy_to_spool=false |
| =batchsubmit.condorcream.environ= | list of environment variables, splitted by white spaces, to be included in the condor attribute environment *verbatim* Therefore, the format should be env1=var1 env2=var2 envN=varN split by whitespaces. |
| =batchsubmit.condorcream.queue= | queue within the local batch system (e.g. short) |
| =batchsubmit.condorcream.port= | port number. |
| =batchsubmit.condorcream.batch= | local batch system (pbs, sge...) |
| =batchsubmit.condorcream.gridresource= | grid resource, built from other vars using interpolation: batchsubmit.condorcream.gridresource = %(batchsubmit.condorcream.webservice)s:%(batchsubmit.condorcream.port)s/ce-cream/services/CREAM2 %(batchsubmit.condorcream.batch)s %(batchsubmit.condorcream.queue)s |
| =batchsubmit.condorcream.proxy= | name of the proxy handler in proxymanager for automatic proxy renewal (See etc/proxy.conf) None if no automatic proxy renewal is desired. |
|  Configuration when batchsubmitplugin is condorosgce  ||
| =batchsubmit.condorosgce.remote_condor_schedd= | condor schedd  |
| =batchsubmit.condorosgce.remote_condor_collector= | condor collector |
| =batchsubmit.condorosgce.gridresource= | to be used in case schedd and collector are the same |
| =batchsubmit.condorosgce.proxy= | name of the proxy handler in proxymanager for automatic proxy renewal (See etc/proxy.conf) None if no automatic proxy renewal is desired. |
| =batchsubmit.condorosgce.condor_attributes= | list of condor attributes, splited by comma, to be included in the condor submit file *verbatim* |
|  Configuration when batchsubmitplugin is condorec2  ||
| =batchsubmit.condorec2.gridresource= | ec2 service's URL (e.g. https://ec2.amazonaws.com/ ) |
| =batchsubmit.condorec2.submitargs= | list of command line input options to be included in the submission command *verbatim* e.g.  batchsubmit.condorgt2.submitargs = -remote my_schedd will drive into a command like condor_submit -remote my_schedd submit.jdl |
| =batchsubmit.condorec2.condor_attributes= | list of condor attributes, splited by comma, to be included in the condor submit file *verbatim* |
| =batchsubmit.condorec2.environ= | list of environment variables, splitted by white spaces, to be included in the condor attribute environment *verbatim* Therefore, the format should be env1=var1 env2=var2 envN=varN split by whitespaces. |
| =batchsubmit.condorec2.ami_id= | identifier for the VM image, previously registered in one of Amazon's storage service (S3 or EBS) |
| =batchsubmit.condorec2.ec2_spot_price= | max price to pay, in dollars to three decimal places. e.g. .040 |
| =batchsubmit.condorec2.instance_type= | hardware configurations for instances to run on, .e.g m1.medium |
| =batchsubmit.condorec2.user_data= | up to 16Kbytes of contextualization data.  This makes it easy for many instances to share the same VM image, but perform different work. |
| =batchsubmit.condorec2.access_key_id= | path to file with the EC2 Access Key ID |
| =batchsubmit.condorec2.secret_access_key= | path to file with the EC2 Secret Access Key |
| =batchsubmit.condorec2.proxy= | name of the proxy handler in proxymanager for automatic proxy renewal (See etc/proxy.conf) None if no automatic proxy renewal is desired. |
|  Configuration when batchsubmitplugin is condordeltacloud  ||
| =batchsubmit.condordeltacloud.gridresource= | ec2 service's URL (e.g. https://deltacloud.foo.org/api ) |
| =batchsubmit.condordeltacloud.username= | credentials in !DeltaCloud |
| =batchsubmit.condordeltacloud.password_file= | path to the file with the password |
| =batchsubmit.condordeltacloud.image_id= | identifier for the VM image, previously registered with the cloud service. |
| =batchsubmit.condordeltacloud.keyname= | in case of using SSH, the command keyname specifies the identifier of the SSH key pair to use.  |
| =batchsubmit.condordeltacloud.realm_id= | selects one between multiple locations the cloud service may have. |
| =batchsubmit.condordeltacloud.hardware_profile= | selects one between the multiple hardware profiles the cloud service may provide |
| =batchsubmit.condordeltacloud.hardware_profile_memory= | customize the hardware profile |
| =batchsubmit.condordeltacloud.hardware_profile_cpu= | customize the hardware profile |
| =batchsubmit.condordeltacloud.hardware_profile_storage= | customize the hardware profile |
| =batchsubmit.condordeltacloud.user_data= | contextualization data |
|  Configuration when batchsubmitplugin is condorlocal  ||
| =batchsubmit.condorlocal.submitargs= | list of command line input options to be included in the submission command *verbatim* e.g.  batchsubmit.condorgt2.submitargs = -remote my_schedd will drive into a command like condor_submit -remote my_schedd submit.jdl |
| =batchsubmit.condorlocal.condor_attributes= | list of condor attributes, splited by comma, to be included in the condor submit file *verbatim* e.g. +Experiment = "ATLAS",+VO = "usatlas",+Job_Type = "cas" Can be used to include any line in the Condor-G file that is not otherwise added programmatically by !AutoPyFactory.  Note the following directives are added by default: <BR>universe = vanilla <BR>transfer_executable = True <BR>should_transfer_files = IF_NEEDED <BR>+TransferOutput = "" <BR>stream_output=False <BR>stream_error=False <BR>notification=Error <BR>periodic_remove = (!JobStatus == 5 && (!CurrentTime - !EnteredCurrentStatus) > 3600) &verbar;&verbar; (!JobStatus == 1 && globusstatus =!= 1 && (!CurrentTime - !EnteredCurrentStatus) > 86400) <BR>To be used in !CondorLocal Batch Submit Plugin. |
| =batchsubmit.condorlocal.environ= | list of environment variables, splitted by white spaces, to be included in the condor attribute environment *verbatim* To be used by !CondorLocal Batch Submit Plugin.  Therefore, the format should be env1=var1 env2=var2 envN=varN split by whitespaces. |
| =batchsubmit.condorlocal.proxy= | name of the proxy handler in proxymanager for automatic proxy renewal (See etc/proxy.conf) None if no automatic proxy renewal is desired. |
|  Configuration when batchsubmitplugin is condorlsf  ||
| =batchsubmit.condorlsf.proxy= | name of the proxy handler in proxymanager for automatic proxy renewal (See etc/proxy.conf) None if no automatic proxy renewal is desired. |
|  Configuration when batchsubmitplugin is nordugrid  ||
| =batchsubmit.condornordugrid.gridresource= | name of the ARC CE i.e. lcg-lrz-ce2.grid.lrz.de |
| =nordugridrsl= | Entire RSL line.  i.e. (jobname = 'prod_pilot')(queue=lcg)(runtimeenvironment = APPS/HEP/ATLAS-SITE-LCG)(runtimeenvironment = ENV/PROXY ) (environment = ('APFFID' 'voatlas94') ('PANDA_JSID' 'voatlas94') ('GTAG' 'http://voatlas94.cern.ch/pilots/2012-11-19/LRZ-LMU_arc/$(Cluster).$(Process).out') ('RUCIO_ACCOUNT' 'pilot') ('APFCID' '$(Cluster).$(Process)') ('APFMON' 'http://apfmon.lancs.ac.uk/mon/') ('FACTORYQUEUE' 'LRZ-LMU_arc')  |
| =nordugridrsladd= | A given tag to be added to the Nordugrid RSL line |
| =nordugridrsl.addenv.<XYZ>= | A given tag to be added within the 'environment' tag to the Nordugrid RSL line i.e. nordugridrsl.addenv.RUCIO_ACCOUNT = pilot will be added as ('RUCIO_ACCOUNT' 'pilot' ) |

  Monitor Section 


| *Variable* | *Description* |
| =monitorsection= | section in monitor.conf where info about the actual monitor plugin can be found.  The value can be a single section or a split by comma list of sections.  Monitor plugins handle job info publishing to one or more web monitor/dashboards.  To specify more than one (sections) simply use a comma-separated list.    |

  Executable variables 

As the purpose of the factory is to submit jobs to the different resources (local batch queues, grid sites, etc.) an executable, with its corresponding list of input arguments, is needed.
This executable can be anything.
Details on how to install the executable and the list of arguments are out of the scope of this documentation.

| *Variable* | *Description* |
| =executable= | path to the script which will be run by condor. |
| =executable.arguments= | input options to be passed verbatim to the executable script. |

<!--
 This variable can be built making use of an auxiliar variable called executable.defaultarguments This proposed ancilla works as a template, and its content is created on the fly from the value of other variables.  This mechanism is called "interpolation", docs can be found here: http://docs.python.org/library/configparser.html 
An example of this type of templates (included in the DEFAULTS block) is like this: executable.defaultarguments = --wrappergrid=%(grid)s \ --wrapperwmsqueue=%(wmsqueue)s \ --wrapperbatchqueue=%(batchqueue)s \ --wrappervo=%(vo)s \ --wrappertarballurl=http://dev.racf.bnl.gov/dist/wrapper/wrapper.tar.gz \ --wrapperserverurl=http://pandaserver.cern.ch:25080/cache/pilot \ --wrapperloglevel=debug executable.defaultarguments =  -s %(wmsqueue)s \ -h %(batchqueue)s -p 25443 \ -w https://pandaserver.cern.ch  -j false  -k 0  -u user |
-->

---## proxy.conf

<!--
*FIXME: check which variables are really mandatory and which ones are optional*

*FIXME: fix the format on the multiline cells*

*FIXME: fix the fake wiki words*
-->

| *Variable* | *Description* |
| =baseproxy= | If used, create a very long-lived proxy, e.g. grid-proxy-init -valid 720:0 -out /tmp/plainProxy Note that maintenance of this proxy must occur completely outside of APF. |
| =proxyfile= | Target proxy path.|
| =lifetime= | Initial lifetime, in seconds (604800 = 7 days)|
| =checktime= | How often to check proxy validity, in seconds|
| =minlife= | Minimum lifetime of VOMS attributes for a proxy (renew if less) in seconds|
| =interruptcheck= | Frequency to check for keyboard/signal interrupts, in seconds|
| =renew= | If you do not want to use !ProxyManager to renew proxies, set this False and only define 'proxyfile' If renew is set to false, then no grid client setup is necessary. |
| =usercert= | Path to the user grid certificate file|
| =userkey= | Path to the user grid key file|
| =vorole= | User VO role for target proxy. !MyProxy Retrieval Functionality: Assumes you have created a long-lived proxy in a !MyProxy server, out of band. |
| =flavor= | voms or myproxy. voms directly generates proxy using cert or baseproxy myproxy retrieves a proxy from myproxy, then generates the target proxy against voms using it as baseproxy.|
| =myproxy_hostname= | Myproxy server host. |
| =myproxy_username= | User name to be used on !MyProxy service|
| =myproxy_passphrase= | Passphrase for proxy retrieval from !MyProxy|
| =retriever_profile= | A list of other proxymanager profiles to be used to authorize proxy retrieval from !MyProxy. |
| =initdelay= | In seconds, how long to wait before generating. Needed for !MyProxy when using cert authentication--we need to allow time for the auth credential to be generated (by another proxymanager profile). |
| =owner= | If running standalone (as root) and you want the proxy to be owned by another account. |
| =Remote= | Proxy Maintenance: Assumes you have enabled ssh-agent key-based access to the remote host where you want to maintain a proxy file. |
| =remote_host= | copy proxyfile to same path on remote host |
| =remote_user= | User to connect as? |
| =remote_owner= | If connect user is root, what account should own the file? |
| =remote_group= | If connect user is root, what group should own the file? |
| =voms.args= | Any extra arbitrary input option to be added to voms-proxy-init command|

---## monitor.conf

<!--
*FIXME: check which variables are really mandatory and which ones are optional*

*FIXME: fix the format on the multiline cells*

*FIXME: fix the fake wiki words*
-->

| *Variable* | *Description* |
| =monitorplugin= | the type of plugin to handle this monitor instance |
| =monitorURL= | URL for the web monitor |



---# Relase notes
---## Relase semantics

!AutoPyFactory release versions are composed by 4 numbers:
<pre>
        major.minor.release-build
</pre>
For example: 1.2.3-4

   * A change in the major number means the entire architecture of 
  !AutoPyFactory has been redesign. It implies a change at the conceptual
  level. In other words, changing the major number means a new project. 

   * A change in the configuration files that requires sys admins intervention
  after updating implies a change in the minor number.

   * Implementing a new relevant feature implies changing the minor number.

   * A significative amount of code refactoring that may affect the performance
  of !AutoPyFactory -including speed, memory usage, disk usage, etc-
  implies changing the minor number.

   * Small features and bug fixes imply changing the release number. 

   * A change in the RPM package but not in the code are reflected in the
  build number.

   * Not all new releases are placed in the production RPM repository. 
  Many of them are available at the development and testing repositories, 
  but only those that have been verified to work are moved 
  to the production repository. 

   * Same RPM will have always the same numbers in all repositories. 

---### version 2.4.0


Version 2.4.0 introduces major changes in name of files and directories, 
programs, users accounts, processes, etc. 
This recipe should help with a step by step migration:
        
 1. stop the factory

<pre class="rootscreen">
%UCL_PROMPT_ROOT% service factory stop
</pre>
  
 2. install RPM for autopyfactory-2.4.0
        
first, it is needed to removed the previous installation and the one for autopyfactory-tools, 
if it is installed, since there are incompatible requirements
This will delete some files and directories. If you customized the logrotation, you may want to make a copy first.
Also make a security copy of the configuration directory
 
<pre class="rootscreen">
%UCL_PROMPT_ROOT% cp /etc/logrotate.d/autopyfactory.logrotate /tmp/
%UCL_PROMPT_ROOT% cp /etc/sysconfig/factory.sysconfig /tmp/
%UCL_PROMPT_ROOT% mkdir /tmp/etc/
%UCL_PROMPT_ROOT% cp /etc/apf/* /tmp/etc/
</pre>
       
Remove the old packages:
  
<pre class="rootscreen">
%UCL_PROMPT_ROOT% rpm -e panda-autopyfactory panda-autopyfactory-tools
</pre>        

Install the new autopyfactory package:

<pre class="rootscreen">
%UCL_PROMPT_ROOT% yum install autopyfactory
</pre>


3. Several directories have been created. 

Directory =/etc/autopyfactory/= has been created, but it is empty. 

The examples for the config files are placed under =/usr/share/doc/autopyfactory-2.4.0/=.
 
Also the examples for logrotate and sysconfig files are in  =/usr/share/doc/autopyfactory-2.4.0/logrotate/= and =/usr/share/doc/autopyfactory-2.4.0/sysconfig/=
        
The old config files are still under =/etc/apf/= and the old sysconfig is still as =/etc/sysconfig/factory.sysconfig=
        
4. sysconfig
        
Option 1: copy the new one

<pre class="rootscreen">
%UCL_PROMPT_ROOT% cp /usr/share/doc/autopyfactory-2.4.0/sysconfig/autopyfactory-example /etc/sysconfig/autopyfactory
</pre>
        
Option 2: copy the old one
   
<pre class="rootscreen">
%UCL_PROMPT_ROOT% cp /tmp/factory.sysconfig  /etc/sysconfig/autopyfactory
</pre>
        
In the second case, some adjustments may be needed:
     
   *  replace      --runas=apf                           -->  --runas=autopyfactory
   *  replace      --log=/var/log/apf/apf.log            -->  --log=/var/log/autopyfactory/autopyfactory.log
   *  replace      CONSOLE_LOG=/var/log/apf/console.log  -->  CONSOLE_LOG=/var/log/autopyfactory/console.log       
 

5. log rotation
        
Option 1: copy the new one
        
<pre class="rootscreen">
%UCL_PROMPT_ROOT% cp /usr/share/doc/autopyfactory-2.4.0/logrotate/autopyfactory-example /etc/logrotate.d/autopyfactory
</pre>

Option 2: copy the old one (saved in /tmp/)

<pre class="rootscreen">
%UCL_PROMPT_ROOT% cp /tmp/autopyfactory.logrotate /etc/logrotate.d/autopyfactory
</pre>
        
In the second case, some adjustments may be needed:
        
   * replace       /var/log/apf/apf.log         -->     /var/log/autopyfactory/autopyfactory.log
   * replace       /var/log/apf/console.log     -->     /var/log/autopyfactory/console.log
   * replace       /etc/init.d/factory          -->     /etc/init.d/autopyfactory
        
6. autopyfactory.conf
        
Option 1: copy the new one
        
<pre class="rootscreen">
%UCL_PROMPT_ROOT% cp /usr/share/doc/autopyfactory-2.4.0/autopyfactory.conf-example  /etc/autopyfactory/autopyfactory.conf
</pre>
        
Option 2: copy the old one
        
<pre class="rootscreen">
%UCL_PROMPT_ROOT% cp /tmp/etc/factory.conf /etc/autopyfactory/autopyfactory.conf
</pre>
        
In the second case, some adjustments may be needed:
        
   * replace        factoryUser = apf                           -->  factoryUser = autopyfactory
   * replace        queueConf = file:///etc/apf/queues.conf     -->  queueConf = file:///etc/autopyfactory/queues.conf
   * replace        queueDirConf = /etc/apf/queues.d/           -->  queueDirConf = /etc/autopyfactory/queues.d/
   * replace        proxyConf = /etc/apf/proxy.conf             -->  proxyConf = /etc/autopyfactory/proxy.conf
   * replace        monitorConf = file:///etc/apf/monitor.conf  -->  monitorConf = /etc/autopyfactory/monitor.conf
   * replace        baseLogDir = /home/apf/factory/logs         -->  baseLogDir = /home/autopyfactory/factory/logs
   * add line                        mappingsConf = /etc/autopyfactory/mappings.conf
        
7. queues.conf
        
Option 1: copy the new one

<pre class="rootscreen">
%UCL_PROMPT_ROOT% cp /usr/share/doc/autopyfactory-2.4.0/queues.conf-example  /etc/autopyfactory/queues.conf
</pre>        

Option 2: copy the old one
        
<pre class="rootscreen">
%UCL_PROMPT_ROOT% cp /tmp/etc/queues.conf /etc/autopyfactory/queues.conf
</pre>
        
In the first case, the file needs to be configured from scratch.
In the second case, no adjustments is needed.
        
8. proxy.conf
        
Option 1: copy the new one
        
<pre class="rootscreen">
%UCL_PROMPT_ROOT% cp /usr/share/doc/autopyfactory-2.4.0/proxy.conf-example  /etc/autopyfactory/proxy.conf
</pre>
        
Option 2: copy the old one
        
<pre class="rootscreen">
%UCL_PROMPT_ROOT% cp /tmp/etc/proxy.conf /etc/autopyfactory/proxy.conf
</pre>
        
In the first case, the file needs to be configured from scratch.
In the second case, no adjustments is needed.
        
9. monitor.conf
        
Option 1: copy the new one
        
<pre class="rootscreen">
%UCL_PROMPT_ROOT% cp /usr/share/doc/autopyfactory-2.4.0/monitor.conf-example  /etc/autopyfactory/monitor.conf
</pre>
        
Option 2: copy the old one
        
<pre class="rootscreen">
%UCL_PROMPT_ROOT% cp /tmp/etc/monitor.conf /etc/autopyfactory/monitor.conf
</pre>
        
In the first case, the file needs to be configured from scratch, but most probably the default configuration is enough.
In the second case, no adjustments is needed.
        
10. mappings.conf
        
This config file is new, so it must be copied 
        
<pre class="rootscreen">
%UCL_PROMPT_ROOT% cp /usr/share/doc/autopyfactory-2.4.0/mappings.conf-example  /etc/autopyfactory/mappings.conf
</pre>        
        
Do not touch it.
        
11. since the factory will now run as user =autopyfactory= instead of =apf=, the new UNIX account needs to be created, if not already done during the RPM install process.
        
<pre class="rootscreen">
%UCL_PROMPT_ROOT% useradd autopyfactory
</pre>
        
assuming that account also hosts the keys for the X509 proxies in the regular directory .globus:

<pre class="rootscreen">
%UCL_PROMPT_ROOT% mkdir ~autopyfactory/.globus
%UCL_PROMPT_ROOT% cp -r ~apf/.globus/* ~autopyfactory/.globus/
%UCL_PROMPT_ROOT% chown -R autopyfactory:autopyfactory ~autopyfactory/.globus
</pre>
        
if that is not the case, then copy the .pem keys and/or change their ownership.
And to avoid problems, delete the current X509 that may still be in =/tmp/=
        
12. Install autopyfactory-tools
        
<pre class="rootscreen">
%UCL_PROMPT_ROOT% yum install autopyfactory-tools
</pre>        

13. start the factory
        
<pre class="rootscreen">
%UCL_PROMPT_ROOT% service autofactory start
</pre>
        
*WARNING*: after migrating to 2.4.0, factory runs under user =autopyfactory= instead of =apf=.
That means no one will now clean the old directories =/var/log/apf/= and =~apf/factory/logs/=
You may delete them at some point (not right away, since they will include condor logs for still running pilots)
        
---### version 2.3.2

   * RPM built incorrectly. Rebuilt against correct copy of code.  

---### version 2.3.0

   * utils not distributed anymore within the RPM. They will be distributed with a dedicated one.
   * variable 'flavor' mandatory in DEFAULT section in =proxy.conf= Values are voms or myproxy
   * In case ==flavor=myproxy== in =proxy.conf=, then these variables are needed:
      * remote_host
      * remote_user
      * remote_owner
      * remote_group
   * New variable ==factorySMTPServer== in =factory.conf=
   * New variable ==proxymanager.sleep== in =factory.conf=

---### version 2.2.0-1

   * examples of executables (a.k.a. wrappers) 
are placed underneath the =/etc/apf/= directory.
They are not copied directly into =/usr/libexec/= anymore.
   * wrapper-0.9.9.sh has a different set of
  input options than previous one. 
  Read carefully the inline documentation before using it.
   * Config plugins have been removed. 
  Any configuration variable in =queues.conf=
  related !PandaConfig or AGISConfig plugins 
  are not valid anymore. 
  Therefore the variable ==configplugin== is gone too.
   * Variables ==override== and ==autofill== are also gone.
   * There is a new configuration file called =monitor.conf=
  An example is provided underneath =/etc/apf/=
   * Old variable in =factory.conf= pointing to the monitor URL
  is now in =monitor.conf=. The =monitor.conf= contains sections
  for different monitor configurations.
  The name of the section is setup in =queues.conf= via
  the new variable ==monitorsection==. Read carefully the inline documentation in =monitor.conf= before using it.
   * Utils, including script to generate =queues.conf=
  with information from AGIS, have changed name and location.
  New scripts are place underneath =/usr/share/apf=

