{
    "docs": [
        {
            "location": "/", 
            "text": "AutoPyFactory \n\n\n 1  About this Document\n\n\n \n 2  Applicable versions\n\n\n \n 3  Description\n \n\n\n 3.1  Plugins-based architecture\n \n\n\n 3.1.1 WMS Status plugin\n\n\n \n 3.1.2  Batch Status plugin\n\n\n \n 3.1.3  Scheduler plugin\n\n\n \n 3.1.4  Batch Submit plugin\n\n\n \n\n \n\n \n 4  Deployment and Configuration\n\n\n \n 5  Development\n\n\n \n 6  AutoPyFactory Tools\n\n\n \n 7  Questions and answers\n\n\n \n 8  Contact\n\n\n \n 9  Talks and Publications\n\n\n \n\n\n\n 1  About this Document \n\n\n\nThis document describes what is AutoPyFactory (a.k.a. APF)\n\n\n\n\n\n\n\n 2  Applicable versions \n\n\n\nThis documentation applies to the latest version of APF: 2.4.9\n\n\n\n\n\n 3  Description \n\n\n\nATLAS, one of the experiments at LHC at CERN, is one of the largest users of grid computing infrastructure. As this infrastructure is now a central part of the experiment's computing operations, considerable efforts have been made to use this technology in the most efficient and effective way, including extensive use of pilot job based frameworks.\n\n\nIn this model the experiment submits 'pilot' jobs to sites without payload. When these jobs begin to run they contact a central service to pick-up a real payload to execute.\n\n\nThe first generation of pilot factories were usually specific to a single VO, and were very bound to the particular architecture of that VO. A second generation is creating factories which are more flexible, not tied to any particular VO, and provide for more features other than just pilot submission (such as monitoring, logging, profiling, etc.)\n\n\nAutoPyFactory has a modular design and is highly configurable. It is able to send different types of pilots to sites, able to exploit different submission mechanisms and different characteristics of queues at sites. It has excellent integration with the PanDA job submission framework, tying pilot flows closely to the amount of work the site has to run. It is able to gather information from many sources, in order to correctly configure itself for a site and its decision logic can easily be updated.\n\n\nIntegrated into AutoPyFactory is a very flexible system for delivering both generic and specific wrappers which can perform many useful actions before starting to run end-user scientific applications, e.g., validation of the middleware, node profiling and diagnostics, monitoring and deciding what is the best end-user application that fits the resource.\n\n\nAutoPyFactory now also has a robust monitoring system and we show how this has helped setup a reliable pilot factory service for ATLAS.\n\n\n\n 3.1  Plugins-based architecture \n\n\n\nAutoPyFactory can serve to different queues in different ways thanks to its modular design basedon plug-ins. \nPlug-ins serve two purposes. \nThey interact with the external services, like the VO WMS or the batch submission system, \nand they translate the information retrieved by those services into the internal AutoPyFactory nomenclature.\n\n\nSome of the most important plugins in AutoPyFactory are described below.\n\n\n\n 3.1.1  WMS Status plugin \n\n\n\nQueries the VO WMS system, retrieving information about the number of jobs in different status (ready, running, finished...)  per queue.\n\nThis information is converted internally into the AutoPyFactory nomenclature. \nAn example of a WMS Status plug-in queries the PanDA API. \nAnother example is a plug-in querying a local Condor pool and interpreting the output as end-user jobs. \nThis source of information is typically where how much work is ready to be done can be found, \nand therefore should trigger pilot submission.\n\n\n\n 3.1.2  Batch Status plugin \n\n\n\n\n\nQueries the batch system being used to submit the jobs (or pilots) to the grid resources, \nto determine how many previously submitted jobs are already being executed and how many are still idle.\n\nThis information is used to avoid submitting an unnecessary number of extra jobs, \nwhich could cause bottlenecks, inefficiencies, and even impose severe loads on remote Grid services. \nAn example is a module querying the Condor queues.\n\n\n\n 3.1.3  Scheduler plugin \n\n\n\nThis is the component in charge of making a decision of whether or not to submit more pilots, and if so how many. \nThat calculation is based on the information provided by the two Status plug-ins (WMSStatus and BatchStatus).\n\nIt implements a given algorithm to decide how many new jobs (or pilots) should be submitted next cycle. \nA typical algorithm calculates the number of new jobs based on the number of end-user jobs in a ready\nstatus in the VO WMS service, with some constraints to prevent the submission of an excessively\nhigh number of jobs, or to eventually keep a minimum number of submissions per cycle.\nOther SchedPlugins may embody other algorithms, e.g. a scheduler plug-in could always return a fixed number of jobs, \nor one could seek to maintain a constant number of pending/queued jobs in the batch system.\n\n\nMore than one scheduler plugins can be used combined, where the output of each one is the input for the next one in the chain.\n\n\n\n 3.1.4  Batch Submit plugin \n\nIt is the component in charge of submitting new jobs (or pilots),\nbased on the decision made by the Scheduler plug-in. \nExamples of these execution plug-ins can submit jobs remotely to a Grid resource using different protocols \n(such as GRAM2, GRAM5, or CREAM), \nto a Cloud Computing resource (using the Amazon EC2 protocol), or to a local Condor pool.\n\n\nIn theory, a submit plug-in could use other mechanisms, \ne.g. simply execute a pilot process,\nor trigger an additional VM startup locally via libvirtd. \nIn this scenario, AutoPyFactory could be run directly on the working resource (wherever the jobs are intended to run).\n\n\n\n\n\n 4  Deployment and Configuration \n\n\n \n\n\n Instructions to deploy AutoPyFactory are \nhere\n\n\n \n Instructions to configure AutoPyFactory can be found \nhere\n\n\n \n Reference manual to configure AutoPyFactory are \nhere\n\n\n \n\n\n\n 5  Development \n\n\n\nNotes on how to write new plugins can be found \nhere\n\n\n\n\n 6 AutoPyFactory Tools \n\n\n\nDocumentation on a set of utils around AutoPyFactory can be found \nhere\n\n\n\n\n\n\n 7  Questions and answers \n\n\n\nThere is a \nQ\nA\n page with some questions from users and the answer.\n\n\n\n 8  Contact \n\n\n\nThere is a mailing list, where new releases are announced and users can post questions. \n\nTo join follow \ninstructions here\n\n\n\n\n 9  Talks and Publications \n\n\n\n\nTalk at ATLAS S\nC Week, 17 October 2011 to 21 October 2011\n\n\n\n\nPoster at CHEP 2012, NY\n\n\n\n\nPaper at CHEP 2012, NY\n\n\n\n\nTalk at Open Science Grid All Hands Meeting, 2013\n\n\n\n\nPoster at CHEP 2013, Amsterdam\n\n\n\n\nTalk at the TIM at CERN, June 2016", 
            "title": "AutoPyFactory"
        }, 
        {
            "location": "/AutoPyFactoryDeployment/", 
            "text": "Deployment of AutoPyFactory \n\n\n 1  About this Document \n\n\n\n\nThis document describes how to install and configure \nAutoPyFactory\n\n\n\n\n\nConventions used in this document:\n\n\n\n\n\nA \nUser Command Line\n is illustrated by a green box that displays a prompt:\n\n\n\n\n\n  [user@client ~]$\n\n\n\n\n\nA \nRoot Command Line\n is illustrated by a red box that displays the \nroot\n prompt:\n\n\n\n\n\n  [root@factory ~]$\n\n\n\n\n\nLines in a file\n are illustrated by a yellow box that displays the desired lines in a file:\n\n\n\npriorities=1\n\n\n\n\n\n\n\n\n\n 2  Applicable versions \n\n\n\nThis documentation applies to the latest version of APF: 2.4.9\n\n\n\n\n\n 3  Deployment using RPM \n\n\n\nInstallation as root via RPMs has now been quite simplified. These instructions assume Red Hat /\nEnterprise Linux 6.x (and derivates) and the system Python 2.6.x. Other distros and higher \nPython versions should work with some extra work. \n\n\n1) Install and enable a supported batch system. Condor is the current supported default. /\nSoftware available from  \nhttp://www.cs.wisc.edu/condor/\n. Condor/Condor-G setup and \nconfiguration is beyond the scope of this documentation. Ensure that it is working\nproperly before proceeding. \n\n\n2) Install a grid client and set up the grid certificate+key under the user APF will run as. \nSee  sysconfig section in \nconfiguration\n for details.\nPlease read the section proxy.conf in \nconfiguration\n regarding the proxy configuration file, so you see what \nwill be needed. Make sure voms-proxy-* commands work properly. \n\n\n3) We distribute now AutoPyFactory RPMs using the Open Science Grid (OSG) yum infrastructure. \nInstall the OSG yum repo files:\n\n\n\n\n[root@factory ~]$ rpm -Uhv  http://repo.grid.iu.edu/osg/3.3/osg-3.3-el6-release-latest.rpm\nRetrieving http://repo.grid.iu.edu/osg/3.3/osg-3.3-el6-release-latest.rpm\nPreparing...                ########################################### [100%]\n   1:osg-release            ########################################### [100%]\n\n\n\n\nFor RedHat 7, in a similar way:\n\n\n\n\n\n\n[root@factory ~]$ rpm -Uhv  http://repo.grid.iu.edu/osg/3.3/osg-3.3-el7-release-latest.rpm\nRetrieving http://repo.grid.iu.edu/osg/3.3/osg-3.3-el7-release-latest.rpm\nPreparing...                ########################################### [100%]\n   1:osg-release            ########################################### [100%]\n\n\n\n\n\n\nMore extensive documentation on how to install the OSG yum files can be found \nhere\n.\n\n\n\nIMPORTANT\n: it may happen that a new version of AutoPyFactory is not available in the release repo, but it is in the development repo. This will happen, for example, when a bug needs to be fixed, and the new RPM is not available yet \"officially\" until next OSG cycle. \nTherefore, for this type of cases, you may want to enable the development repo as well, as it is disabled by default, by setting \nenabled=1\n in the file \n/etc/yum.repos.d/osg-el6-development.repo\n :\n\n\n\n\n[osg-development]\nname=OSG Software for Enterprise Linux 6 - Development - $basearch\n#baseurl=http://repo.grid.iu.edu/osg/3.3/el6/development/$basearch\nmirrorlist=http://repo.grid.iu.edu/mirror/osg/3.3/el6/development/$basearch\nfailovermethod=priority\npriority=98\nenabled=1\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG\nconsider_as_osg=yes\n\n[osg-development-source]\nname=OSG Software for Enterprise Linux 6 - Development - $basearch - Source\nbaseurl=http://repo.grid.iu.edu/osg/3.3/el6/development/source/SRPMS\nfailovermethod=priority\npriority=98\nenabled=0\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG\n\n[osg-development-debuginfo]\nname=OSG Software for Enterprise Linux 6 - Development - $basearch - Debug\nbaseurl=http://repo.grid.iu.edu/osg/3.3/el6/development/$basearch/debug\nfailovermethod=priority\npriority=98\nenabled=0\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG\n\n\n\n\n\nFor RedHat 7, the yum file is \n/etc/yum.repos.d/osg-development.repo\n. \nIt looks similar, and the step is the same, enable the development repo setting \nenabled=1\n.\n\n\n\n\n[osg-development]\nname=OSG Software for Enterprise Linux 7 - Development - $basearch\n#baseurl=http://repo.grid.iu.edu/osg/3.3/el7/development/$basearch\nmirrorlist=http://repo.grid.iu.edu/mirror/osg/3.3/el7/development/$basearch\nfailovermethod=priority\npriority=98\nenabled=0\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG\nconsider_as_osg=yes\n\n[osg-development-source]\nname=OSG Software for Enterprise Linux 7 - Development - $basearch - Source\nbaseurl=http://repo.grid.iu.edu/osg/3.3/el7/development/source/SRPMS\nfailovermethod=priority\npriority=98\nenabled=0\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG\n\n[osg-development-debuginfo]\nname=OSG Software for Enterprise Linux 7 - Development - $basearch - Debug \nbaseurl=http://repo.grid.iu.edu/osg/3.3/el7/development/$basearch/debug\nfailovermethod=priority\npriority=98\nenabled=0\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG\n\n\n\n\n\n4) If you will be performing \nlocal\n batch system submission (as opposed to remote submission\nvia grid interfaces) you must confirm that whatever account you'll be submitting as exists on\nthe batch cluster. This is also the user you should set AutoPyFactory to run as. \n\n\n\nNOTE\n: You do not want local batch logs being written to NFS, so you will need to define a \nlocal directory for logs and be sure the APF user can write there. \n\n\n\n\n\n\n\n\n\n\n5) Install the AutoPyFactory RPMs:\n\n\nAutoPyFactory is now distributed as a set of RPMs, rather than a single one. \nThe complete list of available RPMs is described in the following table:\n\n\n\n\n  \n\n    \nRPM\n\n    \ncontent\n\n    \ndependencies\n\n  \n\n\n\n    \nautopyfactory-common\n\n    \n All core files and directories needed to make AutoPyFactory work \n\n    \n condor \n python-simplejson \n python-pycurl \n\n  \n\n\n\n    \nautopyfactory-proxymanager\n\n    \n The files to install the proxy management tool, standalone \n\n    \n voms-clients \n myproxy \n autopyfactory-common \n\n  \n\n\n\n    \nautopyfactory-plugins-panda\n\n    \n The files needed to allow AutoPyFactory to interact with PanDA \n\n    \n \n\n  \n\n\n\n    \nautopyfactory-plugins-local\n\n    \n The files needed to allow AutoPyFactory to submit locally or to query a local condor pool \n\n    \n \n\n  \n\n\n\n    \nautopyfactory-plugins-remote\n\n    \n All files needed by AutoPyFactory to submit pilots to the grid \n\n    \n \n\n  \n\n\n\n    \nautopyfactory-plugins-cloud\n\n    \n All files needed by AutoPyFactory to submit pilots to the clouds (for example, to EC2) \n\n    \n \n\n  \n\n\n\n    \nautopyfactory-plugins-scheds\n\n    \n The complete set of scheduler plugins \n\n    \n \n\n  \n\n\n\n    \nautopyfactory-plugins-monitor\n\n    \n The complete set of monitor plugins \n\n    \n \n\n  \n\n\n\n\n\n\n\n\n\nYou can select the set of RPMs needed according to the particular purpose of your factory (submitting to a cloud, to the grid based on idle jobs in a local condor pool, etc.)\nIn all cases installing autopyfactory-common is required, as it contains the core functionalities of AutoPyFactory.\n\n\n[root@factory ~]$ yum install autopyfactory-common\n\n\n\nThis performs several setup steps that otherwise would need to be done manually:\n\n \n\n\n Creates 'autopyfactory' user that AutoPyFactory will run under by default.\n\n \n Enables the factory init script via chkconfig.\n\n \n Installs condor.\n\n \n Installs python-simplejson and python-pycurl.\n\n \n Installs RPM autopyfactory-proxymanager.\n\n \n\n\n\n\nIn order to facilitate the installation process, a set of META-RPMs are also available for the more common use cases:\n\n\n\n\n  \n\n    \nMETA RPM\n\n    \nInstalls\n\n  \n\n\n\n    \nautopyfactory-remote\n\n    \n autopyfactory-common \n autopyfactory-plugins-remote \n autopyfactory-proxymanager \n\n  \n\n\n\n    \nautopyfactory-panda\n\n    \n autopyfactory-common \n autopyfactory-plugins-panda  \n\n  \n\n\n\n    \nautopyfactory-wms\n\n    \n autopyfactory-common \n autopyfactory-plugins-local \n\n  \n\n\n\n    \nautopyfactory-cloud\n\n    \n autopyfactory-common \n autopyfactory-plugins-cloud \n autopyfactory-proxymanager \n\n  \n\n\n\n\n\n\n\n6) Start AutoPyFactory:\n\n\n\n\n[root@factory ~]$ /etc/init.d/autopyfactory start\n\n\n\n7) Confirm that everything is OK:\n\n \n\n\n  Check to see if APF is running:\n\n \n\n\n[root@factory ~]$ /etc/init.d/factory status\n\n\n \n\n\n Look at the output of ps to see that AutoPyFactory is running under the expected user.      This should show who it is running as, and the arguments in      \n/etc/sysconfig/factory\n: \n\n \n\n\n\n\n[root@factory ~]$ ps aux | grep autofactory | grep -v grep\n502       6624  0.1  0.0 721440 12392 pts/0    Sl   Oct20   2:04 /usr/bin/python /usr/bin/autopyfactory --conf /etc/autopyfactory/autopyfactory.conf --info --sleep=60 --runas=autopyfactory --log=/var/log/autopyfactory/autopyfactory.log\n\n\n\n \n\n\n  Tail the log output and look for problems.\n\n \n\n\n \n\n[root@factory ~]$ tail -f /var/log/autopyfactory/autopyfactory.log\n\n\n \n\n\n Check to be sure jobs are being submitted by whatever account AutoPyFactory is using by executing condor_q manually:  \n\n \n\n\n \n\n[root@factory ~]$ condor_q | grep autopyfactory\n\n\n\n\n 4  Dependencies \n\n\n\nIn some cases, but not always, HTCondor will be needed to run operations. \nFor those cases, HTCondor RPMs are expected to be deployed. \nAutoPyFactory does not set explicitly it as a dependency, so it will not be installed automatically. \n\n\n\n 5  Configuration \n\n\n\nThe details about configuration variables in AutoPyFactory can be found \nhere\n\n\n\n\n\n\n\n\n 6  Relase notes \n\n\n 6.1  Relase semantics \n\n\n\nAutoPyFactory release versions are composed by 4 numbers:\n\n\n        major.minor.release-build\n\n\nFor example: 1.2.3-4\n\n \n\n\n A change in the major number means the entire architecture of AutoPyFactory has been redesign. It implies a change at the conceptual  level. In other words, changing the major number means a new project. \n\n \n\n \n\n\n A change in the configuration files that requires sys admins intervention  after updating implies a change in the minor number.\n\n \n\n \n\n\n Implementing a new relevant feature implies changing the minor number.\n\n \n\n \n\n\n A significative amount of code refactoring that may affect the performance  of AutoPyFactory -including speed, memory usage, disk usage, etc-  implies changing the minor number.\n\n \n\n \n\n\n Small features and bug fixes imply changing the release number. \n\n \n\n \n\n\n A change in the RPM package but not in the code are reflected in the  build number.\n\n \n\n \n\n\n Not all new releases are placed in the production RPM repository.   Many of them are available at the development and testing repositories,   but only those that have been verified to work are moved   to the production repository. \n\n \n\n \n\n\n Same RPM will have always the same numbers in all repositories. \n\n \n\n\n\n 6.2  version 2.4.6 \n\n\n\nOnly difference between version 2.4.6 and previous 2.4.x versions is the location of the configuration files. \nConfiguration files in 2.4.6 are deployed directly under directory \n/etc/autopyfactory/\n\n\n\n2.4.6 respects the new nomenclature for files, directories and processes introduced in 2.4.0.\nFor factories being migrated from 2.3.x to 2.4.6 it is, therefore, highly recommended to read first section on upgrading a factory from 2.3\n\n\n\n\n 6.3  version 2.4.0 \n\n\n\nVersion 2.4.0 introduces major changes in name of files and directories, \nprograms, users accounts, processes, etc. \nThis recipe should help with a step by step migration:\n\n\n 1. stop the factory\n\n\n\n\n[root@factory ~]$ service factory stop\n\n\n\n\n 2. install RPM for autopyfactory-2.4.0\n\n\nfirst, it is needed to removed the previous installation and the one for autopyfactory-tools, \nif it is installed, since there are incompatible requirements\nThis will delete some files and directories. If you customized the logrotation, you may want to make a copy first.\nAlso make a security copy of the configuration directory\n\n\n\n\n[root@factory ~]$ cp /etc/logrotate.d/autopyfactory.logrotate /tmp/\n[root@factory ~]$ cp /etc/sysconfig/factory.sysconfig /tmp/\n[root@factory ~]$ mkdir /tmp/etc/\n[root@factory ~]$ cp /etc/apf/* /tmp/etc/\n\n\n\n\nRemove the old packages:\n\n\n\n\n[root@factory ~]$ rpm -e panda-autopyfactory panda-autopyfactory-tools\n\n        \n\n\nInstall the new autopyfactory package:\n\n\n\n\n[root@factory ~]$ yum install autopyfactory\n\n\n\n\n\n\n3. Several directories have been created. \n\n\nDirectory \n/etc/autopyfactory/\n has been created, but it is empty. \n\n\nThe examples for the config files are placed under \n/usr/share/doc/autopyfactory-2.4.0/\n.\n\n\nAlso the examples for logrotate and sysconfig files are in  \n/usr/share/doc/autopyfactory-2.4.0/logrotate/\n and \n/usr/share/doc/autopyfactory-2.4.0/sysconfig/\n\n\n\nThe old config files are still under \n/etc/apf/\n and the old sysconfig is still as \n/etc/sysconfig/factory.sysconfig\n\n\n\n4. sysconfig\n\n\nOption 1: copy the new one\n\n\n\n\n[root@factory ~]$ cp /usr/share/doc/autopyfactory-2.4.0/sysconfig/autopyfactory-example /etc/sysconfig/autopyfactory\n\n\n\n\nOption 2: copy the old one\n\n\n\n\n[root@factory ~]$ cp /tmp/factory.sysconfig  /etc/sysconfig/autopyfactory\n\n\n\n\nIn the second case, some adjustments may be needed:\n\n \n\n\n  replace      --runas=apf                           -->  --runas=autopyfactory\n\n \n  replace      --log=/var/log/apf/apf.log            -->  --log=/var/log/autopyfactory/autopyfactory.log\n\n \n  replace      CONSOLE_LOG=/var/log/apf/console.log  -->  CONSOLE_LOG=/var/log/autopyfactory/console.log       \n\n \n\n\n\n\n5. log rotation\n\n\nOption 1: copy the new one\n\n\n\n\n[root@factory ~]$ cp /usr/share/doc/autopyfactory-2.4.0/logrotate/autopyfactory-example /etc/logrotate.d/autopyfactory\n\n\n\n\nOption 2: copy the old one (saved in /tmp/)\n\n\n\n\n[root@factory ~]$ cp /tmp/autopyfactory.logrotate /etc/logrotate.d/autopyfactory\n\n\n\n\nIn the second case, some adjustments may be needed:\n\n \n\n\n replace       /var/log/apf/apf.log         -->     /var/log/autopyfactory/autopyfactory.log\n\n \n replace       /var/log/apf/console.log     -->     /var/log/autopyfactory/console.log\n\n \n replace       /etc/init.d/factory          -->     /etc/init.d/autopyfactory\n\n \n\n\n6. autopyfactory.conf\n\n\nOption 1: copy the new one\n\n\n\n\n[root@factory ~]$ cp /usr/share/doc/autopyfactory-2.4.0/autopyfactory.conf-example  /etc/autopyfactory/autopyfactory.conf\n\n\n\n\nOption 2: copy the old one\n\n\n\n\n[root@factory ~]$ cp /tmp/etc/factory.conf /etc/autopyfactory/autopyfactory.conf\n\n\n\n\nIn the second case, some adjustments may be needed:\n\n \n\n\n replace         factoryUser = apf                           -->  factoryUser = autopyfactory\n\n \n replace   queueConf = file:///etc/apf/queues.conf     -->  queueConf = file:///etc/autopyfactory/queues.conf\n\n \n replace   queueDirConf = /etc/apf/queues.d/           -->  queueDirConf = /etc/autopyfactory/queues.d/\n\n \n replace   proxyConf = /etc/apf/proxy.conf             -->  proxyConf = /etc/autopyfactory/proxy.conf\n\n \n replace   monitorConf = file:///etc/apf/monitor.conf  -->  monitorConf = /etc/autopyfactory/monitor.conf\n\n \n replace   baseLogDir = /home/apf/factory/logs         -->  baseLogDir = /home/autopyfactory/factory/logs\n\n \n add line  mappingsConf = /etc/autopyfactory/mappings.conf\n\n \n\n\n7. queues.conf\n\n\nOption 1: copy the new one\n\n\n\n\n[root@factory ~]$ cp /usr/share/doc/autopyfactory-2.4.0/queues.conf-example  /etc/autopyfactory/queues.conf\n\n        \n\n\nOption 2: copy the old one\n\n\n\n\n[root@factory ~]$ cp /tmp/etc/queues.conf /etc/autopyfactory/queues.conf\n\n\n\n\nIn the first case, the file needs to be configured from scratch.\nIn the second case, no adjustments is needed.\n\n\n8. proxy.conf\n\n\nOption 1: copy the new one\n\n\n\n\n[root@factory ~]$ cp /usr/share/doc/autopyfactory-2.4.0/proxy.conf-example  /etc/autopyfactory/proxy.conf\n\n\n\n\nOption 2: copy the old one\n\n\n\n\n[root@factory ~]$ cp /tmp/etc/proxy.conf /etc/autopyfactory/proxy.conf\n\n\n\n\nIn the first case, the file needs to be configured from scratch.\nIn the second case, no adjustments is needed.\n\n\n9. monitor.conf\n\n\nOption 1: copy the new one\n\n\n\n\n[root@factory ~]$ cp /usr/share/doc/autopyfactory-2.4.0/monitor.conf-example  /etc/autopyfactory/monitor.conf\n\n\n\n\nOption 2: copy the old one\n\n\n\n\n[root@factory ~]$ cp /tmp/etc/monitor.conf /etc/autopyfactory/monitor.conf\n\n\n\n\nIn the first case, the file needs to be configured from scratch, but most probably the default configuration is enough.\nIn the second case, no adjustments is needed.\n\n\n10. mappings.conf\n\n\nThis config file is new, so it must be copied \n\n\n\n\n[root@factory ~]$ cp /usr/share/doc/autopyfactory-2.4.0/mappings.conf-example  /etc/autopyfactory/mappings.conf\n\n        \n\n\nDo not touch it.\n\n\n11. since the factory will now run as user \nautopyfactory\n instead of \napf\n, the new UNIX account needs to be created, if not already done during the RPM install process.\n\n\n\n\n[root@factory ~]$ useradd autopyfactory\n\n\n\n\nassuming that account also hosts the keys for the X509 proxies in the regular directory .globus:\n\n\n\n\n[root@factory ~]$ mkdir ~autopyfactory/.globus\n[root@factory ~]$ cp -r ~apf/.globus/* ~autopyfactory/.globus/\n[root@factory ~]$ chown -R autopyfactory:autopyfactory ~autopyfactory/.globus\n\n\n\n\nif that is not the case, then copy the .pem keys and/or change their ownership.\nAnd to avoid problems, delete the current X509 that may still be in \n/tmp/\n\n\n\n12. Install autopyfactory-tools\n\n\n\n\n[root@factory ~]$ yum install autopyfactory-tools\n\n        \n\n\n13. start the factory\n\n\n\n\n[root@factory ~]$ service autofactory start\n\n\n\n\n\nWARNING\n: after migrating to 2.4.0, factory runs under user \nautopyfactory\n instead of \napf\n.\nThat means no one will now clean the old directories \n/var/log/apf/\n and \n~apf/factory/logs/\n\nYou may delete them at some point (not right away, since they will include condor logs for still running pilots)\n\n\n\n\n\n 6.4  upgrading a factory from 2.3.1 to 2.4.0 \n\n\n\nIn order to upgrade a factory from old stable version 2.3.1 to latest 2.4.x series, a few things need to be taken into account.\n\n \n\n\n The name of files, directories, and processes has changed. A detailed explanation is in the above section version 2.4.0\n\n \n a new optional variable has been included in the file \nproxy.conf\n: \nvoms.args\n\n\n \n a new mandatory option has been added to the new file \nautopyfactory.conf\n: \nmappingsConf\n\n\n \n\n\n\n\n\n 6.5  version 2.3.2 \n\n\n \n\n\n RPM built incorrectly. Rebuilt against correct copy of code.  \n\n \n\n\n\n 6.6  version 2.3.0 \n\n\n \n\n\n utils not distributed anymore within the RPM. They will be distributed with a dedicated one.\n\n \n variable 'flavor' mandatory in DEFAULT section in \nproxy.conf\n Values are voms or myproxy\n\n \n In case \nflavor=myproxy\n in \nproxy.conf\n, then these variables are needed: \n\n\n remote_host\n\n \n remote_user\n\n \n remote_owner\n\n \n remote_group\n\n \n\n \n New variable \nfactorySMTPServer\n in \nfactory.conf\n\n\n \n New variable \nproxymanager.sleep\n in \nfactory.conf\n\n\n \n\n\n\n 6.7  version 2.2.0-1 \n\n\n \n\n\n examples of executables (a.k.a. wrappers) \n\n \nare placed underneath the \n/etc/apf/\n directory.\nThey are not copied directly into \n/usr/libexec/\n anymore. \n\n\n wrapper-0.9.9.sh has a different set of  input options than previous one.   Read carefully the inline documentation before using it.\n\n \n Config plugins have been removed.   Any configuration variable in \nqueues.conf\n  related PandaConfig or AGISConfig plugins   are not valid anymore.   Therefore the variable \nconfigplugin\n is gone too.\n\n \n Variables \noverride\n and \nautofill\n are also gone.\n\n \n There is a new configuration file called \nmonitor.conf\n  An example is provided underneath \n/etc/apf/\n\n\n \n Old variable in \nfactory.conf\n pointing to the monitor URL  is now in \nmonitor.conf\n. The \nmonitor.conf\n contains sections  for different monitor configurations.  The name of the section is setup in \nqueues.conf\n via  the new variable \nmonitorsection\n. Read carefully the inline documentation in \nmonitor.conf\n before using it.\n\n \n Utils, including script to generate \nqueues.conf\n  with information from AGIS, have changed name and location.  New scripts are place underneath \n/usr/share/apf\n\n\n \n\n\n\n 7  CHANGELOG \n\n\n\n2.4.9\n\n* fixed bug in sched plugin StatusOffline not checking if siteinfo is None\n\n2.4.8\n\n* only reading configuration files that ends with string \".conf\"\n* simplified the name of plugins modules and classes, and pluginsmanagement code accordingly\n* added a new layer to the plugins directory structure\n* distinguish between self.keep_running is None than when is 0\n* MaxToRun returns 0 when there is no BatchInfo\n* added function debugrestart to init script\n* added TRACE level to logging systems\n* manually merged various changes to loglevels and sched plugins from ec2fixed adhoc branch.\n* fixed minor bug with debugrestart when log file doesn't exist\n* cleaned up sched plugin messages to be more consistent \n* introduced email notification throttling at factory level per message content\n\n2.4.7 \n\n* port number in case of submit plugin CondorOSGCE not hardcoded. But default is still 9619\n* batchqueue was still mandatory in one plugin. Not anymore\n* added panda status throttled to the mappings as notready\n* some code in proxymanager.py cleaner, and made baseproxy not mandatory\n* made the remote_* variables in proxymanager not mandatory, and removed from proxy.conf\n* created bin/autopyfactory_version\n* checking if methods _updateclouds( ) and _updatesites( ) in PANDA WMS Status plugin return something valid\n* added MissingPluginException to apfexceptions.py\n* split the code in method APFQueue.run( ) into 3 separated methods, so they could be called one by one from outside\n* starting the APFQueue threads in a separate method _start( )\n* using Queue() to serialize the condor_submit tasks\n* using self.factory.qcl instead of self.qcl in cleanlogs, so it always keeps the latest configuration, even after reconfig\n* Factory.__init__ refactorized, for clarity\n* created directory sbin/ add added first draft version of autopyfactory-reconfigure\n* using method Config.compare() to decide which queues to start and stop on every re-configuration\n* created first draft code for a listener class\n* created method Config.addsection() to add and entire section to a configloader object\n* factory.py split into two files: factory.py and queues.py\n* APFQueue objects are started all of them from method APFQueuesManager::start(), instead of being started when created\n* added configplugin variable to autopyfactory.conf\n* config/ plugins\n* moved Singleton templates from factory.py to interfaces.py\n* moved x509 proxy management from CondorBase submit plugin to CondorGrid submit plugin\n* added methods isequal( ), sectionisequal( ) and compare( ) to configloader.py\n* default value for queuesDirConf set to None to allow start just from install\n* printing a WARNING log message when merging two sections with the same name\n* set the default value of \"override\" to False (the new object has precedence over the current one)\n* allowing both queues.conf and queues.d/ at the same time, each one with its own merging mechanism\n* for clarity, code for plugins management in a separate module pluginsmgmt.py\n* first draft for sched plugin Throttle \n* the config files, logrotate file and sysconfig files are not treated as documentation anymore\n* commented out the line reading x509userproxy from ProxyManager in BatchSubmit Exec plugin\n* new import path to find Clients.py  in WMSStatus Panda plugin\n* bug fixed: usercert and userkey only needed in proxymanager.py when baseproxy is none, not always\n* changes in the logrotate config file:\n  no prerotate script, post rotate only restart daemon when there is a PID file\n* normalized the format of the messages returned by the Sched plugins\n\n2.4.6\n\n* importing proxymanager only when proxymanager.enabled is True\n\n2.4.5 \n\n* added external/ directory, with panda client in it\n* trying several paths to import panda Client.py\n\n2.4.4\n\n* each APFQueue carrying only one section in the config loader object qcl, instead of all sections, to avoid scalability issues.\n\n2.4.3\n\n* creating the directory baseLogDir if it does not exist yet (for example, first time APF is installed)\n* removed manpages management from rpm-post.sh script\n\n2.4.2\n\n* variable batchqueue not mandatory\n* typo fixed in proxymanager.py\n\n2.4.1\n\n* reorganizing the docs/* files, etc/* files,  and the man/* files\n* added requirements from voms-clients and myproxy in setup.cfg\n* removed the handling of sysconfig files in rpm-post.sh\n* mappings.conf created.\n  plugins now read the mappings from that config file, instead of having hardcoded dictionaries.\n* explanation for \"lifetime\" and \"minlife\" fixed in proxy.conf-example, \n  and variable \"vomshours\" renamed as \"hours\" in proxymanager.\n  doc updated.\n  method _checkTimeleft() renamed  _checkVOMSTimeLeft(). Log messages fixed accordingly.\n*  created method _checkProxyTimeLeft()\n*  added a check on proxy timeleft shorter than voms timeleft\n* Bugs and typos fixed\n* changed nomenclature: everything now is called \"autopyfactory\" instead of \"apf\" or \"factory\"\n  .sysconfig removed from  /etc/sysconfig/autopyfactory.sysconfig\n  panda- removed from package name\n  no wrappers copied to the /etc/ directory\n* accept multiple XML docs as output of \"condor_q -g\". \n* checking if the output of condor_q in querycondor() is still XML even when RC != 0\n* if -name and -pool are empty, the condor_q_id is again \"local\"\n* checking output of condor_q() is not None in WMS Status plugin Condor\n* removed WMS Status plugin CondorLocal (moved to attic)\n* new grid_resource line in Condor-CE submit plugin: CondorOSGCE\n* added voms.args to proxymanager.py\n* condorlocal -> condor for WMS Status plugin in queues.conf-example and documentation\n* adding Condor WMS Status plugin, which is a multi-singleton, to replace old  CondorLocal WMS Status plugin\n* allow getwmsstatusplugin() to return a multi-singleton plugin object\n* Using queryargs in WMS Status plugin CondorLocal\n* Email sent when the generated X509 proxy is not valid: does not exist, too old, or not proper VOMS attributes\n* Adding a log message in StatusTest and StatusOffline Sched Plugins \n  when the queue is not test or offline.\n* reading self.cloud from queues.conf removed\n  Variables not used anymore deleted from queues.conf-example\n* bug in manpage autopyfactory.1 fixed\n* created submit plugin CondorPBS\n* removed reload() function from the init script\n\nChangeLog 2014-01-24\n\n* Fixed Condor submit plugin initialization hierarchy. \n* Switched from os.getlogin() to pwd.\n\nChangeLog 2014-01-02\n--------\n\n* adjustments to init.d script to improve error codes/detection\n* proxymanager.sleep and remote_* config vars added to -example files. \n\nChangeLog 2013-12-03\n---------\n\n* sched plugin Activated deleted\n* bug fixed in method fill() in info.py to prevent an Exception when the dictionary has an unknown key.\n* CondorEC2 BatchSubmit and BatchStatus plugins usable, supporting standard Condor ec2 grid type. Assuming VMs join local pool as startds, features correlation of VM jobs and corresponding startd. VM retirement via 'condor_off -peaceful' and VM shutdown when stard is retired. \n* KeepNRunning sched plugin to convert absolute to relative numbers. \n* utils not distributed anymore within the RPM. They will be distributed with a dedicated one.\n* Removed reference to Panda cloud status in StatusOfflineSchedPlugin\n* created apf-simulated-scheds in misc/\n* Proxymanager now able to be run standalone as standard system init daemon. This is so base certs need not be owned or readable by the APF user. \n* Added email notification of factory owner when no valid proxy can be retrieved from the proxymanager. \n* Eliminated all _readconfig() methods in submit plugins. Switched to full initialization during __init__.\n* MyProxy support in proxymanager. Allows retrieval of base proxy from MyProxy using passphrase, or using another proxymanager profile proxy for auth.\n* fixed bug in Scale sched plugin returning 0 when n*factor \n<\n 1. \n* minimum Condor version check. Allows particular plugins to specify minimum. Failure aborts. \n* multiproxy functionality. Allow multiple proxy profiles. proxymanager tries all until one is found. failure generates email.  \n* scripts and config files added for logs monitor apf-search-failed\n* strip() after split by comma in configloader, to allow things like 'x = foo, bar' with whitespace after comma\n* bug in MaxPending sched plugin logic fixed. Now no limit imposed when there are no pending pilots.\n* queues configuration files can be read from a directory (i.e. /etc/apf/queues.d/). configloader adapted to accept directories instead of list of URIs\n* method name removed from log messages.\n* fixed bug in some sched plugins, mixing None and 0 in the logic\n* add documentation to install.html (in verbatim, no html format)\n* removed all methods for FactoryCLI from bin/factory, and mainLoop() renamed to run()\n* added new custom logging level -TRACE- and using it for some long messages, like the output of condor_q in XML format\n* configloader converts None in conf to Python None object. Change in default_value logic--if no default_value is provided\n  then generic_get returns None. \n* info major refactoring of Info class hierarchy. most objects now have overridden methods to avoid exception generation. \n* passing queryargs from queues.conf to query.querycondor() call at Condor batchstatus plugin.\n* siteid removed from several places and replaced by wmsqueue.\n* generic_get() simplified\n* InfoContainer removed. \n* fake info classes imports removed, and importing from the right place (info.py instead of factory.py)\n* log messages in sched plugins more homogeneous. Now all of them are \"Return=123\".\n* created a script in misc/ to search for pilots running (in theory) for more than 8 days.\n* Removed allowed variables for Test and Offline Sched plugins, and from the queues.conf. If the plugin is used, we assume it is allowed. \n* more robust code to deal with scenario where condor_q gives no output\n* all JSD.add() method replaced to use the new format (2 inputs instead of 1)\n* jsd.py and jsd3.py moved to attic/ and jsd4.py moved to jsd.py\n* logserver2.py moved to logserver.py, and old one moved to attic/\n* using method merge() instead of deepcopy() in configloader.py\n* method section2dict() created in configloader. It will be usefull to generate the mappings from config files\n* getConfig() method in configloader.py embedded in a try - except block\n  and all config loader object creation in factory.py embedded in try - except blocks\n* Exception class ConfigFailure renamed ConfigFailureMandatoryAttr\n* Cleaning info.py code:\n    -- method dict() removed\n    -- method getconfig() removed\n    -- method __add__() removed\n    -- method set() removed\n    -- method get() removed\n    -- property total removed\n    -- class attribute valid = [] removed from all classes:\n        -- therefore method reset() removed\n        -- therefore method __setattr__() removed\n    -- method __getattribute__() is created\n    -- method valid() in class InfoContainer deleted\n    -- no longer importing Config from configloader \n    -- no longer self.default_value\n    -- all __init__() methods now hardcode the entire list of attributes\n* Removed logger from the input options in method generic_get()\n* As temporary solution, NotImplementedAttr defined in configloader.py\n* Documentation on old Sched Plugin SimpleNQueue deleted.\n* First draft for manpages created, and rpm-post script adapted to install them.\n* Printing env again after switching ID.\n* Bug fixed in CREAM example in queues.conf-example\n* created misc/apf-panda-jobs-info.py \n* INFO messages in configloader for missing non-mandatory variables moved to DEBUG.\n* all __XYZ__ module names deleted, except in factory.py\n* Fixed setup.cfg\n* Adjusted sysconfig, factory init, and logrotate to use a console.log for python interpreter-level \n  debugging. \n* Added a log message with the APF version number.\n* All Sched Plugins returns a tuple (number of pilots, message). \n* Sending to the monitor the messages returned by Sched Plugins.\n* Changed setup.py, factory.py and plugins/monitor/APFMonitorPlugin.py to use release version info directly\n  from factory.py rather than requiring that it be correctly placed in a config file. \n  versionTag removed from factory.conf\n* Added logserver2.py, to create directory listing similar to Apache one.\n* Port number is got from URL instead of from config variable baseLogHttpPort\n* Method setuppandaenv() deleted from factory.py\n\n2013-05-06\n\n* copy_to_spool set to True\n* dumping the content of queues.conf on start\n* allowing raw = True or False in getContent() method in configloader\n* utils distributed via /usr/share/apf instead of /usr/bin, and have different names.\n* created RELEASE_NOTES\n* everything related euca and persistence removed from config files. \n* created test.py, to start creating unittest-like code\n* should_transfer_files = NO\n* new configuration file for monitor plugins.\n* wrapper examples are placed in /etc/apf/ instead of /usr/libexec/\n* No more Config Plugins.\n* module condor.py created. It includes htcondor python bindings. \n* Non plugin modules (jsd.py, persistence.py) moved to main directory.\n* Split plugins into their own directories. \n* version number for panda userinterface package in setup.cfg\n* bugs in CondorLocalWMSStatusPlugin fixed\n* ReadySchedPlugin created -> Activated decommissioned\n* Removing self._valid from all plugins (work in progress)\n* ConfigFailure moved to apfexceptions.\n* Removed hardcoded setup of periodic_remove directive from CondorLocal Batch Submit plugin.\n* Not trying to create robot.txt is logserver is disabled.\n* Changing directory to new HOME directory after switching identity.\n* CondorNordugrid Batch Submit plugin created.\n* Returning cached info object when there are problems to contact the info service in Panda Config Plugin and AGIS Config Plugin. \n* Checking condor daemon is running in CondorBase Batch Submit Plugin and Condor Batch Status Plugin.\n* Created a Singleton metaclass factory. It creates metaclasses for regular Singletons and multi Singletons.\n* URLs for AGIS Config Plugin and PanDA Config Plugin read from queues.conf instead of hardcoded.\n* Printing the path to executable condor_q and condor_submit in debug mode.\n* Using new URL for AGIS Config Plugin, and fixing code to parse new output format (a dict of dicts instead of a list of dicts).\n* New URL for Panda Config Plugin.\n* Added __add__() method to BatchQueueInfo and WMSQueueInfo classes.\n* CondorLSF Submit plugin created.\n* Euca plugins improved. Still under development. NOTE: most probably they will be removed and never used.\n* New type of plugins for monitor added.\n* Added factory config variable 'enablequeues'.\n* ScaleSchedPlugin created.\n* Sched plugins do not check for negative outputs. It has been left up to the Submit Plugin to decide what to do in that case. \n* Added some DEBUG logging messages.\n* Some messages in Activated Sched Plugin moved from DEBUG logging level to INFO level.\n* Bug fixed: getboolean() instead of get() when reading value of proxymanager.enabled and logmanager.enabled.\n* Top logger configured as root, instead of \"main\".\n* Log messages format includes method name for python > 2.4\n* KeepNRunningSchedPlugin created.\n* This release is 'Babcock' v. 2.2.0\n  http://en.wikipedia.org/wiki/List_of_Characters_in_The_Passage_Trilogy#Babcock\n\n\n2012-07-23\n\n* Using new PanDA client API method getJobStatisticsWithLabel.\n* GRAM CE is called OSG-CE in AGIS, instead of CE. Changed in AGISConfig Plugin.\n* Pilots in STAGE-IN status are now considered to be PENDING instead or RUNNING.\n* All get() calls in configloader.py have now raw=True, so interpolations wait until the very end.\n* wmsqueue can be read from SchedConfig via PandaConfig Plugin, and from AGIS via AGISConfig Plugin.\n* PandaConfig Plugin reads from SchedConfig variables related to CREAM CE.\n* MaxToRun sched plugin created.\n* CondorOSGCE submit plugin created.\n* CondorDeltaCloud submit plugin created.\n* Documentation in HTML format under doc/ directory.\n\n2012-06-07\n\n* Bumped minor release version to reflect scale of several new features, and cloud submit plugin. \n* Two new Sched Plugins to handle test queues and offline queues\n* Bug in monitor.py fixed. \n* Creates robots.txt file in base of logserver docroot. \n* Added create_run_var() to init script factory. \n  It creates the subdirectory var/run/ if it does not exist, to place factory.pid\n* Added $APFHEAD to init script factory. \n  This allows for user deployment on any directory, not necessarily $HOME\n* Fixed the bug in Activated plugin, not returning anything under some circumstances. \n* Creating multiple Sched plugins (MinPerCycle, MaxPerCycle, MinPending, MaxPending, MaxPerFactory), \n  and code in factory.py to allow chaining more than one sched plugin.\n* PandaConfig Plugin refactored to query SchedConfig in a singleton thread. \n  This is possible because it now uses an URL that delivers the entire SchedConfig content.\n  Also more variables added: jdladd, environ, and special_pars.\n* First draft for Euca Submit Plugin created.\n* This release is the 'The Hellion' v. 2.1.1\n  http://www.darylgregory.com/pandemonium/Review_NYRSF.aspx\n\n2012-03-30\n\n* Refactored scheduler log cleanup. Now handled in a separate thread, one for entire factory. \n  Defaults may be specified globally in factory.conf or per-queue in queues.conf \n* Added grid and vo queue attributes, and added executable.defaultarguments and executable.arguments \n  interpolation. These changes were to support wrapper.sh, runpilot3.sh, and arbitrary executables in \n  a general way. This feature uses standard Python ConfigParser interpolation. See\n     http://docs.python.org/library/configparser.html\n* Made Monitor object a singleton, to avoid repeated timeout delays (during queue initialization) \n  when APF monitor is unresponsive. Now there is a single attempt when single Monitor is initialized. \n* Move to wrapper 0.9.5, which checks that retrieved tarball is indeed a tarball (and not an HTML error message\n  from a misbehaving HTTP proxy.  \n* Fixed logic problem in Activated plugin. Now correctly assuming that Running jobs are no longer\n  Activated. \n* Added specific Submit plugin for Cloud.\n* Configuration objects handled as python native ConfigParser objects instead of custom\n  APF objects.\n* Refined running as user rather than from RPM. 'setup.py install --home=/path/to/home' does a\n  user-based setup. All libs are in ~/lib/python, configs and init script in ~/etc/, and so on.\n* Added an external queue configuration information plugin mechanism to enable plugin-based \n  auto-fill/overriding of config information. \n* Consolidated all plugins into hierarchical inheritance tree, to eliminate duplicated code \n  (especially the Condor plugins). \n* This release is the 'Sleeper Service' v. 2.1.0\n  http://en.wikipedia.org/wiki/GSV_Sleeper_Service\n\n2011-12-02\n\n* Major refactorization to integrate BNL changes. Full object-oriented functional \n  plugin architecture, each running in a distinct thread. Allows for \n  end-user/third-party customization without touching core code. \n* Refined distutils usage to allow deployment as non-root in a home directory, \n  or as root in system paths. Added functionality to drop privs even when run as\n  root. \n* Added typical init script and sysconfig functionality to handle shell-level/UNIX \n  concerns. \n* Added standard Linux logrotate configuration. \n* Split config system into main source for APF instance (factory.conf), proxy \n  management (proxy.conf), individual queue configuration (queue.conf). The latter \n  can be accessed as URI, paving the way to queue configuration via remote DB+URL.\n* The previous change involved passing ConfigLoader objects to classes, rather than \n  passing through lists of config files. \n* Integrated batch system log export via HTTP; now uses embedded Python HTTP server. \n* Integrated proxy handling and renewal, integrated submit system log file rotation. \n* Adjusted module, file, and class names to follow Python recommended guidelines. See PEP 8:\n  http://www.python.org/dev/peps/pep-0008/ \n* Moved all filehandling to factory.py script from Factory class. Factory is now purely \n  object-oriented suitable for embedding (e.g. in a web application). \n* Added copy of Jose's generic top-level wrapper to libexec/.  \n* IMPORTANT: For various reasons, it was very difficult to maintain the ability to automatically\n  reload configs by detecting file mtime changes. So for now simply restart if you change a file.\n  This feature  can be re-enabled soon. \n* This release is \"The Clockmaker\". v2.0.0\n  http://en.wikipedia.org/wiki/List_of_Revelation_Space_characters#The_Clockmaker\n\n2011-03-28\n\n* Update configuration example to Peter's new monitoring URl.\n* If this is a ptest pilot use Paul's development pilot code (override with\n  PILOT_HTTP_SOURCES still possible).\n* This release is \"The Captain\". v1.2.0.\n  Now the Captain called me to his bed \n  He fumbled for my hand \n  \"Take these silver bars,\" he said \n  \"I'm giving you command.\" \n  \"Command of what, there's no one here \n  There's only you and me -\n  All the rest are dead or in retreat \n  Or with the enemy.\" \n\n2011-03-17\n\n* Only try to use python2.6 if the env var APF_PYTHON26 is defined.\n\n2011-03-16\n\n* Prepended \"FACTORY DEBUG:\" to the setup of the panda client environment\n  variables which get set before logging is enabled (this confused\n  some people).\n* Use traceback.format_exc() to dump exceptions properly into the \n  configured factory logger (\"raise\" sends output to stderr).\n\n2011-01-13\n\n* If PANDA_URL_CONF or PANDA_URL are set then do not alter them.\n\n2010-12-15\n\n* Support 'allowothercountry\" for beyond the pledge resources. (Default False).\n* Set the PANDA_URL_CONF and PANDA_URL environment variables to use the \n  panda server squid port (25085). Suppress this by setting the APF_NOSQUID\n  environment variable.\n* Download of queue data also now happens via the squid cache.\n* Monitoring improvement to help scale better. Removal of monitoring cron jobs.\n* This is the \"Silent Running\" release. Look after the plants, Dewey.\n  http://en.wikipedia.org/wiki/Silent_Running\n\n2010-11-22\n\n* Changed default sleep time to 120s.\n* N.B. Monitoring requires pycurl (install standard python-curl RPM).\n\n2010-11-05\n\n* Wrapper now supports environment variables starting with APF_FORCE_ \n  which set the environment variable with APF_FORCE_ stripped off.\n  This is needed when setting the normal environment variable via\n  condor-g fails because it gets overwritten by the site shell setup.\n\n2010-09-15\n\n* Removed deprecated code from runpilot3-wrapper.sh (uudecode and broken\n  site hacks).\n* This is the \"Time Out of Joint\" release. This pilot factory does not \n  exist. It is a small text file on your computer containing the words\n  \"Pilot Factory\":\n  http://en.wikipedia.org/wiki/Time_Out_of_Joint\n\n2010-07-24\n\n* Corrected bug where \"site\" parameter was used instead of \"siteid\" to get\n  panda job states (broke ANALY sites for which these are different). \n  Reported by Xavi, diagnosed by Graeme, fixed by Peter.\n* Reintroduced support for CREAM CEs - factory now detects CREAM CEs and\n  provides the correct JDL to condor (recommended only for use with \n  condor >= 7.5.3). See configuration example for how to support this.\n* Changed JDL for gt2 GRAM sumbission from anachronistic \"universe = globus\"\n  to \"universe = grid; grid_resource = gt2 GATEKEEPER_URL\".\n* Changed from 'jdl' field in schedconfig to 'queue', which is a better name \n  and has the correct value for condor sites ('jdl' supported but \n  deprecated).\n* Changed default proxy location to /tmp/prodRoleProxy and pilotRoleProxy.\n  This can still be inherited from X509_USER_PROXY.\n* Changed the default source proxy to /tmp/plainProxy.\n* Changed handling of deprecated keys to rewrite the configuration objects\n  themselves, which gives more robust and consistent handling. Unknown\n  keys are deleted and a warning printed.\n* Schedconfig download failures are now errors (not warnings).\n\n2010-06-01\n\n* Changed argument passed to pilot from 'site' to 'siteid', which is correct\n  and necessary for ANALY_ sites.\n* Added new internal status of 'error' where queue has siteid=None. This \n  suppresses pilot submission to that queue.\n* Added new configuration option for the [QueueDefaults] section \n  'analysisGridProxy' which is the default proxy used for ANALY_ sites.\n* Default setting for 'user' of ANALY queues is now 'user' (can still be\n  overwritten). \n* ATLAS software release areas are now ordered correctly by a python code\n  snippet in the wrapper which correctly compares 'rel_M-N'. \n* Updated release number to 1.0.1.\n* This is the \"Feersum Endjinn\" release:\n  http://en.wikipedia.org/wiki/Feersum_Endjinn\n\n2010-04-09\n\n* Added protection in submission logic against None value for \n  depthboost.\n* Updated example configuration file.\n* Changed configuration logic so that override = True prevents loading\n  of schedconfig values only for those values set in the factory\n  configuration (previously this completely supressed schedconfig\n  loading).\n* Renamed various parameters to their new schedconfig names (Oracle \n  column names are case insensitive, so become lower case). \n    pilotDepth -> nqueue\n    pilotDepthBoost -> depthboost\n    idlePilotSuppression -> idlepilotsuppression\n    pilotLimit -> pilotlimit\n    transferringLimit -> transferringlimit\n    env -> environ\n  Old names are still supported for now, but generate a warning message.\n* Changed submission logic for sites in test mode to keep one pilot\n  queued, but up to nqueue running (better throughput for site\n  revalidation).\n* Moved import exception handling to main factory.py script.\n\n2010-03-22\n\n* Restructured logging to inherit from main logging class (deprecated\n  global logging to console). Logging now definable as an option\n  (syslog, rotating file, stdout). N.B. Default logging is now to\n  syslog - if you want the old behaviour set --output=stdout.\n* Rewrote JDL to have separate condor log file for each individual job.\n  Will help integrate with panda monitoring.\n* Added limit setting code. Wrapper will always set a file size of 20GB to \n  prevent mad jobs from killing a worker node. If the queue has a memory\n  limit set, then the pilot also sets this as the VMEM limit in the\n  shell.\n* Removed panda server Client.getSiteSpecs() call. Better to get individual\n  queue statuses from schedconfig (fixes https://savannah.cern.ch/bugs/?62751) \n* Deprecated status=ok,disabled (should be online, offline). Prints warning\n  for now, but remains supported.\n\n2010-03-19\n\n* Internal merge with latest pyfactory trunk (the old release).\n\n2010-03-18\n\n* Setting filesize limit (20GB) and optionally memory limit in pilot wrapper.\n\n2010-02-11 \"Tyger! Tyger!\" Release (r130 factory.py; r128 runpilot3-wrapper.sh)\n\n* Incorporated Rod's patch to support CREAM CEs.\n* Corrected handling of X509_USER_PROXY environment variable and added\n  explicit addition of x509userproxy to JDL.\n* Proxy is now defined per queue, enabling support for multi-role pilot\n  factories with recent condor versions.\n* Pilot tarball now downloaded from pandaserver.cern.ch, removing one\n  dependency of job workflow on the panda monitor.\n* Revised runpilot3-wrapper.sh which removed explict addition of\n  python library installation from the ATLAS s/w area (reversed part 2\n  of change 2 from \"gunpowder\").\n* Corrected an error in detecting OSG sites in runpilot3-wrapper.sh\n  (thanks to Rod).\n* Reintroduced the idlePilotNumber parameter, which controls how many\n  pilots are send during an idling cycle (it's useful for stress\n  testing sites).\n* Fixed a race condition in the vomsrenew.sh script.\n* This release is for William Blake smoking opium at Chinese New Year.\n\n2009-11-24 \"900\" Release (r35 runpilot3-wrapper.sh)\n\n* Define ATLAS_AREA internal variable\n  =$VO_ATLAS_SW_DIR for EGEE sites\n  =$OSG_APP/atlas_app/atlas_rel for OSG sites\n* Source $ATLAS_AREA/local/setup.sh if it exists.\n* This release inspired by 900GeV collisions:\n  http://atlas.web.cern.ch/Atlas/public/EVTDISPLAY/atlas2009-collision-atlantis-141749-406601-hits-web.png\n\n\n2009-11-05 \"Gunpowder\" Release (r25)\n\n* Changed SVN repository to svn.cern.ch/reps/scotgrid/atlas/pyfactory.\n  (Verison number plunged down!)\n* Made wrapper more intelligent in changing system PYTHONPATH to get\n  32 bit lfc module, which helps on sites with 64 bit middleware. Also\n  added ATLAS python library explicitly ahead of system python to\n  prevent old libraries from being loaded by accident.\n* Updated wrapper script to prefer TMPDIR over EDG_WL_SCRATCH (later\n  is an anachronism from the lcg-RB).\n* Source DDM setup.sh to ensure pilot has access to dq2 modules.\n* http://en.wikipedia.org/wiki/Gunpowder_Plot\n\n\n2009-07-08 \"Totoro\" Release (r635)\n\n* Introduced \"country\" and \"group\" parameters for queues. These map to\n  the pilot's -o and -v options allowing for pilots which only pick up\n  particular types of jobs.\n* Polling for queues now intelligently uses the country and group\n  options above to only send pilots when activated jobs of that type\n  are present at a site.\n* Introduced \"cloud\" option to tag a queue to a panda cloud.\n* Site and cloud status are polled and if 'offline' then pilot\n  submission is supressed; if a site/cloud is in 'test' status then\n  the pyfactory status flips to 'test' as well (limits pilot flow).\n* Introuduced 'pilotDepthBoost' parameter (default 2), which allows\n  the factory to submit up to queueDepth * pilotDepthBoost into a\n  non-started state if there are sufficient jobs activated. This helps\n  a lot if the site has short jobs where the lags in job status\n  updates mean that sites can run short of pilots. You may want to set\n  this higher at T1s (especially when doing reco).\n  This is a more controlled version of the short job patch which was\n  released in June 2009.\n* Corrected a small bug in the default queue specification so that it\n  works properly.\n* Renamed the 'suppression' option to 'idlePilotSuppression'.\n* Distributed wrapper script is now called \"runpilot3-wrapper.sh\",\n  which is a much more sensible name.\n* Default server is now pandaserver.cern.ch.\n* Updated INSTALLATION file for new panda client location in SVN.\n* Updates to README, INSTALLATION, Makefile corresponding to the above\n  changes.\n* Eilish wanted this release called 'Totoro', after one of her\n  favourite Hayao Miyazaki films,\n  http://en.wikipedia.org/wiki/My_Neighbour_Totoro.\n\n\n2008-10-08 \"Dawn Treader\" Release (r485)\n\n* GPL license.\n* Internal defaults now work properly.\n* Support for multiple gatekeepers on the same site. These are\n  configured as space separated lists and the factory round robins\n  between them each cycle (see sample configuration file). If you use\n  this a lot then an expansion syntax could be supported - ask.\n* Added a new parameter \"supression\". If this is > 1 then an idling\n  pilot is only submitted to a site every \"supression\" cycles. Use\n  this to mollify sysadmins in your cloud who get grumpy when pilots\n  don't pick up. (When the site has activated jobs submission is as\n  it was before.) Caveat Emptor: this will affect brokering.\n* Added new panda server parameters \"server\" and \"port\" which let the\n  pilot look for jobs from a different panda server. These do not need\n  to be set for most factories (default settings:\n  https://pandasrv.usatlas.bnl.gov:25443).\n* Added a new parameter \"env\" which can list extra environment\n  variables to be set for the pilot by condor. (N.B. be careful if you\n  set a [QueueDefaults] value for \"env\" - the site's value will\n  _overwrite_, not add to, the environment. If there is sufficient\n  demand I can change this behaviour.)\n* Changed runpilot3-script-stub.sh to unset all https_proxy\n  environment variables at a site as this proxy mechanism is broken\n  and rarely necessary for panda server ports.\n* Support for multiple configuration files. These are all loaded and\n  all are reloaded if any of them change. e.g.,\n  --conf=generic.conf,uk.conf,analysis.conf\n  N.B. Configuration files are loaded left to right and latter\n  (re)defined options overwrite earlier ones.\n* Print a warning if queue \"status\" value is invalid.\n* Ruben named this release \"Dawn Treader\" after his favourite Narnia\n  book so far. http://en.wikipedia.org/wiki/Dawn_Treader.\n\n\n2008-06-17 \"Moominsummer Madness\" Release (r458)\n\n* Updated pilot http download to pandamon.usatlas.bnl.gov.\n* Added a timeout to curl for pilot tarball download (30s on\n  connection, 180s total).\n* Fixed the cleanLogs.py script so it now deletes old archived log\n  files.\n* Minor changes to documentation.\n* No longer including a wrapper with a pilot uuencoded into it as it's\n  recommended to download the latest pilot on the fly.\n* Dedication: http://en.wikipedia.org/wiki/Tove_Jansson\n\n\n2008-04-16 \"Umber Hulk\" Release. (r442)\n\n* Upgraded runpilot3.sh to UHURA 18f pilot.\n* Modified the order of the search for an LFC compatible python. The\n  ATLAS release python is now tested first (it's usually more recent\n  than the OS version). N.B. This may provoke a run time python\n  warning about an API mismatch between the python versions.\n* Added factoryId to configuration. This allows multiple factories to\n  run on the same machine. This factory ID is used as the PANDA_JSID\n  passed to the panda dashboard.\n* Added baseLogFileUrl to pass the correct URL for the pilot wrapper's\n  logfiles to the panda dashboard.\n* Added optional \"user\" field to allow pilots to be sent to pick up\n  particular types of jobs from panda, using the \"-u USER\" argument.\n  If this is absent or None then nothing is passed with the pilot\n  arguments (i.e., pickup normal production jobs).\n* Changed configuration so that each configuration section is a\n  \"queue\" which can have a gatekeeper section. This allows pilots to\n  be sent to the same queue on the CE, but having different \"user\"\n  parameters. If there is no \"gatekeeper\" specified then the name of\n  the configuration section name is the gatekeeper contact string, as\n  before.\n* New example factory configuration file, factory.conf-example makes\n  use of the above clearer.\n* Many internal changes to support the above feature.\n* Modified the main submit code to sort alphabetically by queue name.\n* cleanLogs.py now much improved. Will read factory log file directory\n  location from factory.conf. Options for verbosity, days to compress\n  after and days to delete after.\n* This release is dedicated to Gary Gygax. d20... 19 hit!\n  http://en.wikipedia.org/wiki/Gary_Gygax\n\n\n2008-02-28 \"Ice Moon\" Release. (r436)\n\n* Upgraded tarball to UHURA12a pilot.\n* Added makefile target to rebuild the wrapper script with pilot\n  tarball downloaded from subversion (you need svn installed!).\n* Changed tarball http download to use BNL cache first, with fallback\n  to Glasgow (Glasgow will rebuild nightly).\n* Much better exception handling if there is a problem polling condor\n  status or panda status - factory raises an internal exception and\n  then skips a cycle.\n* Added an outer level exception handler which gives instructions as\n  to how to report the error then reraises error.\n\n2008-02-08 \"It's REALLY all about me\" Release. (r416)\n\n* Updated to UHURA11b2-2 pilot.\n\n\n2008-02-08 \"It's all about me\" Release. (r410)\n\n* Major change to pilot delivery mechanism, attaching pilot tarball as\n  a uuencoded suffix of the wrapper script. This cures problems with\n  batch systems which do not deliver \"input\" files properly on STDIN\n  (bqs - there was nothing there; condor - crashes job).\n  *** YOU SHOULD REMOVE THE input PARAMETER FROM factory.conf.\n* Added GTAG environment variable to move towards integration with\n  panda dashboard (as autopilot does).\n* Fixed a bug where the factory would crash if the condor_q query\n  failed (factory now raises a CondorStatusFailure exception, catches\n  it and skips a cycle).\n* Fixed a bug where exception arguments were improperly specified.\n* Added the \"-k MEM\" flag to pilot where a site has a memory limit\n  specified (should be used for sites lacking memory).\n* Cosmetic: print timestamp at the beginning of each submit cycle.\n* Upgraded to UHURA11a pilot as default.\n* Improved cleanLogs.py script and renamed mkPilotWrapper.py.", 
            "title": "Deployment"
        }, 
        {
            "location": "/AutoPyFactoryConfiguration/", 
            "text": "Configuration of AutoPyFactory \n\n\n 1  About this Document\n\n\n \n 2  Applicable versions\n\n\n \n 3  Format of the configuration files\n\n\n \n 4  Reference Manual\n\n\n \n 5  Configuration files\n \n\n\n 5.1  sysconfig/autopyfactory\n\n\n \n 5.2  logrotation\n\n\n \n 5.3  autopyfactory.conf\n\n\n \n 5.4  queues.conf\n\n\n \n 5.5  proxy.conf\n\n\n \n 5.6  monitor.conf\n\n\n \n 5.7  mappings.conf\n\n\n \n\n \n 6  Workflows\n\n\n \n\n\n\n 1  About this Document \n\n\n\nThis document describes all configuration variables in \nAutoPyFactory\n\n\n\n\n\nConventions used in this document:\n\n\n\n\n\nA \nUser Command Line\n is illustrated by a green box that displays a prompt:\n\n\n\n\n\n  [user@client ~]$\n\n\n\n\n\nA \nRoot Command Line\n is illustrated by a red box that displays the \nroot\n prompt:\n\n\n\n\n\n  [root@factory ~]$\n\n\n\n\n\nLines in a file\n are illustrated by a yellow box that displays the desired lines in a file:\n\n\n\npriorities=1\n\n\n\n\n\n\n\n\n\n 2  Applicable versions \n\n\n\nThis documentation applies to the latest version of APF: 2.4.9\n\n\n\n\n\n 3  Format of the configuration files \n\n\n\nThe format of the configuration files is similar to the Microsoft INI files. The configuration file consists of sections, led by a \n[section]\n header and followed by \nname: value\n entries, with continuations in the style of \nRFC 822\n (see section 3.1.1, \u0093LONG HEADER FIELDS\u0094); \nname=value\n is also accepted. Note that leading whitespace is removed from values. Additional defaults can be provided on initialization and retrieval. Lines beginning with '#' or ';' are ignored and may be used to provide comments.\n\n\nIt accepts interpolation. This means values can contain format strings which refer to other values in the same section, or values in a special \n[DEFAULT]\n section.\n\n\nConfiguration files may include comments, prefixed by specific characters (# and ;). Comments may appear on their own in an otherwise empty line, or may be entered in lines holding values or section names. In the latter case, they need to be preceded by a whitespace character to be recognized as a comment. (For backwards compatibility, only ; starts an inline comment, while # does not.)\n\n\nAs the python package ConfigParser is being used to digest the configuration files, a wider explanation and examples can be found in the \npython documentation page\n. \n\n\n\n 4  Reference Manual \n\n\n\nA detailed description of all configuration variables used in AutoPyFactory can be found \nhere\n\n\n\n\n 5  Configuration files \n\n\n\n\n 5.1  sysconfig/autopyfactory \n\n\n\nThe first set of configuration for the factory is done via the \n/etc/sysconfig/autopyfactory\n config file. \nThis is the configuration file used by the daemon service to decide how to run the factory. \nIt set the following variables: \n\n\n log level: \nWARNING\n, \nINFO\n or \nDEBUG\n\n\n \n time to sleep between internal cycles\n\n \n the username to switch into to. Note the factory will not run as root, but as a non priviledged account\n\n \n the log file\n\n \nAlso, any other modifications to the environment needed by the factory can be set in this file.\nExample\n\n\n\n\nOPTIONS=\"--debug --sleep=60 --runas=autopyfactory --log=/var/log/autopyfactory/autopyfactory.log\"\nCONSOLE_LOG=/var/log/autopyfactory/console.log\nenv | sort \n $CONSOLE_LOG\n\n\n\n\n\n 5.2  logrotation \n\n\n\nThe default location of the log files, as they are in the \n/etc/sysconfig/autopyfactory\n config file, are \n\n\n[root@factory ~]$ ls /var/log/autopyfactory/\n/var/log/autopyfactory/autopyfactory.log\n/var/log/autopyfactory/console.log (when used)\n\n\n\n\nHowever, if the location of the files is different and the \n/etc/sysconfig/autopyfactory\n file has been modified, \nthen the logrotation file \n/etc/logrotate.d/autopyfactory\n needs to be adjusted accordingly.\n\n\n\n\n/var/log/autopyfactory/autopyfactory.log {   \n\\lt\n&\n;-- CHANGE THIS IF NEEDED. \n\n  missingok\n  notifempty\n  sharedscripts\n  size 100M\n  rotate 10\n  prerotate\n    [ -e /etc/init.d/autopyfactory ] \n /etc/init.d/autopyfactory stop \n/dev/null 2\n1 || true\n    sleep 5\n  endscript\n  postrotate\n    sleep 5\n    [ -e /etc/profile ] \n . /etc/profile \n/dev/null 2\n1 || true\n    [ -e /etc/init.d/autopyfactory ] \n /etc/init.d/autopyfactory start \n/dev/null 2\n1 || true\n  endscript\n}\n/var/log/autopyfactory/console.log {    \n\\lt\n&\n;-- CHANGE THIS IF NEEDED. \n\n  missingok\n  notifempty\n  sharedscripts\n  size 50M\n  rotate 2\n  prerotate\n    [ -e /etc/init.d/autopyfactory ] \n /etc/init.d/autopyfactory stop \n/dev/null 2\n1 || true\n    sleep 5\n  endscript\n  postrotate\n    sleep 5\n    [ -e /etc/profile ] \n . /etc/profile \n/dev/null 2\n1 || true\n    [ -e /etc/init.d/autopyfactory ] \n /etc/init.d/autopyfactory start \n/dev/null 2\n1 || true\n  endscript\n}\n\n\n\n\n\n\n\n\n\n 5.3  autopyfactory.conf \n\n\n\nIt is the configuration file where the generic parameters for the whole factory are set. \nIts location and name, \n/etc/autopyfactory/autopyfactory.conf\n, is set in the daemon init script, \n/etc/init.d/autopyfactory\n, and therefore cannot be changed. \n\n\nA detailed description of this file variables can be found in the \nReference Manual\n\n\n\n\n 5.4  queues.conf \n\n\n\nIt is the file where each APFQueue is defined. \nAutoPyFactory is built around the concept of APFQueues. An APFQueue can be seen as the unique combination of these concepts: \n\n\n a wms system queue: where the payload jobs are waiting. The concept of wms queue is an abstraction, and it will depend on what WMS System is being used each case. Whe the WMS System is \nPanDA\n, the wms queue is the panda queue. When the WMS System is a condor pool, a wms queue is each different value of the classad \n+MATCH_APF_QUEUE\n that jobs in that condor pool carry.\n\n \n a batch system queue: where the pilots will be submitted. It could be a CE based on globus GT5, or GT2. It could be a CE based on CREAM. A CE based on HTCondor-CE. It could be an Amazon EC2 workflow managed by condor. It can be a remote condor pool. For all possibilities, chech the \nReference Manual\n\n\n \n\n\nThat, plus an algorithm to decide how many pilots to submit each cycle.\n\n\nTherefore, each APFQueue in AutoPyFactory needs to set, at least, these elements: \n\n\n the plugin to query the wms service being used\n\n \n the name of the queue in that wms service (wmsqueue)\n\n \n the plugin to query the status of previous pilots in the batch system\n\n \n the plugin to submit new pilots to the batch system\n\n \n the plugin(s) to decide how many new pilots to submit each cycle \n\n \n\n\nThe complete list of configuration parameters can be found \nReference Manual\n\n\n\nFor each APFQueue, a section is set in this file. All APFQueues sections become threads running asynchronously. \n\n\n\n 5.5  proxy.conf \n\n\n\nIt is the file where the x509 proxy files are set, when needed. \nThe complete list of configuration parameters can be found \nReference Manual\n\n\n\n\n 5.6  monitor.conf \n\n\n\nIt is the file to setup, if needed, where the factory will send information periodically for web monitor. \nThe complete list of configuration parameters can be found \nReference Manual\n\n\n\n\n 5.7  mappings.conf \n\n\n\nThis file is being used internally by the factory to translate external tools information into internal nomenclature. \nIt defines completely the behavior of the factory. Unless you are an expert on the factory, \nDO NOT TOUCH IT\n.\n\n\n\n 6  Workflows \n\n\n \n\n\n Configuration for WMS being PanDA \nhere\n\n\n \n glideins based workflows. Some details on how to configure the factory to work in a glidein style are \nhere\n\n\n \n Submission to CREAM CE \nhere\n\n\n \n Submission to NorduGrid CE \nhere\n\n\n \n Submission to EC2 \nhere", 
            "title": "Configuration"
        }, 
        {
            "location": "/AutoPyFactoryReferenceManual/", 
            "text": "AutoPyFactory: Reference Manual \n\n\n 1  About this Document\n\n\n \n 2  Applicable versions\n\n\n \n 3  Format of the configuration files\n\n\n \n 4  sysconfig/autopyfactory\n\n\n \n 5  autopyfactory.conf\n\n\n \n 6  queues.conf\n \n\n\n 6.1  generic variables\n\n\n \n 6.2 WMS Status Plugin variables\n\n\n \n 6.3  Batch Status Plugin variables\n\n\n \n 6.4  Sched Plugin variables\n \n\n\n 6.4.1  List of plugins\n\n\n \n 6.4.2  Plugins parameters\n\n\n \n\n \n 6.5  Batch Submit Plugin variables\n\n\n \n\n \n 7  proxy.conf\n\n\n \n 8  monitor.conf\n\n\n \n\n\n\n 1  About this Document \n\n\n\nThis document describes all configuration variables in \nAutoPyFactory\n\n\n\n\n\nConventions used in this document:\n\n\n\n\n\nA \nUser Command Line\n is illustrated by a green box that displays a prompt:\n\n\n\n\n\n  [user@client ~]$\n\n\n\n\n\nA \nRoot Command Line\n is illustrated by a red box that displays the \nroot\n prompt:\n\n\n\n\n\n  [root@factory ~]$\n\n\n\n\n\nLines in a file\n are illustrated by a yellow box that displays the desired lines in a file:\n\n\n\npriorities=1\n\n\n\n\n\n\n\n\n\n 2  Applicable versions \n\n\n\nThis documentation applies to the latest version of APF: 2.4.9\n\n\n\n\n\n 3  Format of the configuration files \n\n\n\nAn explanation of the format of the configuration files in AutoPyFactory can be seen \nhere\n\n\n\n\n 4  sysconfig/autopyfactory \n\n\n\nThe first set of configuration for the factory is done via the \n/etc/sysconfig/autopyfactory\n config file. \nThis is the configuration file used by the daemon service to decide how to run the factory. \nIt set the following variables: \n\n\n log level: \nWARNING\n, \nINFO\n or \nDEBUG\n\n\n \n time to sleep between internal cycles\n\n \n the username to switch into to. Note the factory will not run as root, but as a non priviledged account\n\n \n the log file\n\n \nAlso, any other modifications to the environment needed by the factory can be set in this file.\nExample\n\n\n\n\nOPTIONS=\"--debug --sleep=60 --runas=autopyfactory --log=/var/log/autopyfactory/autopyfactory.log\"\nCONSOLE_LOG=/var/log/autopyfactory/console.log\nenv | sort \n $CONSOLE_LOG\n\n\n\n\n\n 5  autopyfactory.conf \n\n\n\n\n\n\n\n  \n\n    \nvariable\n\n    \ndescription\n\n    \ncomments\n\n  \n\n\n\n\n\n    \nabort_no_queues\n\n    \n decides if the factory should shutdown when there are no active queues. \n Default = False\n\n    \n new in 2.5\n\n  \n\n\n\n    \nauthmanager.enabled\n\n    \nto determine if automatic credentials management is used or not.  Accepted values are True\nFalse \n\n    \n\n  \n\n\n\n    \nauthmanager.sleep\n\n    \n Sleep interval for authmanager thread.  \n\n    \n\n  \n\n\n\n    \nbaseLogDir\n\n    \n where outputs from pilots are stored NOTE: No trailing '/'!!! \n\n    \n \n\n  \n\n\n\n    \nbaseLogDirUrl\n\n    \nwhere outputs from pilots are available via http.  NOTE: It must include the port.  NOTE: No trailing '/'!!! \n\n    \n \n\n  \n\n\n\n    \nbatchstatus.condor.sleep\n\n    \ntime the Condor BatchStatus Plugin waits between cycles Value is in seconds. \n\n    \n \n\n  \n\n\n\n    \nbatchstatus.maxtime\n\n    \nmaximum time while the info is considered reasonable.  If info stored is older than that, is considered not valid, and some NULL output will be returned. \n\n    \n \n\n  \n\n\n\n    \ncycles\n\n    \nmaximum number of times the queues will loop.  None means forever. 0 means the queue threads are not started.\n\n    \n0 meaning the threads are not started is new in 2.4.14 \n\n  \n\n\n\n    \ncleanlogs.keepdays\n\n    \nmaximum number of days the condor logs will be kept, in case they are placed in a subdirectory for an APFQueue that is not being currently managed by AutoPyFactory.  For example, an apfqueue that has been created and used for a short amount of time, and it does not exist anymore.  Still the created logs have to be cleaned at some point... \n\n    \n \n\n  \n\n\n\n    \nenablequeues\n\n    \ndefault value to enable/disable all queues at once.  When True, its value will be overriden by the queue config variable 'enabled', queue by queue.  When False, all queues will stop working, but the factory will still be alive performing basic actions (eg. printing logs). \n\n    \n \n\n  \n\n\n\n    \nfactoryId\n\n    \nName that the factory instance will have in the APF web monitor.  Make factoryId something descriptive and unique for your factory, for example \n-\n-\n (e.g. BNL-gridui11-jhover) \n\n    \n \n\n  \n\n\n\n    \nfactoryAdminEmail\n\n    \nEmail of the local admin to contact in case of a problem with an specific APF instance. \n\n    \n \n\n  \n\n\n\n    \nfactorySMTPServer\n\n    \nServer to use to send alert emails to admin.  \n\n    \n \n\n  \n\n\n\n    \nfactoryUser\n\n    \naccount under which APF will run \n\n    \n \n\n  \n\n\n\n    \nfactory.sleep\n\n    \nsleep time between cycles in mainLoop in Factory object Value is in seconds. \n\n    \n \n\n  \n\n\n\n    \nmappingsConf\n\n    \nlocal path to the configuration file with the mappings: for example, globus2info, jobstatus2info, etc. \n\n    \n \n\n  \n\n\n\n    \nmaxperfactory.maximum\n\n    \nmaximum number of condor jobs to be running at the same time per Factory.  It is a global number, used by all APFQueues submitting pilots with condor.  The value will be used by MaxPerFactorySchedPlugin plugin \n\n    \n \n\n  \n\n\n\n    \nmonitorConf\n\n    \nlocal path to the configuration file for Monitor plugins. \n\n    \n \n\n  \n\n\n\n\n    \nlogserver.allowrobots\n\n    \nif false, creates a robots.txt file in the docroot.  Valid valudes are True\nFalse \n\n    \n \n\n  \n\n\n\n\n    \nlogserver.enabled\n\n    \ndetermines if batch logs are exported via HTTP.  Valid values are True\nFalse \n\n    \n \n\n  \n\n\n\n\n    \nlogserver.index\n\n    \ndetermines if automatic directory indexing is allowed when log directories are browsed.  Valid values are True\nFalse \n\n    \n \n\n  \n\n\n\n\n    \nproxyConf\n\n    \nlocal path to the configuration file for automatic proxy management.  NOTE: must be a local path, not a URI.  \n\n    \n \n\n  \n\n\n\n\n    \nproxymanager.enabled\n\n    \nto determine if automatic proxy management is used or not.  Accepted values are True\nFalse \n\n    \n deprecated in 2.4.10\n\n  \n\n\n\n\n    \nproxymanager.sleep\n\n    \n Sleep interval for proxymanager thread.  \n\n    \n deprecated in 2.4.10\n\n  \n\n\n\n\n    \nqueueConf\n\n    \nURI plus path to the configuration file for APF queues.  NOTE: Must be expressed as a URI (file:// or http://) Cannot be used at the same time that queueDirConf \n\n    \n \n\n  \n\n\n\n\n    \nqueueDirConf\n\n    \ndirectory with a set of configuration files, all of them to be used at the same time.  i.e.  /etc/autopyfactory/queues.d/ Cannot be used at the same time that queueConf \n\n    \n \n\n  \n\n\n\n\n    \nreconfig\n\n    \n boolean to decide if queues configuration needs to be recalculated periodically  \n Default=True.\n\n    \n New in 2.4.14 \n\n  \n\n\n\n\n    \nwmsstatus.condor.sleep\n\n    \ntime to wait between cycles when WMS Status Plugin is Condor. Value is in seconds. \n\n    \n \n\n  \n\n\n\n\n    \nwmsstatus.maximum\n\n    \nmaximum time while the info is considered reasonable.  If info stored is older than that, is considered not valid, and some NULL output will be returned. \n\n    \n \n\n  \n\n\n\n\n    \nwmsstatus.panda.sleep\n\n    \ntime to wait between cycles when WMS Status Plugin is Panda. Value is in seconds. \n\n    \n \n\n  \n\n\n\n\n\n\n\n\n\n 6  queues.conf \n\n\n\n\n\n\n\n\n 6.1  generic variables \n\n\n\n\n\n  \n\n    \nvariable\n\n    \ndescription\n\n    \ncomments\n\n  \n\n\n \n    \napfqueue.sleep\n \n    \n sleep time between cycles in APFQueue object.  Value is in seconds.    \n \n    \n \n \n  \n\n\n \n    \nbatchqueue\n \n    \n the Batch system related queue name.  E.g. the PanDA queue name (formerly called nickname) \n \n    \n mostly needed for the wrapper. Not needed with APF 2.4.7+ and wrapper-0.9.16+\n \n  \n\n\n \n    \ncleanlogs.keepdays\n \n    \n maximum number of days the condor logs will be kept \n \n    \n \n \n  \n\n\n \n    \ncloud\n \n    \n is the cloud this queue is in. You should set this to suppress pilot submission when the cloud goes offline N.B. Panda clouds are UPPER CASE, e.g., UK \n \n    \n Deprecated \n \n  \n\n\n \n    \nenabled\n \n    \n determines if each queue section must be used by AutoPyFactory or not. Allows to disable a queue without commenting out all the values.  Valid values are True\nFalse. \n \n    \n \n \n  \n\n\n\n    \nexecutable\n\n    \n path to the script which will be executed.  As the purpose of the factory is to submit jobs to the different resources (local batch queues, grid sites, etc.) an executable, with its corresponding list of input arguments, is needed.  This executable can be anything.  \n In principle, details on how to install the executable and the list of arguments are out of the scope of this documentation. However, in the case of ATLAS experiment, executable documentation can be found \nhere\n \n    \n \n \n  \n \n\n\n    \nexecutable.arguments\n\n    \n input options to be passed verbatim to the executable script. \n \n    \n \n\n  \n \n\n\n    \nmonitorsection\n\n    \n section in monitor.conf where info about the actual monitor plugin can be found.  The value can be a single section or a split by comma list of sections.  Monitor plugins handle job info publishing to one or more web monitor/dashboards.  \n \n    \n \n \n  \n\n\n\n    \nstatus\n\n    \n can be \"test\", \"offline\" or \"online\" \n \n    \n \n\n  \n\n\n\n    \nwmsqueue\n\n    \n the WMS system queue name.  E.g. the PanDA siteid name \n \n    \n \n\n  \n \n\n\n\n\n\n\n\n 6.2  WMS Status Plugin variables \n\n\n\n\n\n  \n\n    \nvariable\n\n    \ndescription\n\n    \ncomments\n\n  \n\n\n\n\n\n    \nwmsstatusplugin\n\n    \n WMS Status Plugin. \n\n    \n \n\n  \n\n\n\n    \nwmsstatus.condor.queryargs\n\n    \n list of command line input options to be included in the query command \nverbatim\n. E.g.  wmsstatus.condorqueryargs = -name \n ...  \n\n    \n \n\n  \n\n\n\n\n\n\n 6.3  Batch Status Plugin variables \n\n\n\n\n\n  \n\n    \nvariable\n\n    \ndescription\n\n    \ncomments\n\n  \n\n\n\n\n\n    \nbatchstatusplugin\n\n    \n Batch Status Plugin. \n\n    \n \n\n  \n\n\n\n    \nbatchstatus.condor.queryargs\n\n    \n list of command line input options to be included in the query command \nverbatim\n. E.g.  batchstatus.condor.queryargs = -name \n -pool \ncentralmanagerhostname[:portnumber]\n \n\n    \n \n\n  \n\n\n\n\n\n\n 6.4  Sched Plugin variables \n\n\n\nSpecific Scheduler Plugin implementing the algorithm deciding how many new pilots to submit next cycle.\n\nThe value can be a single Plugin or a split by comma list of Plugins.\n\nIn the case of more than one plugin, each one will acts as a filter with respect to the value returned by the previous one.\n\nBy selecting the right combination of Plugins in a given order, a complex algorithm can be built. \n \nE.g., the algorithm can start by using Ready Plugin, which will determine the number of pilots based on the number of activated jobs in the WMS queue and the number of already submitted pilots.\n\nAfter that, this number can be filtered to a maximum (MaxPerCycleSchedPlugin) or a minimum (MinPerCycleSchedPlugin) number of pilots.\n\nOr even can be filtered to a maximum number of pilots per factory (MaxPerFactorySchedPlugin). \nAlso it can be filtered depending on the status of the wmsqueue (StatusTestSchedPlugin, StatusOfflineSchedPlugin).\n\n\nOnce the plugin (or list of plugins) to be used has been set, the corresponding list of specific variables need to be set as well when needed. \nExample:\n\n\n\n\nschedplugin = Ready, Scale, MaxPerCycle, MaxPending\nsched.scale.factor = 0.25\nsched.maxpercycle.maximum = 100\nsched.maxpending.maximum = 10\n\n\n\n\n\n 6.4.1  List of plugins \n\n\n\nThe following table lists the complete list of available sched plugins.\n\n\n\n\n  \n\n    \nplugin\n\n    \ndescription\n\n    \ncomments\n\n  \n\n\n\n    \nFixed\n\n    \nAlways submits a fixed number of pilots. \n\n    \n \n\n  \n\n\n\n    \nKeepNRunning\n\n    \n Strives to keep a certain number of pilots running, regardless anything else. \n\n    \n \n\n  \n\n\n\n    \nMaxPending\n\n    \n  If there are currently pending pilots, imposses a limit on how many more to submit. If there are no currently any pending pilots, that limit is not applied.   \n\n    \n \n\n  \n\n\n\n    \nMaxPerCycle\n\n    \n Imposses a limit on the maximum number of pilots to be submitted each cycle.  \n\n    \n \n\n  \n\n\n\n    \nMaxToRun\n\n    \n  Imposses a maximum limit on the total number of pilots, including both the currently ones running and pending. \n\n    \n \n\n  \n\n\n\n    \nMinPending\n\n    \n Submits enough pilots to try keepinig a minimum number of them pending. \n\n    \n \n\n  \n\n\n\n    \nMinPerCycle\n\n    \n  Imposses a limit on the minimum number of pilots to be submitted each cycle. \n\n    \n \n\n  \n\n\n\n    \nReady\n\n    \nChecks the number of jobs ready to be run in the WMS service, the number of previously submitted pilot still in idle state, and calculates the difference. \n\n    \n \n\n  \n\n\n\n    \nScale\n\n    \n Multiplies by a factor the decision made by the previous plugin in the chain.  \n\n    \n \n\n  \n\n\n\n    \nStatusOffline\n\n    \n Makes a decission about how many pilots to submit when the WMS queue is in internal status \noffline\n. \n This plugin was introduced mostly for the case when WMS Status plugin is \nPanda\n, so it is not too much helpful in other cases.  \n\n    \n \n\n  \n\n\n\n    \nStatusTest\n\n    \n Makes a decission about how many pilots to submit when the WMS queue is in internal status \ntest\n. \nThis plugin was introduced mostly for the case when WMS Status plugin is \nPanda\n, so it is not too much helpful in other cases.  \n\n    \n \n\n  \n\n\n\n    \nThrottle\n\n    \n  Reduces the number of pilots to be submitted if it detects a significative number of previously submitted pilots have finished too fast, as that may be an indication of a broken node in the target resource (a.k.a. a black hole). \n\n    \n New in 2.4.7\n\n  \n\n\n\n    \nWeightedActivated\n\n    \n Similar to Ready, but applying multiply factors to the number of ready jobs and idle pilots. \n\n    \n \n\n  \n\n\n\n\n\n\n 6.4.2  Plugins parameters \n\n\n\nSome of the sched plugins accept one or more parameters.\nThe following table lists all valid parameters for all sched plugins.\n\n\n\n\n  \n\n    \nvariable\n\n    \ndescription\n\n    \ncomments\n\n  \n\n\n\n    \nsched.fixed.pilotspercycle\n\n    \n fixed number of pilots to be submitted each cycle \n\n    \n Mandatory \n\n  \n\n\n\n    \nsched.keepnrunning.keep_running\n\n    \n number of total jobs to keep running and/or pending. \n If None, then it changes the sense of input from  new jobs (relative) to a target number (absolute)\n\n    \n \n\n  \n\n\n\n    \nsched.maxpending.maximum\n\n    \n maximum number of pilots to be pending \n\n    \n Mandatory\n\n  \n\n\n\n    \nsched.maxpercycle.maximum\n\n    \n maximum number of pilots to be submitted per cycle \n\n    \n Mandatory\n\n  \n\n\n\n    \nsched.maxtorun.maximum\n\n    \n maximum number of pilots allowed at a time.  \n\n    \n Mandatory \n\n  \n\n\n\n    \nsched.minpending.minimum\n\n    \n minimum number of pilots to be pending \n\n    \n Mandatory\n\n  \n\n\n\n    \nsched.minpercycle.minimum\n\n    \n minimum number of pilots to be submitted per cycle \n\n    \n Mandatory \n\n  \n\n\n\n    \nsched.ready.offset\n\n    \n the minimum value in the number of ready jobs to trigger submission. \n\n    \n Optional \n\n  \n\n\n\n    \nsched.scale.factor\n\n    \n scale factor to correct the previous value of the number of pilots.  \n Value is a float number. \n Default = 1.0 \n\n    \n \n\n  \n\n\n\n    \nsched.statusoffline.pilots\n\n    \n number of pilots to submit when the wmsqueue or the cloud is in status = offline \n Default = 0 \n\n    \n \n\n  \n\n\n\n    \nsched.statustest.pilots\n\n    \n number of pilots to submit when the wmsqueue is in status = test \n Default = 0 \n\n    \n \n\n  \n\n\n\n    \nsched.throttle.interval\n\n    \n the time windows we observe. \n Value in seconds. \n Default = last hour \n\n    \n \n\n  \n\n\n\n    \nsched.throttle.maxtime\n\n    \n maximum WallTime for a pilot to be declared \"too short\". \n Value in seconds. \n Default = 10 \n\n    \n \n\n  \n\n\n\n    \nsched.throttle.ratio\n\n    \n minimum ratio too short pilots over total pilots to decide there is a blackhole \n Default = 0.5 \n\n    \n \n\n  \n\n\n\n    \nsched.throttle.submit\n\n    \n number of pilots to submit when a blackhole is detected \n Default = 1 \n\n    \n \n\n  \n\n\n\n    \nsched.weightedactivated.activated\n\n    \n weight to be applied to the number of jobs activated \n Value is a float \n Default = 1.0 \n\n    \n \n\n  \n\n\n\n    \nsched.weightedactivated.pending\n\n    \n weight to be applied to the number of pilots pending \n Value is a float \n Default = 1.0 \n\n    \n \n\n  \n\n\n\n\n\n\n\n\n 6.5  Batch Submit Plugin variables \n\n\n\n\n\n  \n\n    \nvariable\n\n    \ndescription\n\n    \ncomments\n\n  \n\n\n\n\n\n    \nbatchsubmitplugin\n\n    \n Batch Submit Plugin.  Currently available options are: CondorGT2, CondorGT5, CondorCREAM, CondorLocal, CondorLSF, CondorEC2, CondorDeltaCloud. \n\n    \n \n\n  \n\n\n\n    \nConfiguration when batchsubmitplugin is condorgt2\n\n  \n\n\n\n    \nbatchsubmit.condorgt2.condor_attributes\n\n    \n list of condor attributes, splited by comma, to be included in the condor submit file \nverbatim\n \ne.g. +Experiment = \"ATLAS\",+VO = \"usatlas\",+Job_Type = \"cas\" Can be used to include any line in the Condor-G file that is not otherwise added programmatically by AutoPyFactory.  Note the following directives are added by default: \ntransfer_executable=True \nstream_output=False \nstream_error=False \nnotification=Error \ncopy_to_spool=false \n\n    \n \n\n  \n\n\n\n    \nbatchsubmit.condorgt2.environ\n\n    \n list of environment variables, splitted by white spaces, to be included in the condor attribute environment \nverbatim\n Therefore, the format should be env1=var1 env2=var2 envN=varN split by whitespaces. \n\n    \n \n\n  \n\n\n\n    \nbatchsubmit.condorgt2.gridresource\n\n    \n name of the CE (e.g. gridtest01.racf.bnl.gov/jobmanager-condor) \n\n    \n \n\n  \n\n\n\n    \nbatchsubmit.condorgt2.proxy\n\n    \n name of the proxy handler in proxymanager for automatic proxy renewal (See etc/proxy.conf) None if no automatic proxy renewal is desired. \n\n    \n \n\n  \n\n\n\n    \nbatchsubmit.condorgt2.submitargs\n\n    \n list of command line input options to be included in the submission command \nverbatim\n e.g.  batchsubmit.condorgt2.submitargs = -remote my_schedd will drive into a command like condor_submit -remote my_schedd submit.jdl \n\n    \n \n\n  \n\n\n\n    \nGlobusRSL GRAM2 variables\n\n  \n\n\n\n    \ngram2\n\n    \n The following are GRAM2 RSL variables.  They are just used to build batchsubmit.condorgt2.globusrsl (if needed) The globusrsl directive in the condor submission file looks like globusrsl=(jobtype=single)(queue=short) Documentation can be found here: \nhttp://www.globus.org/toolkit/docs/2.4/gram/gram_rsl_parameters.html\n \n\n    \n \n\n  \n\n\n\n    \nglobusrsl.gram2.\nargument\n\n    \n globusrsl.gram2.count \n globusrsl.gram2.directory \n globusrsl.gram2.dryRun \n globusrsl.gram2.environment \n globusrsl.gram2.executable \n globusrsl.gram2.gramMyJob \n globusrsl.gram2.hostCount \n globusrsl.gram2.jobType \n globusrsl.gram2.maxCpuTime \n globusrsl.gram2.maxMemory \n globusrsl.gram2.maxTime \n globusrsl.gram2.maxWallTime \n globusrsl.gram2.minMemory \n globusrsl.gram2.project \n globusrsl.gram2.queue \n globusrsl.gram2.remote_io_url \n globusrsl.gram2.restart \n globusrsl.gram2.save_state \n globusrsl.gram2.stderr \n globusrsl.gram2.stderr_position \n globusrsl.gram2.stdin \n globusrsl.gram2.stdout \n globusrsl.gram2.stdout_position \n globusrsl.gram2.two_phase \n\n    \n \n\n  \n\n\n\n    \nglobusrsl.gram2.globusrsl\n\n    \n GRAM RSL directive.  If this variable is not setup, then it will be built programmatically from all non empty globusrsl.gram2.XYZ variables.  If this variable is setup, then its value will be taken \nverbatim\n, and all possible values for globusrsl.gram2.XYZ variables will be ignored.  \n\n    \n \n\n  \n\n\n\n    \nglobusrsl.gram2.globusrsladd\n\n    \n custom fields to be added \nverbatim\n to the GRAM RSL directive, after it has been built either from globusrsl.gram2.globusrsl value or from all globusrsl.gram2.XYZ variables.  e.g. (condorsubmit=('+AccountingGroup' '\\\"group_atlastest.usatlas1\\\"')('+Requirements' 'True')) \n\n    \n \n\n  \n\n\n\n    \nConfiguration when batchsubmitplugin is condorgt5\n\n  \n\n\n\n    \nbatchsubmit.condorgt5.condor_attributes\n\n    \n list of condor attributes, splited by comma, to be included in the condor submit file \nverbatim\n e.g. +Experiment = \"ATLAS\",+VO = \"usatlas\",+Job_Type = \"cas\" Can be used to include any line in the Condor-G file that is not otherwise added programmatically by AutoPyFactory.  Note the following directives are added by default: \ntransfer_executable=True \nstream_output=False \nstream_error=False \nnotification=Error \ncopy_to_spool=false \n\n    \n \n\n  \n\n\n\n    \nbatchsubmit.condorgt5.environ\n\n    \n list of environment variables, splitted by white spaces, to be included in the condor attribute environment \nverbatim\n Therefore, the format should be env1=var1 env2=var2 envN=varN split by whitespaces. \n\n    \n \n\n  \n\n\n\n    \nbatchsubmit.condorgt5.gridresource\n\n    \n name of the CE (e.g. gridtest01.racf.bnl.gov/jobmanager-condor) \n\n    \n \n\n  \n\n\n\n    \nbatchsubmit.condorgt5.proxy\n\n    \n name of the proxy handler in proxymanager for automatic proxy renewal (See etc/proxy.conf) None if no automatic proxy renewal is desired. \n\n    \n \n\n  \n\n\n\n    \nbatchsubmit.condorgt5.submitargs\n\n    \n list of command line input options to be included in the submission command \nverbatim\n e.g.  batchsubmit.condorgt2.submitargs = -remote my_schedd will drive into a command like condor_submit -remote my_schedd submit.jdl \n\n    \n \n\n  \n\n\n\n    \nGlobusRSL GRAM5 variables\n\n  \n\n\n\n    \ngram5\n\n    \n The following are GRAM5 RSL variables.  They are just used to build batchsubmit.condorgt5.globusrsl (if needed) The globusrsl directive in the condor submission file looks like globusrsl=(jobtype=single)(queue=short) Documentation can be found here: \nhttp://www.globus.org/toolkit/docs/5.2/5.2.0/gram5/user/#gram5-user-rsl\n  \n\n    \n \n\n  \n\n\n\n    \nglobusrsl.gram5.\nargument\n\n    \n globusrsl.gram5.count \n globusrsl.gram5.directory \n globusrsl.gram5.dry_run \n globusrsl.gram5.environment \n globusrsl.gram5.executable \n globusrsl.gram5.file_clean_up \n globusrsl.gram5.file_stage_in \n globusrsl.gram5.file_stage_in_shared \n globusrsl.gram5.file_stage_out \n globusrsl.gram5.gass_cache \n globusrsl.gram5.gram_my_job \n globusrsl.gram5.host_count \n globusrsl.gram5.job_type \n globusrsl.gram5.library_path \n globusrsl.gram5.loglevel \n globusrsl.gram5.logpattern \n globusrsl.gram5.max_cpu_time \n globusrsl.gram5.max_memory \n globusrsl.gram5.max_time \n globusrsl.gram5.max_wall_time \n globusrsl.gram5.min_memory \n globusrsl.gram5.project \n globusrsl.gram5.proxy_timeout \n globusrsl.gram5.queue \n globusrsl.gram5.remote_io_url \n globusrsl.gram5.restart \n globusrsl.gram5.rsl_substitution \n globusrsl.gram5.savejobdescription \n globusrsl.gram5.save_state \n globusrsl.gram5.scratch_dir \n globusrsl.gram5.stderr \n globusrsl.gram5.stderr_position \n globusrsl.gram5.stdin \n globusrsl.gram5.stdout \n globusrsl.gram5.stdout_position \n globusrsl.gram5.two_phase \n globusrsl.gram5.username \n\n    \n \n\n  \n\n\n\n    \nglobusrsl.gram5.globusrsl\n\n    \n GRAM RSL directive.  If this variable is not setup, then it will be built programmatically from all non empty globusrsl.gram5.XYZ variables.  If this variable is setup, then its value will be taken \nverbatim\n, and all possible values for globusrsl.gram5.XYZ variables will be ignored.  \n\n    \n \n\n  \n\n\n\n    \nglobusrsl.gram5.globusrsladd\n\n    \n custom fields to be added \nverbatim\n to the GRAM RSL directive, after it has been built either from globusrsl.gram5.globusrsl value or from all globusrsl.gram5.XYZ variables.  e.g. (condorsubmit=('+AccountingGroup' '\\\"group_atlastest.usatlas1\\\"')('+Requirements' 'True')) \n\n    \n \n\n  \n\n\n\n    \nConfiguration when batchsubmitplugin is condorcream\n\n  \n\n\n \n    \nbatchsubmit.condorcream.batch\n \n    \n local batch system (pbs, sge...) \n \n \n \n  \n\n\n \n    \nbatchsubmit.condorcream.condor_attributes\n \n    \n list of condor attributes, splited by comma, to be included in the condor submit file \nverbatim\n e.g. +Experiment = \"ATLAS\",+VO = \"usatlas\",+Job_Type = \"cas\" Can be used to include any line in the Condor-G file that is not otherwise added programmatically by AutoPyFactory.  Note the following directives are added by default: \ntransfer_executable=True \nstream_output=False \nstream_error=False \nnotification=Error \ncopy_to_spool=false \n \n \n \n  \n\n\n \n    \nbatchsubmit.condorcream.environ\n \n    \n list of environment variables, splitted by white spaces, to be included in the condor attribute environment \nverbatim\n Therefore, the format should be env1=var1 env2=var2 envN=varN split by whitespaces. \n \n \n \n  \n\n\n \n    \nbatchsubmit.condorcream.gridresource\n \n    \n grid resource, built from other vars using interpolation: batchsubmit.condorcream.gridresource = %(batchsubmit.condorcream.webservice)s:%(batchsubmit.condorcream.port)s/ce-cream/services/CREAM2 %(batchsubmit.condorcream.batch)s %(batchsubmit.condorcream.queue)s \n \n \n \n  \n\n\n \n    \nbatchsubmit.condorcream.port\n \n    \n port number. \n \n \n \n  \n\n\n \n    \nbatchsubmit.condorcream.proxy\n \n    \n name of the proxy handler in proxymanager for automatic proxy renewal (See etc/proxy.conf) None if no automatic proxy renewal is desired. \n \n \n \n  \n\n\n \n    \nbatchsubmit.condorcream.queue\n \n    \n queue within the local batch system (e.g. short) \n \n \n \n  \n\n\n \n    \nbatchsubmit.condorcream.submitargs\n \n    \n list of command line input options to be included in the submission command \nverbatim\n e.g.  batchsubmit.condorgt2.submitargs = -remote my_schedd will drive into a command like condor_submit -remote my_schedd submit.jdl \n \n \n \n  \n\n\n \n    \nbatchsubmit.condorcream.webservice\n \n    \n web service address (e.g. ce04.esc.qmul.ac.uk:8443/ce-cream/services/CREAM2) \n \n \n \n  \n\n\n\n    \nConfiguration when batchsubmitplugin is condorosgce\n\n  \n\n\n\n    \nbatchsubmit.condorosgce.condor_attributes\n\n    \n list of condor attributes, splited by comma, to be included in the condor submit file \nverbatim\n \n\n    \n \n\n  \n\n\n\n    \nbatchsubmit.condorosgce.gridresource\n\n    \n to be used in case schedd and collector are the same \n\n    \n \n\n  \n\n\n\n    \nbatchsubmit.condorosgce.port\n\n    \n port number of the remote HTCondor-CE \n default=9619 \n\n    \n new in 2.4.7\n\n  \n\n\n\n    \nbatchsubmit.condorosgce.proxy\n\n    \n name of the proxy handler in proxymanager for automatic proxy renewal (See etc/proxy.conf) None if no automatic proxy renewal is desired. \n\n    \n \n\n  \n\n\n\n    \nConfiguration when batchsubmitplugin is condorec2\n\n  \n\n\n \n    \nbatchsubmit.condorec2.gridresource\n \n    \n ec2 service's URL (e.g. \nhttps://ec2.amazonaws.com/\n ) \n \n    \n \n \n  \n\n\n \n    \nbatchsubmit.condorec2.submitargs\n \n    \n list of command line input options to be included in the submission command \nverbatim\n e.g.  batchsubmit.condorgt2.submitargs = -remote my_schedd will drive into a command like condor_submit -remote my_schedd submit.jdl \n \n    \n \n \n  \n\n\n \n    \nbatchsubmit.condorec2.condor_attributes\n \n    \n list of condor attributes, splited by comma, to be included in the condor submit file \nverbatim\n \n \n    \n \n \n  \n\n\n \n    \nbatchsubmit.condorec2.environ\n \n    \n list of environment variables, splitted by white spaces, to be included in the condor attribute environment \nverbatim\n Therefore, the format should be env1=var1 env2=var2 envN=varN split by whitespaces. \n \n    \n \n \n  \n\n\n \n    \nbatchsubmit.condorec2.ami_id\n \n    \n identifier for the VM image, previously registered in one of Amazon's storage service (S3 or EBS) \n\n    \n \n \n  \n\n\n \n    \nbatchsubmit.condorec2.ec2_spot_price\n \n    \n max price to pay, in dollars to three decimal places. e.g. .040 \n\n    \n \n \n  \n\n\n \n    \nbatchsubmit.condorec2.instance_type\n \n    \n hardware configurations for instances to run on, .e.g m1.medium \n \n    \n \n \n  \n\n\n \n    \nbatchsubmit.condorec2.user_data\n \n    \n up to 16Kbytes of contextualization data.  This makes it easy for many instances to share the same VM image, but perform different work. \n\n    \n \n \n  \n\n\n \n    \nbatchsubmit.condorec2.access_key_id\n \n    \n path to file with the EC2 Access Key ID \n \n    \n \n \n  \n\n\n \n    \nbatchsubmit.condorec2.secret_access_key\n \n    \n path to file with the EC2 Secret Access Key \n \n    \n \n \n  \n\n\n \n    \nbatchsubmit.condorec2.proxy\n \n    \n name of the proxy handler in proxymanager for automatic proxy renewal (See etc/proxy.conf) None if no automatic proxy renewal is desired. \n \n    \n \n \n  \n\n\n\n    \nConfiguration when batchsubmitplugin is condordeltacloud\n\n  \n\n\n \n    \nbatchsubmit.condordeltacloud.gridresource\n \n    \n ec2 service's URL (e.g. \nhttps://deltacloud.foo.org/api\n ) \n \n    \n \n \n  \n\n\n \n    \nbatchsubmit.condordeltacloud.username\n \n    \n credentials in DeltaCloud \n \n    \n \n \n  \n\n\n \n    \nbatchsubmit.condordeltacloud.password_file\n \n    \n path to the file with the password \n \n    \n \n \n  \n\n\n \n    \nbatchsubmit.condordeltacloud.image_id\n \n    \n identifier for the VM image, previously registered with the cloud service. \n \n    \n \n \n  \n\n\n \n    \nbatchsubmit.condordeltacloud.keyname\n \n    \n in case of using SSH, the command keyname specifies the identifier of the SSH key pair to use.  \n \n    \n \n \n  \n\n\n \n    \nbatchsubmit.condordeltacloud.realm_id\n \n    \n selects one between multiple locations the cloud service may have. \n\n    \n \n \n  \n\n\n \n    \nbatchsubmit.condordeltacloud.hardware_profile\n \n    \n selects one between the multiple hardware profiles the cloud service may provide \n \n    \n \n \n  \n\n\n \n    \nbatchsubmit.condordeltacloud.hardware_profile_memory\n \n    \n customize the hardware profile \n \n    \n \n \n  \n\n\n \n    \nbatchsubmit.condordeltacloud.hardware_profile_cpu\n \n    \n customize the hardware profile \n \n    \n \n \n  \n\n\n \n    \nbatchsubmit.condordeltacloud.hardware_profile_storage\n \n    \n customize the hardware profile \n \n    \n \n \n  \n\n\n \n    \nbatchsubmit.condordeltacloud.user_data\n \n    \n contextualization data \n \n    \n \n \n  \n\n\n\n    \nConfiguration when batchsubmitplugin is condorlocal\n\n  \n\n\n\n    \nbatchsubmit.condorlocal.condor_attributes\n\n    \n list of condor attributes, splited by comma, to be included in the condor submit file \nverbatim\n e.g. +Experiment = \"ATLAS\",+VO = \"usatlas\",+Job_Type = \"cas\" Can be used to include any line in the Condor-G file that is not otherwise added programmatically by AutoPyFactory.  Note the following directives are added by default: \nuniverse = vanilla \ntransfer_executable = True \nshould_transfer_files = IF_NEEDED \n+TransferOutput = \"\" \nstream_output=False \nstream_error=False \nnotification=Error \nperiodic_remove = (JobStatus \n= 5 \n (CurrentTime - EnteredCurrentStatus) \n 3600) \n (JobStatus =\n 1 \n globusstatus \n!\n 1 \n (CurrentTime - EnteredCurrentStatus) \n 86400) \nTo be used in CondorLocal Batch Submit Plugin. \n\n    \n \n\n  \n\n\n\n    \nbatchsubmit.condorlocal.environ\n\n    \n list of environment variables, splitted by white spaces, to be included in the condor attribute environment \nverbatim\n To be used by CondorLocal Batch Submit Plugin.  Therefore, the format should be env1=var1 env2=var2 envN=varN split by whitespaces. \n\n    \n \n\n  \n\n\n\n    \nbatchsubmit.condorlocal.proxy\n\n    \n name of the proxy handler in proxymanager for automatic proxy renewal (See etc/proxy.conf) None if no automatic proxy renewal is desired. \n\n    \n \n\n  \n\n\n\n    \nbatchsubmit.condorlocal.submitargs\n\n    \n list of command line input options to be included in the submission command \nverbatim\n e.g.  batchsubmit.condorgt2.submitargs = -remote my_schedd will drive into a command like condor_submit -remote my_schedd submit.jdl \n\n    \n \n\n  \n\n\n\n    \nConfiguration when batchsubmitplugin is condorlsf\n\n  \n\n\n\n    \nbatchsubmit.condorlsf.proxy\n\n    \n name of the proxy handler in proxymanager for automatic proxy renewal (See etc/proxy.conf) None if no automatic proxy renewal is desired. \n\n    \n \n\n  \n\n\n\n    \nConfiguration when batchsubmitplugin is nordugrid\n\n  \n\n\n\n    \nbatchsubmit.condornordugrid.gridresource\n\n    \n name of the ARC CE i.e. lcg-lrz-ce2.grid.lrz.de \n\n    \n \n\n  \n\n\n\n    \nnordugridrsl\n\n    \n Entire RSL line.  i.e. (jobname = 'prod_pilot')(queue=lcg)(runtimeenvironment = APPS/HEP/ATLAS-SITE-LCG)(runtimeenvironment = ENV/PROXY ) (environment = ('APFFID' 'voatlas94') ('PANDA_JSID' 'voatlas94') ('GTAG' 'http://voatlas94.cern.ch/pilots/2012-11-19/LRZ-LMU_arc/$(Cluster).$(Process).out') ('RUCIO_ACCOUNT' 'pilot') ('APFCID' '$(Cluster).$(Process)') ('APFMON' 'http://apfmon.lancs.ac.uk/mon/') ('FACTORYQUEUE' 'LRZ-LMU_arc')  \n\n    \n \n\n  \n\n\n\n    \nnordugridrsladd\n\n    \n A given tag to be added to the Nordugrid RSL line \n\n    \n \n\n  \n\n\n\n    \nnordugridrsl.addenv.\n\n    \n A given tag to be added within the 'environment' tag to the Nordugrid RSL line i.e. nordugridrsl.addenv.RUCIO_ACCOUNT = pilot will be added as ('RUCIO_ACCOUNT' 'pilot' ) \n\n    \n \n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 7  proxy.conf \n\n\n\n\n\n\n\n\n\n  \n\n    \nvariable\n\n    \ndescription\n\n    \ncomments\n\n  \n\n\n \n    \nbaseproxy\n \n    \n If used, create a very long-lived proxy, e.g. grid-proxy-init -valid 720:0 -out /tmp/plainProxy Note that maintenance of this proxy must occur completely outside of APF. \n \n    \n Only needed if flavor=voms\n \n  \n\n\n \n    \nchecktime\n \n    \n How often to check proxy validity, in seconds\n \n    \n \n \n  \n\n\n \n    \nflavor\n \n    \n voms or myproxy. voms directly generates proxy using cert or baseproxy myproxy retrieves a proxy from myproxy, then generates the target proxy against voms using it as baseproxy.\n \n    \n \n \n  \n\n\n \n    \ninitdelay\n \n    \n In seconds, how long to wait before generating. Needed for MyProxy when using cert authentication--we need to allow time for the auth credential to be generated (by another proxymanager profile). \n \n    \n \n \n  \n\n\n \n    \ninterruptcheck\n \n    \n Frequency to check for keyboard/signal interrupts, in seconds\n \n    \n \n \n  \n\n\n \n    \nlifetime\n \n    \n Initial lifetime, in seconds (604800 = 7 days)\n \n    \n \n \n  \n\n\n \n    \nminlife\n \n    \n Minimum lifetime of VOMS attributes for a proxy (renew if less) in seconds\n \n    \n \n \n  \n\n\n \n    \nmyproxy_hostname\n \n    \n Myproxy server host. \n \n    \n \n \n  \n\n\n \n    \nmyproxy_passphrase\n \n    \n Passphrase for proxy retrieval from MyProxy\n \n    \n \n \n  \n\n\n \n    \nmyproxy_username\n \n    \n User name to be used on MyProxy service\n \n    \n \n \n  \n\n\n \n    \nowner\n \n    \n If running standalone (as root) and you want the proxy to be owned by another account. \n \n    \n \n \n  \n\n\n \n    \nproxyfile\n \n    \n Target proxy path.\n \n    \n \n \n  \n\n\n \n    \nrenew\n \n    \n If you do not want to use ProxyManager to renew proxies, set this False and only define 'proxyfile' If renew is set to false, then no grid client setup is necessary. \n \n    \n \n \n  \n\n\n \n    \nretriever_profile\n \n    \n A list of other proxymanager profiles to be used to authorize proxy retrieval from MyProxy. \n \n    \n \n \n  \n\n\n \n    \nusercert\n \n    \n Path to the user grid certificate file\n\n    \n Only needed if flavor=voms\n \n  \n\n\n \n    \nuserkey\n \n    \n Path to the user grid key file\n \n    \n Only needed if flavor=voms\n \n  \n\n\n \n    \nvoms.args\n \n    \n Any extra arbitrary input option to be added to voms-proxy-init command\n \n    \n \n \n  \n\n\n \n    \nvorole\n \n    \n User VO role for target proxy. MyProxy Retrieval Functionality: Assumes you have created a long-lived proxy in a MyProxy server, out of band. \n \n    \n mandatory\n \n  \n\n\n\n\n\n\n\n\n\n\n\n\n 8  monitor.conf \n\n\n\n\n\n\n\n\n\n\n\n  \n\n    \nvariable\n\n    \ndescription\n\n    \ncomments\n\n  \n\n\n\n    \nmonitorplugin\n\n    \n the type of plugin to handle this monitor instance \n\n    \n \n\n  \n\n\n\n    \nmonitorURL\n\n    \n URL for the web monitor", 
            "title": "Reference Manual"
        }, 
        {
            "location": "/AutoPyFactoryWorkflowCream/", 
            "text": "Submission to CREAM CE \n\n\n 1  About this Document\n\n\n\n\n\n 2  Applicable versions\n\n\n \n 3  Configuration\n\n\n \n\n\n\n\n 1  About this Document \n\n\n\nThis document describes how to configure AutoPyFactory to submit to a CREAM CE.\n\n\n\n\n\n 2  Applicable versions \n\n\n\nThis documentation applies to the latest version of APF: 2.4.9\n\n\n\n\n\n 3  Configuration \n\n\n\nIn order to submit to CREAM, just set accordingly the batch submit plugin, and all related attributes. \nFor example:\n\n\n\n\nbatchsubmitplugin = CondorCREAM\n\nbatchsubmit.condorcream.webservice = lcgce04.gridpp.rl.ac.uk\nbatchsubmit.condorcream.port = 8443\nbatchsubmit.condorcream.batch = pbs\nbatchsubmit.condorcream.queue = grid3000M \nbatchsubmit.condorcream.condor_attributes = periodic_remove = (JobStatus == 5 \n&\n&\n (CurrentTime - EnteredCurrentStatus) > 3600) || (JobStatus == 1 \n&\n&\n globusstatus =!= 1 \n&\n&\n (CurrentTime - EnteredCurrentStatus) > 86400)\n\nbatchsubmit.condorcream.proxy = atlas-production", 
            "title": "Workflow CREAM"
        }, 
        {
            "location": "/AutoPyFactoryWorkflowEC2/", 
            "text": "Submission to EC2 \n\n\n 1  About this Document\n\n\n \n 2  Applicable versions\n\n\n \n 3  Configuration\n\n\n \n\n\n\n 1  About this Document \n\n\n\nThis document describes how to configure AutoPyFactory to submit to Amazon EC2\n\n\n\n\n\n 2  Applicable versions \n\n\n\nThis documentation applies to the latest version of APF: 2.4.9\n\n\n\n\n\n 3  Configuration \n\n\n\nIn order to submit to EC2, just set accordingly the batch status and submit plugins, and all related attributes. \nFor example:\n\n\n\n\nbatchstatusplugin = CondorEC2\nbatchsubmitplugin = CondorEC2\n\n\nbatchsubmit.condorec2.gridresource = https://ec2.us-west-1.amazonaws.com/\n\n\nbatchsubmit.condorec2.access_key_id = /home/autopyfactory/etc/ec2-racf-cloud/access.key\nbatchsubmit.condorec2.secret_access_key = /home/autopyfactory/etc/ec2-racf-cloud/secret.key\nbatchsubmit.condorec2.security_groups = foo\nbatchsubmit.condorec2.usessh = False\nbatchsubmit.condorec2.peaceful = False\nbatchsubmit.condorec2.ami_id = ami-abc123\nbatchsubmit.condorec2.instance_type = m2.4xlarge\nbatchsubmit.condorec2.spot_price = 0.25", 
            "title": "Workflow EC2"
        }, 
        {
            "location": "/AutoPyFactoryWorkflowGlidein/", 
            "text": "Glidein style workflows with AutoPyFactory \n\n\n 1  About this Document\n\n\n \n 2  Applicable versions\n\n\n \n 3  Configuration\n\n\n \n 4  Example 1\n\n\n \n 5  Example 2\n\n\n \n\n\n\n 1  About this Document \n\n\n\nThis document explains how to setup \nAutoPyFactory\n to work in a glidein style fashion\n\n\n\n\n\n\n\n 2  Applicable versions \n\n\n\nThis documentation applies to the latest version of APF: 2.4.9\n\n\n\n\n\n 3  Configuration \n\n\n\nIn the case of workflows using glideins, the payloads are condor jobs submitted to a pool, usually with no startd's associated. \nThen, when the factory notices those IDLE jobs in the condor pool, submits pilots (glideins in this case) to remote resources. These glideins will be configured to join the pool and allow those IDLE jobs to finally run. \nIn this scenario, in order to allow the APF factory to see the IDLE jobs in the condor pool, they need to be submitted with an specific classad:\n\n\n\n\n+MATCH_APF_QUEUE=\nlabel\n\n\n\n\n\nThis line will typically be included in the condor description file being used to submit the payload jobs to the condor pool. \nThe particular string \n+MATCH_APF_QUEUE\n tells the AutoPyFactory that those jobs are to be managed by the factory. \nThe different values of the \nlabel\n will allow the factory to treat them differently. All payload jobs with the same \nlabel\n will be managed together. Payload jobs with different \nlabel\n will be managed separately. \n\n\nAs it was explained \nhere\n, and APFQueue can be interpreted as the unique combination of a wms queue and a batch queue. in this particular type of workflow, the value of the \n+MATCH_APF_QUEUE\n classad represents the wms queue. \n\n\nThen, the batch queue, the system where the pilots will be submitted to, can be, for example, a remote Compute Element, or another condor pool. \n\n\n\n 4  Example 1 \n\n\n\nLet's say some of the payload jobs have been submitted to the empty pool by the end user with classad:\n\n\n\n\n+MATCH_APF_QUEUE=montecarlo\n\n\n\n\nThis first type of jobs are meant to be run in a remote grid site. \n\n\nIn this case, the \nWMSStatus\n plugin is \nCondor\n, as the condor pool is our WMS service in this case. \nAs the glideins will be submitted to a remote grid site -for example with a Globus GT5 gatekeeper, the \nBatchSubmitPlugin\n is \nCondorGT5\n. \nTo track the status of previously submitted glideins, as they are being submitted with condor-g, the \nBatchStatusPlugin\n is \nCondor\n.\n\n\nSo the basic configuration for this workflow in the \nqueues.conf\n file would include a section similar to this:\n\n\n\n\n[MC_SITE1_CE]\nwmsqueue = montecarlo\nwmsstatusplugin = Condor\nbatchstatusplugin = Condor\nbatchsubmitplugin = CondorGT5\n...\n\n\n\n\nFor the rest of variables, see the \nReference Manual\n\n\n\n\n\n\n 5  Example 2 \n\n\n\nIn this case, the end user payload jobs have classad:\n\n\n\n\n+MATCH_APF_QUEUE=analysis\n\n\n\n\nThis second type of jobs are meant to run on a local condor cluster. \nIn that case, the value of the \nCondorSubmitPlugin\n is \nCondorLocal\n. It is called \"local\" even if the schedd for this pool is in a remote host. \nSo the  \nqueues.conf\n file would have a section like this:\n\n\n\n\n[ANALYSIS_CONDORPOOL_XYZ]\nwmsqueue = analysis\nwmsstatusplugin = Condor\nbatchstatusplugin = Condor\nbatchsubmitplugin = CondorLocal\nbatchsubmit.condorlocal.submitargs = -remote \nremote_schedd\n\n...\n\n\n\n\nFor the rest of variables, see the \nReference Manual", 
            "title": "Workflow Glidein"
        }, 
        {
            "location": "/AutoPyFactoryWorkflowNorduGrid/", 
            "text": "Submission to NorduGrid CE \n\n\n 1  About this Document\n\n\n \n 2  Applicable versions\n\n\n \n 3  Configuration\n\n\n \n\n\n\n 1  About this Document \n\n\n\nThis document describes how to configure AutoPyFactory to submit to NorduGrid\n\n\n\n\n\n 2  Applicable versions \n\n\n\nThis documentation applies to the latest version of APF: 2.4.9\n\n\n\n\n\n 3  Configuration \n\n\n\nIn order to submit to ND, just set accordingly the batch submit plugin, and all related attributes. \nFor example:\n\n\n\n\nbatchsubmitplugin = CondorNordugrid\n\n\nbatchsubmit.condornordugrid.gridresource = lcg-lrz-ce2.grid.lrz.de \n\n\nnordugridrsl.jobname = 'analy_pilot'\nnordugridrsl.queue = lcg\nnordugridrsl.nordugridrsladd = (runtimeenvironment = APPS/HEP/ATLAS-SITE-LCG)(runtimeenvironment = ENV/PROXY )\nnordugridrsl.addenv.RUCIO_ACCOUNT = pilot", 
            "title": "Workflow NorduGrid"
        }, 
        {
            "location": "/AutoPyFactoryWorkflowPanda/", 
            "text": "PanDA-based workflow with AutoPyFactory \n\n\n 1  About this Document\n\n\n \n 2  Applicable versions\n\n\n \n 3  Configuration\n\n\n \n 4  Wrapper\n \n\n\n 4.1  Applicable version\n\n\n \n 4.2  Deployment\n\n\n \n 4.3  Input options\n\n\n \n\n \n\n\n\n 1  About this Document \n\n\n\nThis document describes how to configure AutoPyFactory when the WMS is PanDA.\n\n\n\n\n\n 2  Applicable versions \n\n\n\nThis documentation applies to the latest version of APF: 2.4.9\n\n\n\n\n\n 3  Configuration \n\n\n\nTo use PanDA as WMS system, set accordingly the wmsstatus plugin:\n\n\n\n\nwmsstatusplugin = Panda\n\n\n\n\nEach APFQueue must include the PanDA queue and the PanDA resource names, needed by the ATLAS pilot to request a job from the Dispatcher. For example:\n\n\n\n\nbatchqueue = ANALY_BNL_LONG-condor\nwmsqueue = ANALY_BNL_LONG\n\n\n\n\nThe rest of the configuration depends on the type of submission, documented in other sections.\n\n\n\n 4  Wrapper \n\n\n\n\n 4.1  Applicable version \n\n\n\nThis section describes how to use wrapper version 0.9.16\n\n\n\n 4.2  Deployment \n\n\n\nInstall the RACF yum repo files:\n\n\n\n\n[root@factory ~]$ rpm -Uhv http://dev.racf.bnl.gov/yum/grid/production/rhel/6Workstation/x86_64/racf-grid-release-latest.noarch.rpm\nRetrieving http://dev.racf.bnl.gov/yum/grid/production/rhel/6Workstation/x86_64/racf-grid-release-latest.noarch.rpm\nwarning: /var/tmp/rpm-tmp.TaawC5: Header V3 DSA/SHA1 Signature, key ID e6f6b87c: NOKEY\nPreparing...                ########################################### [100%]\n   1:racf-grid-release      ########################################### [100%]\n\n\n\n\nInstall the wrapper:\n\n\n\n\n[root@factory ~]$ yum install wrapper\n\n\n\n\nNOTE: if you want to to keep older versions of the wrapper when installing or updating to latest one, add the package \nwrapper\n to the yum configuration variable \ninstallonlypckgs\n in file \n/etc/yum.conf\n, and ensure \ninstallonly_limit\n is high enough (documentation can be found in manpage for yum.conf). Example: \n\n\n\n\ninstallonly_limit=3\ninstallonlypkgs=wrapper\n\n\n\n\n\n\n\n 4.3  Input options \n\n\n\n\n\n  \n\n    \nvariable\n\n    \ndescription\n\n    \ncomments\n\n  \n\n\n\n    \n--wrapperloglevel=\n\n    \n the log level. Current valid values are \ninfo\n and \ndebug\n \n\n    \n \n\n  \n\n\n\n    \n--wrapperpilotcode=\n\n    \n the location of the pilot code. \n Accepts more than one value, split by comma, to pick one randomly (see --wrapperpilotcoderatio). \n It is possible to set a set of URIs to just failover from one to another, without randomization. In that case, the list is enclosed between square parenthesis \n[\n and \n]\n. \n\n    \n \n\n  \n\n\n\n    \n--wrapperpilotcodechecksum=\n\n    \n the checksum of the pilot code. If more than one URIs for the pilotcode are provided, then a checksum value for each one of them is required, in a list split-by-comma \n\n    \n Optional\n\n  \n\n\n\n    \n--wrapperpilotcoderatio=\n\n    \n when a list of values split-by-comma are passed to the variable wrapprepilotcode, it sets the ratio to pick randomly between them \n\n    \n \n\n  \n\n\n\n    \n--wrapperplatform=\n\n    \nset the type of grid middleware available on the node. The wrapper may take some actions depending on the value. \n Current valid values are: \nOSG\n and \nEGI\n. \n\n    \n \n\n  \n\n\n\n    \n--wrapperplugin=\n\n    \n the exact plugin that will be invoked to run the final payload. \n Current valid values are: \natlasosg\n, \natlasegi\n, and \ntrivial\n. \n\n    \n \n\n  \n\n\n\n    \n--wrapperspecialcmd=\n\n    \n a special command needed to be performed before anything else. Very rearly needed. When needed, usually is a command to source an had-oc setup.sh file. \n Use with caution.\n\n    \n Optional\n\n  \n\n\n\n    \n--wrappertarballchecksum=\n\n    \n the checksum of the wrapper tarball\n\n    \n Optional\n\n  \n\n\n\n    \n--wrappertarballuri=\n\n    \n it is the URI with the location of the wrapper tarball. \n\n    \n \n\n  \n\n\n\n    \n--wrapperwmsqueue=\n\n    \nthe name of the queue in the WMS system (for example, in PanDA).\n\n    \n \n\n  \n\n\n\n    \nother options\n\n    \n Any other input option, not starting with string \n--wrapper\n, will be passed \nverbatim\n to the payload job (a.k.a. pilot) \n\n    \n \n\n  \n\n\n\n\n\nA typical example for ATLAS is like this:\n\n\n\n\nexecutable = /usr/libexec/wrapper-0.9.16.sh\narguments = --wrapperloglevel=debug \\\n           --wrapperplatform=OSG \\\n           --wrapperwmsqueue=ANALY_BNL_SHORT \\\n           --wrappertarballuri=http://dev.racf.bnl.gov/dist/wrapper/wrapperplugins-dev12.tar.gz \\\n           --wrapperpilotcode=http://pandaserver.cern.ch:25085/cache/pilot/pilotcode-PICARD.tar.gz \\\n           --wrapperplugin=atlasosg \\\n           -w https://pandaserver.cern.ch -p 25443 -u user\n\n\n\n\nHere are examples on how to pass more than one value to \n--wrapperpilotcode\n.\n\n\n\n\narguments = ... \\\n           --wrapperpilotcode = ['file:///cvmfs/atlas.cern.ch/pilot.py', 'http://atlas.cern.ch/pilot.tar.gz'], 'http://atlas.cern.ch/pilot-devel.tar.gz' \\\n           --wrapperpilotcoderatio = 99,1 \\\n           ...\n\n\n\n\nIn that case, 99% of the times the pilot code from CVMFS will be tried, failing over the tarball at the CERN URL in case of failures. 1% of the times the development pilot code will be downloaded.\n\n\n\n\narguments = ... \\\n           --wrapperpilotcode = ['file:///cvmfs/atlas.cern.ch/pilot.py', 'http://atlas.cern.ch/pilot.tar.gz'], 'http://atlas.cern.ch/pilot-devel.tar.gz' \\\n           --wrapperpilotcodechecksum = 123,456,789 \\\n           --wrapperpilotcoderatio = 99,1 \\\n           ...\n\n\n\n\nIn the second case, the same logic applies, but a checksum is passed for each possible case.", 
            "title": "Workflow Panda"
        }, 
        {
            "location": "/AutoPyFactoryWritePlugins/", 
            "text": "1  About this Document\n\n\n\n\n\n 2  Applicable versions\n\n\n \n 3  Understanding plugins in AutoPyFactory\n \n\n\n 3.1  Nomenclature\n\n\n \n 3.2  Singletons\n\n\n \n\n \n 4  Writing a new plugin\n \n\n\n 4.1  Write a new batchsubmit plugin\n\n\n \n 4.2  Write a new batchstatus plugin\n\n\n \n 4.3  Write a new sched plugin\n\n\n \n\n \n 5 UML diagrams \n \n\n\n 5.1  Plugins interfaces\n\n\n \n 5.2  Info classes in AutoPyFactory\n\n\n \n\n\n\n\n 1  About this Document \n\n\n\nThis document describes how to write new plugins in \nAutoPyFactory\n\n\n\n\n\nConventions used in this document:\n\n\n\n\n\nA \nUser Command Line\n is illustrated by a green box that displays a prompt:\n\n\n\n\n\n  [user@client ~]$\n\n\n\n\n\nA \nRoot Command Line\n is illustrated by a red box that displays the \nroot\n prompt:\n\n\n\n\n\n  [root@client ~]$\n\n\n\n\n\nLines in a file\n are illustrated by a yellow box that displays the desired lines in a file:\n\n\n\npriorities=1\n\n\n\n\n\n\n\n\n\n 2  Applicable versions \n\n\n\nThis documentation applies to the latest version of APF: 2.4.9\n\n\n\n\n\n 3  Understanding plugins in AutoPyFactory \n\n\n\nBasic description of the different types of plugins and their purposes can be found in section \"Plugins-based architecture\" in \nhere\n\n\n\nThere are two basic ideas to keep in mind to write a new plugin for AutoPyFactory: \n\n\n plugins have an API\n\n \n plugins use mappings to convert external information into internal abstract AutoPyFactory language\n\n \n the name of the plugin is also the name of the class implementing it, and the name of the .py module containing it\n\n \n all plugins' __init__() method always receive the same input parameters:  \n\n\n a reference to the calling object\n\n \n the ConfigParser object containing configuration that may be relevant for the plugin\n\n \n the corresponding section name for that ConfigParser object\n\n \n\n \n\n\nTherefore, to write a new plugin, one only needs to pay attention to the API, which is defined in file \nautopyfactory/interfaces.py\n, \nand, when needed, add a new conversion table to file \netc/mappings.conf\n\n\n\nPlugins are placed in a well defined directory tree, depending on the type:\n\n\nautopyfactory/\n    plugins/\n        factory/\n        queue/\n            batchstatus/\n            batchsubmit/\n            wmsstatus/\n            monitor/\n            sched/\n        authmanager/\n            auth/\n\n\n\n\nFor example, for the sched plugin \nReady\n, there is a class \nReady\n in module \nautopyfactory/plugins/queue/sched/Ready.py\n\n\n\n\n 3.1  Nomenclature \n\n\n\nAs mentioned, the name of the plugin is also the name of the class implementing it,  as well the name of the module containing it. \n\n\nIn the AutoPyFactory configuration files, there are a few convention rules that help to keep a consistent nomenclature between all plugins: \n\n\n the key that refers to a given plugin is built as \n type_of_plugin \n + \"plugin\"\n\n \n the name of the plugin is referred by its exact name (case sensitive)\n\n \n keys for internal parameters to be digested by the plugins are always lower case, and built as \n type_of_plugin \n + \".\" + \n name_of_the_plugin_lower_case \n + \".\" + \n parameter \n\n\n \n\n\nExample:\n\n\n\n\nschedplugin = MinPerCycle\nsched.minpercycle.minimum = 1\n\n\n\n\n\n 3.2  Singletons \n\n\n\nSome plugins may act on behalf of many AutoPyFactory internal components. For example, the \nbatchstatus\n plugins in charge of running periodically \ncondor_q\n commands does it for many APFQueue objects.\nIn those cases, it makes sense to have that plugin be a \nSingleton\n. \n\n\nSingletons are implemented in AutoPyFactory is by overriding the \nnew\n method of the plugin class, and moving the actual code of the plugin functionalities to a separate class whose instances are returned by that __new__() implementation.\n\n\nExample, for a plugin called MyPlugin, the module MyPlugin.py would have like this:\n\n\n\n\n\nclass _myplugin(....):\n\n        def __init__(self, parent, config, section_name):\n        ....\n        ....\n\n        def get(self):\n        ...\n        ...\n\n        def put(self):\n        ... \n        ...\n\n\nclass MyPlugin(object):\n\n        instance = None\n\n        def __new__(cls, *k, **kw):\n                # here we check if an object\n                # has already being created\n                if not MyPlugin.instance:\n                        MyPlugin.instance = _myplugin(*k, **kw)\n                return MyPlugin.instance\n\n\n\n\nThere is a special kind of Singletons in AutoPyFactory: the so called \"MultiSingletons\".\nThose are plugins that, even doing the same for several parent calling objects, they may have differences in their setup. A typical example is the \nbatchstatus\n plugin \nCondor\n, when it requires to query 2 different pools. \nIn that case, we manage 2 plugin objects, each one being a Singleton. \nThe implementation is similar:\n\n\n\n\n\n\n\nclass _myplugin(....):\n\n        def __init__(self, parent, config, section_name):\n        ....\n        ....\n\n        def get(self):\n        ...\n        ...\n\n        def put(self):\n        ... \n        ...\n\n\nclass MyPlugin(object):\n\n        instances = {}\n\n        def __new__(cls, *k, **kw):\n                # here we check if a new object\n                # is needed or not,\n                # usually by inspecting the content of input options\n                # passed to the plugin via *k\n                #       parent = k[0]\n                #       config = k[1]\n                #       section = k[2]\n                # and generating, when needed, a new key\n                ...\n                ...\n                if key not in MyPlugin.instances.keys():\n                        MyPlugin.instances[key] = _myplugin(*k, **kw)\n                return MyPlugin.instances[key]\n\n\n\n\n\n\n\n 4  Writing a new plugin \n\n\n\n\n 4.1  Write a new batchsubmit plugin \n\n\n\nTo write a new batchsubmit plugin, place the new module under directory \nautopyfactory/plugins/queue/batchsubmit/\n, and create a class inheriting the API.\n\n\n\n\n\nfrom autopyfactory.interfaces import BatchSubmitInterface\n\nclass MySubmit(BatchSubmitInterface):\n\n        def __init__(self, apfqueue, config, section):\n            qname = apfqueue.apfqname\n            foo = conf.get(qname, \"batchsubmit.mysubmit.bar\")    \n\n        def submit(self, n):\n             ...\n             ...\n\n\n\n\n\nThe method submit receives the number of pilots to be submitted. This method implements the code to perform the actual submission. \n\n\n\n 4.2  Write a new batchstatus plugin \n\n\n\nTo write a new batchstatus plugin, place the new module under directory \nautopyfactory/plugins/queue/batchstatus/\n, and create a class inheriting the API.\n\n\n\n\n\nfrom autopyfactory.interfaces import BatchStatusInterface\nfrom autopyfactory.info import BatchStatusInfo\nfrom autopyfactory.info import QueueInfo\n\nclass MyStatus(BatchStatusInterface):\n\n        def __init__(self, apfqueue, config, section):\n            qname = apfqueue.apfqname\n            foo = conf.get(qname, \"batchstatus.mystatus.bar\")    \n            mytarget2info_dict = apfqueue.factory.mappingscl.section2dict('MAPPINGS-FOR-MY-TARGET')\n\n       def getInfo(self):\n            ...\n\n\n\n\n\nAn example of mappings is \n\n\n\n\n[CONDORBATCHSTATUS-APFINFO]\n0 = pending\n1 = pending\n2 = running\n3 = done\n4 = done\n5 = suspended\n6 = running\n\n\n\n\nThe current list of internal AutoPyFactory statuses is \n\n\n\n\npending\nrunning\nsuspended\ndone\n\n\n\n\n\n\nMethod getInfo() returns aggregate statistics about jobs in batch system, indexed by queue.\nIt returns an object of class BatchStatusInfo, defined in \nautopyfactory/info.py\n, \nwhich is a dictionary of QueueInfo objects, also defined in \nautopyfactory/info.py\n.\n\n\nThe BatchStatusInfo dictionary is indexed by APFQueue name, the QueueInfo dictionary is filled via its method fill(), which receives as inputs a dictionary with status values and the corresponding mapping to convert those values into internal AutoPyFactory agnostic vocabulary. \nAn example of dictionary to be passed would be:\n\n\n{'0':5,\n'2':10}\n\n\nwhere the mappings will then convert '0' into 'pending' and '2' into 'running'\n\n\n\n 4.3  Write a new sched plugin \n\n\n\nThe Sched plugins have a very simple interface, implemented in module SchedInterface: just a method \ncalcSubmitNum(self, n=0)\n.\nThis method calculates how many new pilots to submit next time, implementing a given algorithm or policy. \n\n\nThe output returned by the method is a tuple of two items \n(out, msg)\n where \nout\n is the number of pilots to be submitted (can be a negative number), and \nmsg\n is a string representing the decision made. \nThis string message can then be propagated to some monitoring services. \n\n\n\n\n...\nfrom autopyfactory.interfaces import SchedInterface\n\nclass MySched(SchedInterface):\n\n        def __init__(self, apfqueue, config, section):\n                ...\n\n        def calcSubmitNum(self, n=0):\n                ...\n                out = 123\n                msg = 'MySched:in=%s;ret=%s' %(n, out)\n                return (out, msg)\n\n\n\n\n\n\n\n\n\n\n\n 5  UML diagrams \n\n\n\n\n 5.1  Plugins interfaces \n\n\n\n\n\n\n\n\n\n\n 5.2  Info classes in AutoPyFactory", 
            "title": "How to write your own plugins"
        }, 
        {
            "location": "/AutoPyFactoryQA/", 
            "text": "Questions and Answers related AutoPyFactory \n\n\n\n\n 1  About this Document \n\n\n\nThis document is just a somehow free-format list of questions asked about \nAutoPyFactory\n, and some of the answers provided. \nThe answer may be straight forward, or may be just some hints about where to look.\n\n\nIt does not include the complete list of questions, as this documented was started on August 10th, 2015. \nIt is not a FAQ page, therefore.\n\n\n\n\nConventions used in this document:\n\n\n\n\n\nA \nUser Command Line\n is illustrated by a green box that displays a prompt:\n\n\n\n\n\n  [user@client ~]$\n\n\n\n\n\nA \nRoot Command Line\n is illustrated by a red box that displays the \nroot\n prompt:\n\n\n\n\n\n  [root@factory ~]$\n\n\n\n\n\nLines in a file\n are illustrated by a yellow box that displays the desired lines in a file:\n\n\n\npriorities=1\n\n\n\n\n\n\n\n 2  Applicable versions \n\nThis document mostly applies to the version of AutoPyFactory 2.4.5 and higher. Some answer may be valid for older versions too, but that is not guaranteed. \n\n\n\n 3  Questions and answers \n\n\n\n\n\n\n\n\n 3.1  how do I interpret comments like \"fixed in version X\" at the same time that \"This documentation applies to the latest version of APF: Y\"? \n\n\n\nWhen the versions X and Y are different, usually that means that a bug was found in the version Y -the one the current documentation is for-, \nand a fix for that bug will be released in a future version -X or higher-, but not yet. \n\n\nIt can also mean that we forgot to update the message \"This documentation applies to the latest version of APF... \" \n \n\n\n\n 3.2  what do I do if I want to change the path of the log files? \n\n\n\nSome explanation can be found \nhere\n \n\n\n\n 3.3  how can I submit to a remote HTCondor schedd? \n\n\n\nIt is not needed the APF host also be an HTCondor schedd. \nRemote submission is allowed. \nThe queryargs -for the BatchStatus Plugin- and the submitargs -for the Submit Plugin- can be used to pass any arbitrary options, for example to refer to a remote schedd.\n\n\n\n\n    batchsubmitplugin = CondorLocal\n    batchsubmit.condorlocal.submitargs = -remote \nremote_schedd\n:\nport\n\n\n    batchstatusplugin = Condor\n    batchstatus.condor.queryargs = -pool \nremote_pool\n -name \nremote_schedd\n\n\n\n\n\n\n\n\n 3.4  why pilots submitted to a CREAM CE fail? \n\n\n\nIf your pilots being submitted to a CREAM CE fail, with this type of content in the condor log file:\n\n\n\n\n        009 (45454545.000.000) 12/10 20:35:01 Job was aborted by the user.\n            CREAM error: BLAH error: submission command failed (exit code = 1) (stdout:) (stderr:) N/A (jobId = CREAM123456789)\n\n\n\n\nit is possible that the grid_resource line in the condor submit file is not correct. \nAutoPyFactory creates a line like this\n\n\n\n\ngrid_resource = cream matrix.net:8443/ce-cream/services/CREAM2 pbs queuename\n\n\n\n\nwhile perhaps the CE needs a line like\n\n\n\n\ngrid_resource = cream matrix.net:8443/queuename\n\n\n\n\nIf that is the case, or similar, there is dirty trick to fix it. Add this line to your queue configuration (typically in queues.conf):\n\n\n\n\nbatchsubmit.condorcream.condor_attributes.grid_resource = cream matrix.net:8443/queuename\n\n\n\n\n\n\n\n\n\n 3.5  to which type of resource targets I can submit pilots with AutoPyFactory? \n\n\n\nIn principle, to those supported by HTCondor. That includes globus GT2, globus GT5, HTCondor-CE, CREAM CE, NorduGrid CE, Amazon EC2, a local or remote HTCondor cluster,...\nIf HTCondor cannot submit to a given resource, then AutoPyFactory cannot either.\n\n\n\n 3.6  how do I find out the version of AutoPyFactory I have installed? \n\n\n\nA rpm command will give the answer:\n\n\n\n\n[root@factory ~]$ rpm -qa | grep autopyfactory\nautopyfactory-tools-1.0.1-1.noarch\nautopyfactory-2.4.1-1.noarch\n\n\n\n\n\n 3.7  how should I configure the factory if I want to renew the proxies myself? \n\n\n\nIf you don't want the factory to renew the x509 proxies for you because you have in place a different mechanism for that, you only need to disable the renewal feature. \nBut, if the factory is configured to submit to grid sites or any other resource that requires an x509 file, the proxymanager still must be enabled anyways. \nSo, in your \nautopyfactory.conf\n file:\n\n\n\n\nproxymanager.enabled = True\n\n\n\n\nbut in your \nproxy.conf\n file:\n\n\nrenew = False\n\n\n\n\n\n 3.8  how can I add environment variables to the condor submit file? \n\n\n\nUsing the configuration variable \nbatchsubmit.\nplugin\n.environ\n. \nFor details, check the \nReference Manual\n\n\n\n\n 3.9  how can I add any arbitrary line to the condor submit file? \n\n\n\nUsing the configuration variable \nbatchsubmit.\nplugin\n.condor_attributes\n. \nFor details, check the \nReference Manual\n\n\n\n\n\n\n 3.10  how can I tune the number of pilots to be submitted depending on the number of activated jobs in PanDA and the number of pilots still pending? \n\n\n\nThe sched plugin \nReady\n checks the number of activated jobs, the number of idle pilots, and returns the difference. \nFor details on the sched plugins parameters, check the \nReference Manual\n\n\n\n\n 3.11  how can I run a factory with a special setup? \n\n\n\nIf you need to ensure some environment variables are set when running the factory, for example, you can add any custom setup in the \n/etc/sysconfig/autopyfactory\n file. It is sourced by the daemon init script \n/etc/init.d/autopyfactory\n before start running.  \n\n\n\n\nOPTIONS=\"--debug --sleep=60 --runas=autopyfactory --log=/var/log/autopyfactory/autopyfactory.log\"\nCONSOLE_LOG=/var/log/autopyfactory/console.log\nenv | sort >> $CONSOLE_LOG\n\nsource /path/to/my/custom/factory/setup.sh\n\n\n\n\n\n 3.12  my factory is not submitting pilots anymore for a given queue, why? \n\n\n\nFirst check that queue has no \nenabled=False\n set in the configuration file \n/etc/autopyfactory/queues.conf\n\nIf that was not the problem, there are several main reasons why this can happens:\n\n \n\n\n One reason is that there are no ready jobs in the WMS system (for example, in PanDA). Standard factory configuration does not submit pilots when there are no jobs.  \n\n \n\n \n\n\n Another reason may be data corruption from the CE/gatekeeper. Sometimes it happens the CE/gatekeeper reports back to condor a fake number of IDLE and RUNNING pilots. If the number of IDLE pilots returned is high enough (even if that number is not real), the factory most probably will stop submitting and wait for those pending pilot to start running and finish. \n\n \n\n \n\n\n The queue in the WMS system is disabled.\n\n \n\n \n\n\n There is no WMSStatus or BatchStatus information available.\n\n \n\n\nHow to check?\n\n \n\n\n for the first case, it is enough to check the AutoPyFactory log files, with a command like this:\n\n \n\n\n\n\n[root@factory ~]$ grep ANALY_BNL_SHORT-gridgk04-htcondor /var/log/autopyfactory/autopyfactory.log | grep schedplugin\n...\n2015-08-10 18:38:20,363 (UTC) [ DEBUG ] main.schedplugin[ANALY_BNL_SHORT-gridgk04-htcondor] ReadySchedPlugin.py:34 calcSubmitNum(): Starting.\n2015-08-10 18:38:20,364 (UTC) [ DEBUG ] main.schedplugin[ANALY_BNL_SHORT-gridgk04-htcondor] ReadySchedPlugin.py:60 _calc(): pending = 10 running = 17 offset = 0\n2015-08-10 18:38:20,364 (UTC) [ INFO ] main.schedplugin[ANALY_BNL_SHORT-gridgk04-htcondor] ReadySchedPlugin.py:68 _calc(): input=0; activated=83; offset=0 pending=10; running=17; Return=73\n2015-08-10 18:38:20,364 (UTC) [ DEBUG ] main.schedplugin[ANALY_BNL_SHORT-gridgk04-htcondor] ScaleSchedPlugin.py:31 calcSubmitNum(): Starting with n=73\n2015-08-10 18:38:20,364 (UTC) [ INFO ] main.schedplugin[ANALY_BNL_SHORT-gridgk04-htcondor] ScaleSchedPlugin.py:36 calcSubmitNum(): Return=19\n2015-08-10 18:38:20,364 (UTC) [ DEBUG ] main.schedplugin[ANALY_BNL_SHORT-gridgk04-htcondor] MaxPerCycleSchedPlugin.py:23 calcSubmitNum(): Starting with n=19\n2015-08-10 18:38:20,365 (UTC) [ INFO ] main.schedplugin[ANALY_BNL_SHORT-gridgk04-htcondor] MaxPerCycleSchedPlugin.py:35 calcSubmitNum(): input=19; Return=19\n2015-08-10 18:38:20,365 (UTC) [ DEBUG ] main.schedplugin[ANALY_BNL_SHORT-gridgk04-htcondor] MinPerCycleSchedPlugin.py:24 calcSubmitNum(): Starting with n=19\n2015-08-10 18:38:20,365 (UTC) [ INFO ] main.schedplugin[ANALY_BNL_SHORT-gridgk04-htcondor] MinPerCycleSchedPlugin.py:34 calcSubmitNum(): Return=19\n2015-08-10 18:38:20,365 (UTC) [ DEBUG ] main.schedplugin[ANALY_BNL_SHORT-gridgk04-htcondor] StatusTestSchedPlugin.py:26 calcSubmitNum(): Starting.\n2015-08-10 18:38:20,366 (UTC) [ DEBUG ] main.schedplugin[ANALY_BNL_SHORT-gridgk04-htcondor] StatusTestSchedPlugin.py:37 calcSubmitNum(): site status is online\n2015-08-10 18:38:20,366 (UTC) [ INFO ] main.schedplugin[ANALY_BNL_SHORT-gridgk04-htcondor] StatusTestSchedPlugin.py:45 calcSubmitNum(): [Queue is not test] input=19; Return=19\n2015-08-10 18:38:20,366 (UTC) [ DEBUG ] main.schedplugin[ANALY_BNL_SHORT-gridgk04-htcondor] StatusOfflineSchedPlugin.py:29 calcSubmitNum(): Starting.\n2015-08-10 18:38:20,366 (UTC) [ DEBUG ] main.schedplugin[ANALY_BNL_SHORT-gridgk04-htcondor] StatusOfflineSchedPlugin.py:51 calcSubmitNum(): site status is online\n2015-08-10 18:38:20,367 (UTC) [ INFO ] main.schedplugin[ANALY_BNL_SHORT-gridgk04-htcondor] StatusOfflineSchedPlugin.py:66 calcSubmitNum(): [Queue is not offline] input=19; Return=19\n2015-08-10 18:38:20,367 (UTC) [ DEBUG ] main.schedplugin[ANALY_BNL_SHORT-gridgk04-htcondor] MaxPendingSchedPlugin.py:25 calcSubmitNum(): Starting with n=19\n2015-08-10 18:38:20,367 (UTC) [ DEBUG ] main.schedplugin[ANALY_BNL_SHORT-gridgk04-htcondor] MaxPendingSchedPlugin.py:36 calcSubmitNum(): Pending is 10\n2015-08-10 18:38:20,367 (UTC) [ INFO ] main.schedplugin[ANALY_BNL_SHORT-gridgk04-htcondor] MaxPendingSchedPlugin.py:53 calcSubmitNum(): Return=0 \n\n\n\n\nPay attention to the messages coming from the ReadySchedPlugin. Field \nActivated\n will tell you is there are jobs or not. \n\n \n\n\n For the second scenario, best way is to run condor_q and see how it says. When possible, print the JobStatus information, or equivalent:\n\n \n\n\n\n\n[root@factory ~]$ condor_q  -format '%s.' ClusterId -format '%s ' ProcId -format ' MATCH_APF_QUEUE=%s' match_apf_queue -format ' JobStatus=%d\\n' JobStatus \n1230.0 MATCH_APF_QUEUE=BNL_PROD-gridgk01-htcondor JobStatus=1\n1231.0 MATCH_APF_QUEUE=BNL_ATLAS_2-gridgk04-htcondor JobStatus=2\n1232.0 MATCH_APF_QUEUE=ANALY_BNL_LONG-gridgk01-htcondor JobStatus=2\n1233.0 MATCH_APF_QUEUE=BNL_ATLAS_2-gridgk01-htcondor JobStatus=2\n1234.0 MATCH_APF_QUEUE=BNL_ATLAS_2-gridgk07-htcondor JobStatus=2\n1235.0 MATCH_APF_QUEUE=BNL_PROD_MCORE-gridgk07-htcondor JobStatus=1\n\n\n\n\nAlso, when it is installed, try running command apf-queue-status\n\n\n\n\n[root@factory ~]$ apf-queue-status\nMon Aug 10 15:12:24 2015\n-----------------------------------------------------------------\nANALY_BNL_LONG-gridgk04-htcondor    UNSUB = 0   IDLE = 4    RUNNING = 29    COMPLETE = 0    HELD = 0    ERROR = 0   REMOVED = 0 \nANALY_BNL_LONG-gridgk05         UNSUB = 0   PENDING = 10    STAGE_IN = 0    ACTIVE = 10 STAGE_OUT = 0   SUSP = 0    DONE = 0    FAILED = 0  \n\n\n\n\n\n \n\n\n the 3rd case applies mostly to PanDA. Check if the PanDA queue is \noffline\n. In that case, AutoPyFactory usually does not submit pilots.\n\n \n\n \n\n\n Besides all of that, you can also check the queue object has actually been created. A grep command like this may help:\n\n \n\n\n\n\n[root@factory ~]$ grep \"APFQueue: Initializing object...\" /var/log/autopyfactory/autopyfactory.log\n...\n2015-08-10 19:31:51,271 (UTC) [ DEBUG ] main.apfqueue[ANALY_BNL_LONG-gridgk02-htcondor] factory.py:870 __init__(): APFQueue: Initializing object...\n2015-08-10 19:31:51,557 (UTC) [ DEBUG ] main.apfqueue[BNL_ATLAS_RCF-gridgk02-htcondor] factory.py:870 __init__(): APFQueue: Initializing object...\n2015-08-10 19:31:51,859 (UTC) [ DEBUG ] main.apfqueue[BNL_ATLAS_RCF-gridgk06-htcondor] factory.py:870 __init__(): APFQueue: Initializing object...\n...\n\n\n\n \n\n\n When there is no WMS Status or Batch Status information available, the Sched Plugin Ready returns 0. To check if that is the case, search for a line like this in the logs file:\n\n \n\n\n\n\n2015-11-30 03:33:14,326 (UTC) [ WARNING ] main.schedplugin[A_QUEUE_AT_A_SOTE] ReadySchedPlugin.py:39 calcSubmitNum(): Missing info. wmsinfo is WMSQueueInfo: notready=0, ready=532,     running=314, done=0, failed=0, unknown=0 batchinfo is None\n\n\n\n\nIf the WMSQueueInfo is None, something is broken in the communication with the WMS service (for example, with a PanDA server).\nIf the batchinfo is None, most probably the condor daemon is not running and command condor_q does not work.\n\n\n\n\n\n 3.13  I do not want my factory to show up in the generic AutoPyFactory web monitor \n\n\n\nSolution currently for this is to remove the config variable \nmonitorsection\n from file \n/etc/autopyfactory/queues.conf\n\n\n\n\n\n\n 3.14  how can I increase the verbosity level in the log files? \n\n\n\nEdit file \n/etc/sysconfig/autopyfactory\n and change the loglevel from INFO level to DEBUG level:\n\n\n\n\nOPTIONS=\"--debug --sleep=60 --runas=autopyfactory --log=/var/log/autopyfactory/autopyfactory.log\"\n\n\n\n\n\n 3.15  How can I change the non-root running user? \n\n\n\nThe default configuration sets the factory to run as user \nautopyfactory\n.\nThat can be changed in file \n/etc/sysconfig/autopyfactory\n, via the configuration variable \n--runas\n\n\n\n\n\nOPTIONS=\"--debug --sleep=60 --runas=autopyfactory --log=/var/log/autopyfactory/autopyfactory.log\"\n\n\n\n\nNote that the RPM creates user account \nautopyfactory\n during the installation process. If you want to run as a different user, you will need to create that account manually.\n\n\n\n 3.16  How can I force the value of variable arguments in the condor submit file to be enclosed in between quotes? \n\n\n\nIf you want the condor submit file to look like this\n\n\n\n\narguments = \"--wrapperloglevel=debug ...-p 25443 -u user -u managed\"\n\n\n\n\nyou only need to add the quoutes in the queues configuration file. Typically, like this:\n\n\n\n\n[DEFAULT]\nexecutable.defaultarguments = --wrapperloglevel=debug ...-p 25443 -u user\n\n[ANALY_QUEUE]\nexecutable.arguments = \"%(executable.defaultarguments)s -u managed\"\n\n\n\n\n\n 3.17  I have variables in the \n[DEFAULT]\n section whose value requires interpolation, and they fail on some sections \n\n\n\nWhen the \n[DEFAULT]\n section in a given configuration file contains variables like this\n\n\n\n\n[DEFAULT]\nfoo = %(bar)s blah\n\n\n\n\nthe variable \nbar\n is expected to be defined in every section of that configuration file.\nWhen that is not the case, a traceback like this is printed out:\n\n\n\n\n    self._interpolate_some(option, L, rawval, section, vars, 1)\n  File \"/usr/lib64/python2.6/ConfigParser.py\", line 646, in _interpolate_some\n    option, section, rest, var)\nInterpolationMissingOptionError: Bad value substitution:\nsection: [ANALY_TRIUMF_ARC-ce3]\noption : foo\nkey    : bar\nrawval : blah\n\n\n\n\nThis will mostly ocurr in the case of queues configuration files, where the variable \nfoo\n is being resolved in a section where it is not needed. \nThe simplest solution is to provide for a fake value for token \nbar\n to allow the interpolation to work.\nIf that is not desired because it is confusing, or there are too many sections where it happens, then a good idea is to split the configuration file into two separate files, each one with the right \n[DEFAULT]\n section, and list both of them, split by comma, in the factory configuration file.\n\n\n\n\n\n 3.18  what do I do when I don't want to use an X509 proxy? \n\n\n\nTo avoid AutoPyFactory from trying to get X509 credentials, set the variable \nbatchsumit.\nyour_submit_plugin\n.proxy\n to \nNone\n\n\n\n\n 4  Troubleshooting \n\n\n\nThis section contains reported tracebacks and other misbehavior, and their possible explanation.\nThey mean there is a bug in the code, otherwise, no traceback should appear in the log files. But bugs happen. And, even if they are fixed in the next release, the problem persist for those instances running old versions. \n\n\n\n 4.1  qcl_dir \n\n\n\n\n\n2017-03-03 08:29:53,565 (UTC) [ ERROR ] main factory.py:435 run(): Traceback (most recent call last):\n  File \"/usr/lib/python2.6/site-packages/autopyfactory/factory.py\", line 411, in run\n    f.run()\n  File \"/usr/lib/python2.6/site-packages/autopyfactory/factory.py\", line 657, in run\n    self.reconfig()\n  File \"/usr/lib/python2.6/site-packages/autopyfactory/factory.py\", line 695, in reconfig\n    tmpqcl = config_plugin.getConfig()\n  File \"/usr/lib/python2.6/site-packages/autopyfactory/plugins/factory/config/File.py\", line 69, in getConfig\n    raise ConfigFailure('Failed to create queues ConfigLoader: %s' %err)\nConfigFailure: Failed to create queues ConfigLoader: local variable 'qcl_dir' referenced before assignment\n\n\n\n\nA traceback related variable \nqcl_dir\n usually means the configuration variable \nqueuesDirConf\n has been set in file \nautopyfactory.con\n, but the directory does not actually exists, or it is empty. \n\n\nMost probably, after a fresh deployment, it is set as \nqueuesDirConf = /etc/autopyfactory/queues.d/\n, but that directory does not exists.\n\n\nNote: fixed in version 2.4.10\n\n\n\n 4.2  condor_q against remote pool \n\n\n\n\n\n2017-03-10 17:47:20,581 (UTC) [ WARNING ] main.condor condor.py:356 querycondor(): Leaving with bad return code. rc=1 err=Error: Collector has no record of schedd/submitter\n\n\n\n\nThis happens due a bug in the code that creates the condor_q command line incorrectly, with a missing white space between arguments.\n\n\nNote: fixed in version 2.4.10", 
            "title": "Q&As"
        }
    ]
}